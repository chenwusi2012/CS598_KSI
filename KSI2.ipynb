{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfbLDAbD-Ho9"
      },
      "source": [
        "# Check GPU Status"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook requires hardware acceleration with GPU. Run the following code to make sure the GPU is running."
      ],
      "metadata": {
        "id": "z0kJAKAWc_4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PysjRxy7Ue3a",
        "outputId": "8d41fd87-4ac3-4527-f98f-b05eb0daad1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 22 18:35:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    24W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading"
      ],
      "metadata": {
        "id": "MTpL8Y_PYIVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the following files are loaded:\n",
        "\n",
        "* NOTEEVENTS.csv\n",
        "* DIAGNOSES_ICD.csv\n",
        "* wikipedia_knowledge\n",
        "* IDlist.npy\n",
        "\n"
      ],
      "metadata": {
        "id": "sraXyzOFZFDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load NOTEEVENTS.csv and DIAGNOSES_ICD.csv, take and pass the training \"CITI Data or Specimens Only Research\" at [https://about.citiprogram.org/](https://about.citiprogram.org/), and apply for the access to the MIMIC-III Clinical Database at PhysioNet at [https://physionet.org/content/mimiciii/1.4/](https://physionet.org/content/mimiciii/1.4/). After gaining the access, download NOTEEVENTS.csv and DIAGNOSES_ICD.csv from PhysioNet, and upload these two CSV files to a created folder \"cs598_project\" in Google drive.  \n",
        "Mount the Google Drive to Google Colab runtime."
      ],
      "metadata": {
        "id": "dnmtN942Zvsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "houR55vShhQE",
        "outputId": "d26727ae-2b3a-47b0-b48f-25dca9911d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the file in the \"cs598_project\" folder to make sure NOTEEVENTS.csv and DIAGNOSES_ICD.csv are uploaded to this folder."
      ],
      "metadata": {
        "id": "l95OzOa5aToH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ePN8cqnMog",
        "outputId": "d160171e-034b-4795-a912-4888235a1443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined_dataset   IDlist.npy\t   wikipedia_knowledge\n",
            "DIAGNOSES_ICD.csv  NOTEEVENTS.csv\n"
          ]
        }
      ],
      "source": [
        "!ls drive/MyDrive/cs598_project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy NOTEEVENTS.csv and DIAGNOSES_ICD.csv from Google Drive to Google Colab Runtime."
      ],
      "metadata": {
        "id": "wY0S-SeNammD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnsInD8unfwD"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/cs598_project/DIAGNOSES_ICD.csv DIAGNOSES_ICD.csv\n",
        "!cp drive/MyDrive/cs598_project/NOTEEVENTS.csv NOTEEVENTS.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wikipedia_knowledge and IDlist.npy can be downloaded from the GitHub Repository of the original paper."
      ],
      "metadata": {
        "id": "eV4EY3L_a7dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/tiantiantu/KSI/blob/master/wikipedia_knowledge\n",
        "!wget https://github.com/tiantiantu/KSI/blob/master/IDlist.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61F2nkrrNlfT",
        "outputId": "fbe0f144-ab70-4474-ab75-962c24e34ad7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 21:30:55--  https://github.com/tiantiantu/KSI/blob/master/wikipedia_knowledge\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘wikipedia_knowledge’\n",
            "\n",
            "wikipedia_knowledge     [ <=>                ] 134.78K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-04-11 21:30:55 (18.6 MB/s) - ‘wikipedia_knowledge’ saved [138012]\n",
            "\n",
            "--2023-04-11 21:30:55--  https://github.com/tiantiantu/KSI/blob/master/IDlist.npy\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘IDlist.npy’\n",
            "\n",
            "IDlist.npy              [ <=>                ] 134.39K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-04-11 21:30:56 (14.0 MB/s) - ‘IDlist.npy’ saved [137612]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import timeit and datetime to track the running time of the notebook."
      ],
      "metadata": {
        "id": "EebpslhDelbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Time Tracking"
      ],
      "metadata": {
        "id": "PSFxY46aet2F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1HPfLA60Nc"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlWdp-4V-aqS"
      },
      "source": [
        "# Data Pre-processing 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install stop-words to filter out stop words in the clinical notes."
      ],
      "metadata": {
        "id": "s6t9c0y4ebaW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHBorULhpNXR",
        "outputId": "dfad45a5-6ada-4e92-b384-3f12ffa93516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=d11499108d1cf270475561cd7060c07a3d475f2471147a08c04b0a01ed8fb3b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/d8/66/395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ],
      "source": [
        "!pip install stop-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhl6WfZAoeUD"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import string\n",
        "from stop_words import get_stop_words    # download stop words package from https://pypi.org/project/stop-words/\n",
        "import numpy as np\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjxnwUaaGg1i"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kk0iqHOBo4S"
      },
      "source": [
        "Create a dictionary from NOTEEVENTS.csv. The key is HADM_ID (ID of a visit), and the value is clinical note.  \n",
        "For clinical note, replace line change with whitespace, remove punctuation, and lowercase all letters.  \n",
        "From paper: \"During preprocessing we lowercased all\n",
        "tokens and removed punctuations, stop words, words containing\n",
        "only digits, and words whose frequency is less than 10\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVfixu8AVr2"
      },
      "outputs": [],
      "source": [
        "stop_words = get_stop_words('english')\n",
        "\n",
        "admidic=defaultdict(list)\n",
        "count=0\n",
        "\n",
        "with open('NOTEEVENTS.csv', 'r') as csvfile:\n",
        "  spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "  for row in spamreader: # Iterate all entries in NOTEEVETS\n",
        "    if row[6]=='Discharge summary': # Check if the category is discharge summary\n",
        "      # identify patients through subject_id\n",
        "      # append text to the list of text for a HADM_ID (visit) \n",
        "      # lower case all words, and remove punctuation\n",
        "      admidic[row[2]].append(row[-1].replace('\\n',' ').translate(str.maketrans('','',string.punctuation)).lower())\n",
        "      count=count+1 # count number of discharge summary notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow1MYjANCmhO"
      },
      "source": [
        "Print the number of notes in NOTEEVENTS.csv (From paper: \"The total number of\n",
        "discharge summary notes is 59,652.\").  \n",
        "Print a sample key-value pair (the clinical note for visit 167853)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS_gtxj1Ahrv",
        "outputId": "db6d7673-e245-4ac0-ac23-7b537ddfe439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of notes in NOTEEVENTS.csv = 59652\n",
            "['admission date  2151716       discharge date  215184   service addendum  radiologic studies  radiologic studies also included a chest ct which confirmed cavitary lesions in the left lung apex consistent with infectious processtuberculosis  this also moderatesized left pleural effusion  head ct  head ct showed no intracranial hemorrhage or mass effect but old infarction consistent with past medical history  abdominal ct  abdominal ct showed lesions of t10 and sacrum most likely secondary to osteoporosis these can be followed by repeat imaging as an outpatient                                first name8 namepattern2  first name4 namepattern1 1775 last name namepattern1  md  md number1 1776  dictated byhospital 1807 medquist36  d  215185  1211 t  215185  1221 job  job number 1808 ', 'admission date  2151716       discharge date  215184    history of present illness  the patient is an 86 year old african american female who on the morning of 716 was found on the floor of her unairconditioned home by a relative during the heat wave  she was conscious but the family reports she had not been drinking much and had not been feeling well one day prior to admission  the family also reports a productive cough 30 lb weight loss in the last three months shortness of breath over the last few months worsened by exertion and increasing edema  the patient was taken to the emergency department and rectal and irregular  blood pressure was 13776 respiratory rate 27 cooling measures were started in the emergency department she was placed on a 100 oxygen nonrebreather mask  the patient gradually became alert and oriented times two  in the emergency department she subsequently became hypotensive with blood pressure of 8049 heart rate 87 and irregular more aggressive fluid resuscitation was started  she was briefly placed on a norepinephrine drip for hemodynamic instability which was later changed to a levophed drip  a central line was placed  the patients temperature gradually came down to 986 over several hours  laboratory studies and blood cultures were drawn  chest xray was done electrocardiogram urinalysis arterial blood gases were doneand the patient was started on broad spectrum antibiotics  past medical history  cardiomyopathy idiopathic echocardiogram in 2141 showed an ejection fraction of 20  hypertension pulmonary hypertension chronic atrial fibrillation intermittent left bundle branch block history of anemia and heme positive stools previously refused colonoscopy  history of previous pulmonary embolism in 2141  history of previous stroke in 2141 history of previous myocardial infarction undocumented in 2144 chronic rightsided pleural effusion first found in 21501112  history of multiple episodes of cellulitis claustrophobia  medications on admission  coumadin 6 mg alternating 4 mg qd lasix 20 mg qd lisinopril 10 mg qd diltiazem 300 mg qd digoxin 0125 mg qd  allergies  no known drug allergies  social history  the patient is an african american female who lived alone  family checks on her often and lives nearby  family history  there is a family history of colon cancer  physical examination  on physical examination as noted in emergency departmentmedicine intensive care unit notes general a thin malnourished african american female disoriented  head eyes ears nose and throat pupils are equal round and reactive to light extraocular movements intact  dry mucous membranes  oropharynx clear  neck supple  jugulovenous distension noted to be 6 cm  lungs bibasilar crackles denies cough  rhonchi throughout  no wheezes  cardiovascular irregularly irregular rhythm iivi systolic murmur  abdominal positive bowel sounds soft nontender nondistended no rebound or guarding  no masses  guaiac positive rectal examination  extremities no edema severe chronic venous insufficiencystasis dermatitis in the lower extremities 1 pulses bilaterally neurological no photophobia  the patient waxes and wanes with her disorientation  laboratory data  complete blood count showed white count 72 hematocrit 342 platelets 285 differential 96 neutrophils 3 lymphocytes 1 monocyte  chem7 sodium 140 potassium 48 chloride 100 bicarbonate 27 bun 38 creatinine 14 glucose 157  pt 254 inr 45 ptt 40 lactate 18 calcium 9 magnesium 17 prostate 3  ck was 53 alkaline phosphatase 137 total bilirubin 03 alt 15 ast 18  electrocardiogram showed atrial fibrillation at a rate of 144 with left bundle branch block  chest xray showed possible interstitial edema persistent right basilar opacity which could represent pleural thickening or effusion possible infection process in the right lung infection could not be excluded new opacities in the left lung apex which were concerning for acute infection  intensive care unit course  in the medicine intensive care unit pressors were discontinued a few hours after admission secondary to improved hemodynamic instability  repeat chest xray showed possible pneumonia in the lower lobes with a cavitary lesion in the left upper lobe concerning for tuberculosis  the patient was subsequently placed in negative pressure room with respiratory precautions started the patient also had three cycles of induced sputums for acid fast bacillus smear and culture  antibiotics were continued  on hospital day 2 the patient began taking po she remained afebrile throughout her hospital course  on hospital day 4 the patient was transferred to the medicine service and acid fast bacillus smear from hospital day 2 came back as moderately positive for acid fast bacilli  hospital course  infectious disease  as noted above repeat chest xray showed cavitary lesions in the left apex  on hospital day 2 the patient was placed on isolationrespiratory precautions with negative pressure isolation room  acid fast sputums times three were taken for acid fast bacilli  on hospital day 4 sputum doctor last name 1770 from hospital day 2 showed positive acid fast bacilli infectious disease consult was called on hospital day 5  720 the patient was started on isoniazid rifampin pyrazinamide for pulmonary tb ethambutol was started on hospital day 6 infectious control and the department of public health were notified and got in touch with the family members and hospital staff  the patient completed a seven day course of ceftriaxone and azithromycin for five days for question of aspiration pneumonia  all of the patients blood cultures and sputum and urine cultures were  she was clostridium difficile negative and rpr negative  there were three attempts one fluoroscopy guided at lumbar puncture to evaluate for meningeal tuberculosis but were unsuccessful due to severe degenerative joint disease of the spine ultimately it was decided there was a low suspicion for cns tb given the patients improvement in mental status final recommendations for duration of antituberculosis regimen will be given at a later date by infectious disease service sensitivities are pending for tuberculosis  tuberculosis was confirmed by pcr gene probe the patient remained throughout the rest of the hospital day 2 throughout hospital course and isolation in negative pressure room with respiratory precautions  she will remain so until she has negative acid fast bacillus cultures by three sputums sputum on hospital day 15 was positive for acid fast bacilli and she thus remamined in isolation  pulmonary  the patients initial acute respiratory acidosis was corrected with resolution of hypoventilation secondary to carbon dioxide retention and dehydration  in the medicine intensive care unit she was followed by repeated arterial blood gases  the patient denied shortness of breath throughout  she was positive for productive cough  the patient did have an episode of somnolence when supplemental oxygen was turned up secondary to carbon dioxide retention somnolence resolved with supplemental oxygen being decreased she was kept on oxygen approximately 5 to 1 liter to keep saturations between 90 and 95  on hospital day 7 her thoracentesis was performed on a new left effusion with no complications showed white blood cells of 50l red blood cells 2500 differential of 16 neutrophils 33 lymphocytes and 4 monocytes with 3 eosinophils  protein was 17 glucose 157 ldh 73 ph 78  gram stain negative  doctor first name  was negative smear was negative  effusion consistent with a transudative process  on hospital day 12 thoracentesis was performed on her right pleural effusion red blood cells 50 red blood cells of 3200 protein 12 glucose 92 ldh 106 ph 726 gram stain negative  at the time of discharge doctor first name  was still pending smears were negative  cultures were at this time still pending and both pleural effusions were negative for malignant cells  cardiovascular  as noted above the patient was in rapid atrial fibrillation initially  by hospital day 2 and throughout course the patient remained in rate control atrial fibrillation  digoxin lasix and metoprolol were started lisinopril was also restarted  diltiazem was discontinued the patient was found to have a troponin leak  troponin peak was noted to be 146 ck mb ranged between 12 and 3  cks ranged between 124 and 129  it is thought that the high troponin and cks were secondary to myocardial damage suffered through heat stroke  echocardiogram on hospital day 4 showed right atrium markedly dilated mild symmetric left ventricular hypertrophy severe global ventricular hypokinesis severely depressed left ventricular systolic function and pulmonary hypertension there was a question of a marantic endocarditis versus degenerative changes on the aortic valve  there was no significant aortic valve stenosis no aortic regurgitation prior to previous report of 2141812 there was no significant change in regards to left ventricular function  on hospital day 2 the patient underwent repeat echocardiogram which again showed left atrium moderately dilated right atrium markedly dilated with left ventricular wall thickness normal  the overall left ventricular systolic function at this time was considered normal greater than 55  there was mild to moderate mitral regurgitation severe tricuspid regurgitation and again this could not rule out marantic endocarditis though echogenic density is noted on aortic cusps and were consistent with degenerative changes  the patient was subsequently evaluated for infective endocarditis by blood cultures times three and computerized tomography scan of the head looking for signs of emboli  computerized tomography scan of the head results are subsequently pending at the time of this dictation  digoxin level at discharge was 06  it is the recommendation of the medicine staff to restart coumadin at 5 mg approximately two days after discharge at the rehabilitation hospital and then to follow inrs every day until therapeutic range is met  we have very low suspicion for infective endocarditis  heme  coumadin was held secondary to supertherapeutic inr inr peaked at 77  vitamin k times one was given  inr had normalized by hospital day 6  hematocrit was stable at 299 to 34 until hospital day 15 when hematocrit was found to be 275 secondary to occult gastrointestinal bleed  the patient was transfused 1 unit which was well tolerated  follow up hematocrit was 322  hematocrit remained stable for the rest of the hospital course  renal  baseline creatinine is 1 to 14  she arrived with mild prerenal azotemia diuretics were held  during the hospital course she required several boluses for low urine output  the patient started mobilizing fluids approximately hospital day 12  urine outpatient remained good thereafter  fluids electrolytes and nutrition  initial dehydration was treated with intravenous fluids  she tolerated po throughout her placement and was placed on a thin pureed diet with supplemental boost at breakfast lunch and dinner  this was following swallow and nutrition evaluations for malnutrition  she was also placed on thiamine folate and multivitamin supplements  asymptomatic hypercalcemia was noted because of the low setting of albumin 24  free calcium was noted to be 14 free calcium on discharge was noted to be 19  during hospital stay her potassium rose to 55 secondary to increased dosage of ace inhibitor subsequently lisinopril was decreased to 5 mg at which time her potassium normalized  other laboratory studies were as follows b12 79 folate 75 pth 15 25 hydroxy vitamin d less than 7  neurological  change in mental status secondary to heat strokemalnutrition  throughout the hospital course the patients mental status was alert awake and oriented to two sometimes to three  on discharge she is awake alert and oriented times three  head computerized tomography scan on hospital day 20 was performed results are pending at the time of this dictation  gastrointestinal  the patient has a history of heme positive stools with refusal of colonoscopy  she had intermittent guaiac positive stools while inhouse  she was seen by gastrointestinal service for possible occult malignancy versus tuberculosis colitis  colonoscopy was performed on hospital day 20 which showed diverticulosis and two polyps in the right colon that were subsequently removed and biopsied  liver function tests were followed q two weeks secondary to antituberculosis medications  liver function tests after tuberculosis medications were started were within normal limits  alt was 22 alt 23 ldh 342 alkaline phosphatase 158 bilirubin 05  our medical team recommended rehabilitation hospital follow liver function tests q two weeks  endomysal antibody was 3 this was within normal limits  endocrine  free cortisol was noted to be 22 the hyperclcemia was likely due to granulomatous disease tb and improved with treatment of the tb  musculoskeletal  the patient complained early on of hip soreness  xray of the pelvis showed no fractures  the patient was followed by physical therapy  this improved over time  prophylaxis  the with zantac for stomach heparin for deep vein thrombosis and fall and aspiration precautions were maintained throughout the hospital stay  condition on discharge  stable to hospital3 7 for rehabilitation  discharge instructions  maintain respiratory precautions while keeping patient in isolationnegative pressure room the patient will continue to need physical therapy  her diet should be regular plus boost tid   discharge medications 1  isoniazid 300 mg po qd 2  rifampin 600 mg po qd 3  pyrazinamide 2149 mg qd 4  ethambutol 1600 mg po qd 5  pyridoxine 50 mg qd  note that tuberculosis medications should be given on an empty stomach 6  lisinopril 5 mg qd 7  digoxin 025 mg po qd 8  metoprolol 25 mg bid 9  lasix 20 mg po qd 10 aspirin 325 mg po qd 11 heparin 5000 units subcutaneously q 12 hours 12 ranitidine 150 mg po bid 13 nitroglycerin sublingual 04 mg q 5 minutes maximum three doses 14 colace 100 mg po bid prn constipation 15 folic 1 mg po qd 16 thiamine 100 mg po qd 17 multivitamin po qd 18 tylenol 3 one to two tablets po q 6 hours prn pain 19 tylenol 325 mg to 650 mg po q 46 hours prn feverpain  it is recommended that coumadin be restarted on 86 at 5 mg po qd with following inr daily until therapeutic dose is set  the patient will also require liver function tests checked q two weeks  she is due for her next liver function tests next week  the patients digoxin level should be followed q one to two weeks goal digoxon level is 1012  follow up 1  follow up appointment with her primary care physician last name namepattern4  first name4 namepattern1  last name namepattern1 1771 825 3 pm at hospital ward name 23 center on location un 1772 2  pulmonary dr last name stitle  on 819 at 9 am in the hospital1  building location un 1773 3  infectious disease dr last name stitle 1774 215999 am in the doctor last name 780 building location un  4  fall precautions to be maintained the patient can not bear her weight  discharge diagnosis 1  pulmonary tuberculosis reactivated 2  bilateral pleural effusions 3  atrial fibrillation 4  cardiomyopathy 5  left bundle branch block 6  hypercalcemia resolved 7  ho cva 8  ho pe                             first name8 namepattern2  first name4 namepattern1 1775 last name namepattern1  md  md number1 1776  dictated bylast name namepattern1 1777 medquist36  d  215184  1552 t  215184  1610 job  job number 1778  ']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of notes in NOTEEVENTS.csv = {count}\")\n",
        "print(admidic['167853'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-cDy3beDLjk"
      },
      "source": [
        "Calculate the word ocurrence (including stop words) in NOTEEVENTS.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHvu1UZbDc_Z"
      },
      "outputs": [],
      "source": [
        "u=defaultdict(int) # The default count for each word is 0\n",
        "for i in admidic: # for each visit\n",
        "  for jj in admidic[i]: # for each saved note\n",
        "    line=jj.strip('\\n').split() # split into a list of words\n",
        "    for j in line: # Iterate each word\n",
        "      u[j]=u[j]+1 # count the number of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hfrS-9ENB6"
      },
      "source": [
        "Print the number of words (vocabulary) in NOTEEVENTS.csv.  \n",
        "Print the ocurrence of the word \"consistent\" in NOTEEVENTS.csv.  \n",
        "Check if the stop word \"the\" in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvhIGKugD0rb",
        "outputId": "9e67c9c3-1770-4dda-9767-bd1f1066f7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in NOTEEVENTS.csv = 529818\n",
            "35831\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of words in NOTEEVENTS.csv = {len(u)}\")\n",
        "print(u[\"consistent\"])\n",
        "print(\"the\" in u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OiaRFg8E3Ep"
      },
      "source": [
        "Remove the stopwords.  \n",
        "Remove the words whose number of occurence is less than 10.  \n",
        "From paper: \"During preprocessing we lowercased all\n",
        "tokens and removed punctuations, stop words, words containing\n",
        "only digits, and words whose frequency is less than 10\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhdazHeWFFBg"
      },
      "outputs": [],
      "source": [
        "u2=defaultdict(int) # Create a new dict to filter out some words\n",
        "for i in u: # iterate each word\n",
        "  if i.isdigit()==False: # Make sure not a number\n",
        "    if u[i]>10: # Make sure the word occurence is higher than 10\n",
        "      if i not in stop_words: # Make sure not stop words\n",
        "        u2[i]=u[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3WioX6eFW3C"
      },
      "source": [
        "Print the number of words (vocabulary) in NOTEEVENTS.csv AFTER the removal of stopwords (From paper: \"The final\n",
        "word vocabulary contains 47,965 unique words.\").  \n",
        "Check if the stop word \"the\" in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTGa6IooFTBV",
        "outputId": "e069046e-40d2-4721-98f8-573e2d56794a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in NOTEEVENTS.csv = 47964\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of words in NOTEEVENTS.csv = {len(u2)}\")\n",
        "print(\"the\" in u2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIe0nf07Ij7S"
      },
      "source": [
        "Create a dictionary for DIAGNOSES_ICD.csv. The key is HADM_ID, and the value if a list of ICD-9 codes for HADM_ID.  \n",
        "Add a prefix \"d_\" to the ICD-9 codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkbht_bfHum5"
      },
      "outputs": [],
      "source": [
        "u=[]   \n",
        "\n",
        "file1=codecs.open('DIAGNOSES_ICD.csv','r')\n",
        "ad2c=defaultdict(list)\n",
        "line=file1.readline() # Skip the 1st line\n",
        "line=file1.readline() # Read the 2nd line\n",
        "\n",
        "while line:\n",
        "  line=line.strip().split(',') # Split a row into a list\n",
        "\n",
        "  if line[4][1:-1]!='': # If ICD9_CODE column is not empty\n",
        "    ad2c[line[2]].append(\"d_\"+line[4][1:-1]) # Append the code to list of codes for a HADM_ID\n",
        "  \n",
        "  line=file1.readline() # Read the next line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98bQUxEJGjl"
      },
      "source": [
        "Print a sample key-value pair (ICD-9 codes for visit 172335)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TygSO_osJPFP",
        "outputId": "55d52456-bf80-40b5-952c-e971263a1a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['d_40301', 'd_486', 'd_58281', 'd_5855', 'd_4254', 'd_2762', 'd_7100', 'd_2767', 'd_7243', 'd_45829', 'd_2875', 'd_28521', 'd_28529', 'd_27541']\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "print(ad2c[\"172335\"])\n",
        "print(len(ad2c[\"172335\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUuWJPrmKRhv"
      },
      "source": [
        "Calculate the code ocurrence in DIAGNOSES_ICD.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFsluq1iKNpB"
      },
      "outputs": [],
      "source": [
        "codeu=defaultdict(int)\n",
        "for i in ad2c:\n",
        "  for j in ad2c[i]:\n",
        "    codeu[j]=codeu[j]+1 # counter the occurence of codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GybgwUEBK2DI"
      },
      "source": [
        "Print the number of unique ICD-9 codes (original codes) in DIAGNOSES_ICD.csv.  \n",
        "Print the number of occurence for a code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4K06NaIKkdn",
        "outputId": "64bfd289-5a45-44c7-be2e-8e508d4ce21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of codes in DIAGNOSES_ICD.csv = 6984\n",
            "4839\n"
          ]
        }
      ],
      "source": [
        "print(f\"number of codes in DIAGNOSES_ICD.csv = {len(codeu)}\")\n",
        "print(codeu[\"d_486\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO0Dyqtc_aaa"
      },
      "source": [
        "Group ICD-9 codes in DIAGNOSES_ICD.csv by the first 3 letters (From paper: \"We extracted all\n",
        "listed ICD-9 diagnosis codes for each visit and grouped them by\n",
        "their first three digits\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j6BW_VA9M7j"
      },
      "outputs": [],
      "source": [
        "ad2c2 = defaultdict(list)\n",
        "for hadm_id in ad2c:\n",
        "  for code in ad2c[hadm_id]:\n",
        "    if code[0:5] not in ad2c2[hadm_id]:\n",
        "      ad2c2[hadm_id].append(code[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTpz739_mn6"
      },
      "source": [
        "Print a sample key-value pair (same as the one before ICD-9 code grouping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cScwSxfM_GCb",
        "outputId": "0d960020-6939-49d2-81e7-8e50107af7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['d_403', 'd_486', 'd_582', 'd_585', 'd_425', 'd_276', 'd_710', 'd_724', 'd_458', 'd_287', 'd_285', 'd_275']\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "print(ad2c2[\"172335\"])\n",
        "print(len(ad2c2[\"172335\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGWvVPCw_xrn"
      },
      "source": [
        "Calculate the code ocurrence in DIAGNOSES_ICD.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR1NS10fQs6z"
      },
      "outputs": [],
      "source": [
        "codeu2=defaultdict(int)\n",
        "for hadm_id in ad2c2:\n",
        "  for code in ad2c2[hadm_id]:\n",
        "    codeu2[code]=codeu2[code]+1 # counter the occurence of codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa_vMKj0AEOb"
      },
      "source": [
        "Print the number of unique ICD-9 codes (after grouping) in DIAGNOSES_ICD.csv (From paper: \"The code vocabulary contains 942 codes.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80bKVTDc_Q5W",
        "outputId": "16e2abfb-4dad-49dc-a910-0e388669e157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of codes after grouping = 942\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of codes after grouping = {len(codeu2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCjfDnUNifD"
      },
      "source": [
        "Iterate HADM_ID in IDlist.npy, and combine the code data (from DIAGNOSES_ICD.csv) and note data (from NOTEEVENTS.csv) into a single file \"combined_dataset\".  \n",
        "**combined_dataset**: A dataset contains the clinical notes of hospital visits and corresponding diagnosis (ICD-9 codes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31lmsgy23wPv"
      },
      "outputs": [],
      "source": [
        "fileo=codecs.open(\"combined_dataset\",'w')\n",
        "\n",
        "num_of_notes = 0\n",
        "lower_limit = 0\n",
        "upper_limit = 10000000\n",
        "\n",
        "IDlist=np.load('IDlist.npy',encoding='bytes').astype(str) # a list of HADM_ID\n",
        "for hadm_id in IDlist:\n",
        "  if ad2c2[hadm_id]!=[]: # If HADM_ID exists in ad2c\n",
        "    add_to_set = False\n",
        "    for code in ad2c2[hadm_id]:\n",
        "      if codeu2[code] >= lower_limit and codeu2[code] <= upper_limit:\n",
        "        add_to_set = True\n",
        "    if add_to_set:\n",
        "      fileo.write('start! '+i+'\\n')\n",
        "      fileo.write('codes: ')\n",
        "      tempc=[]\n",
        "      for code in ad2c2[hadm_id]: # for each code\n",
        "        if codeu2[code] >= lower_limit and codeu2[code] <= upper_limit: # if code occurence greater than threshold\n",
        "          if code not in tempc:\n",
        "            tempc.append(code) # save d_ and first 3 digits of ICD-9 code\n",
        "      \n",
        "      for code in tempc:\n",
        "        fileo.write(code+\" \") # write code to combined dataset\n",
        "      fileo.write('\\n')\n",
        "      fileo.write('notes:\\n') # write note\n",
        "      for line in admidic[hadm_id]: # iterate each line    \n",
        "        thisline=line.strip('\\n').split() \n",
        "        for j in thisline: # iterate each word\n",
        "          if u2[j]!=0: # if this word is a qualified word\n",
        "            fileo.write(j+\" \") # write this word\n",
        "          fileo.write('\\n')\n",
        "      fileo.write('end!\\n')\n",
        "      num_of_notes = num_of_notes + 1\n",
        "fileo.close()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olb3AGDhEI-x"
      },
      "source": [
        "Print the number of note-code pairs added to the dataset (From paper:\"The number of aggregated discharge summary notes is 52,722..\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhzMBfP7D_qe",
        "outputId": "6c03e19c-3f76-4ca4-aaaa-252c9d665dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of notes written to combined dataset = 52722\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of notes written to combined dataset = {num_of_notes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZDh7aDeD_fY",
        "outputId": "db4fd4b0-a42f-4a5b-8ce2-e54f69154e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 01m 49s\n"
          ]
        }
      ],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COK-Hrv5-knl"
      },
      "source": [
        "# Data Pre-processing 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrbstc1758MT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLfX0hlBGsBK"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2ORuVJIlnU"
      },
      "source": [
        "Build a vocabulary for Wiki documents. The key is a word, and the value is 1 if the word exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5kSmQc3ImLq"
      },
      "outputs": [],
      "source": [
        "wikivocab={}\n",
        "file1=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file1.readline()\n",
        "while line:\n",
        "  if line[0:3]!='XXX': # if this line is not start or end of doc\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    for i in line: # iterate each word\n",
        "      wikivocab[i.lower()]=1 # if a word exists in doc, set to 1\n",
        "  line=file1.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxyybgFLJgNK"
      },
      "source": [
        "Print the size of the vocabulary for Wiki documents (From paper: \"The size\n",
        "of the word vocabulary of Wikipedia documents is 60,968.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4tCwHxXJDwi",
        "outputId": "fb4fa043-431b-461b-dd4f-04e517a22ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in Wiki documents = 60968\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of unique words in Wiki documents = {len(wikivocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOo0A3-9KEAC"
      },
      "source": [
        "Build a vocabulary for NOTEEVENTS.csv (after word processing). The key is a word, and the value is 1 if the word exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In7y-or4KWE8"
      },
      "outputs": [],
      "source": [
        "notesvocab={}\n",
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  \n",
        "  if line[0]=='codes:':\n",
        "    line=filec.readline()\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    \n",
        "    if line[0]=='notes:': \n",
        "      line=filec.readline()\n",
        "      while line!='end!\\n':\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for word in line:\n",
        "          notesvocab[word]=1\n",
        "        line=filec.readline()            \n",
        "  line=filec.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPXbUzq6LcRR"
      },
      "source": [
        "Print the size of vocabulary for NOTEEVENTS.csv after word processing (From paper: \"The final word vocabulary contains 47,965 unique words.\").  \n",
        "Print a sample key-value pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtN8IgIZK_pY",
        "outputId": "643767d1-c22f-48c4-becd-de3eae1238ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47964\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(len(notesvocab))\n",
        "print(notesvocab[\"consistent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3VGW7dkL_PE"
      },
      "source": [
        "Get an intersection of the vocabulary for wiki documents and the one for NOTEVENTS.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9JMMHhiMITq"
      },
      "outputs": [],
      "source": [
        "a1=set(notesvocab)\n",
        "a2=set(wikivocab)\n",
        "a3=a1.intersection(a2) # get the intersection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ladA4oybMY73"
      },
      "source": [
        "Print number of unique words in the intersection (From paper: \"The size of the word vocabulary of Wikipedia documents is 60,968, out of which only 12,173 are also in the word vocabulary of MIMIC-III clinical notes.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcJK3dukMeYS",
        "outputId": "57490e52-80d9-458d-a241-8165dfedc28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12173\n"
          ]
        }
      ],
      "source": [
        "print(len(a3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZxImH0NdDf"
      },
      "source": [
        "Create a list of lists. Each element list is a list of intersected words for one Wiki document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHj2uu7SNaQn"
      },
      "outputs": [],
      "source": [
        "wikidocuments=[] # a list of lists, each element is a list of intersected and filtered words for one doc\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline()\n",
        "while line:\n",
        "  if line[0:4]=='XXXd': # Check if it is a start of a wiki doc\n",
        "    tempf=[]\n",
        "    line=file2.readline() # read the next line\n",
        "    while line[0:4]!='XXXe': # keep reading until the end of that doc\n",
        "      line=line.strip('\\n')\n",
        "      words=line.split()\n",
        "      for word in words:\n",
        "        if word.lower() in a3: # if this word also appears in note\n",
        "          tempf.append(word.lower()) # add this word to the list for this doc\n",
        "      line=file2.readline()\n",
        "    wikidocuments.append(tempf) # add word list of doc to list of docs \n",
        "  line=file2.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSeBdRaHOF3p"
      },
      "source": [
        "Print a sample element list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLfOCUqBN7o1",
        "outputId": "be7dcbb5-14c7-4f80-ce9f-5cce0881cfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['breast', 'cancer', 'breast', 'cancer', 'cancer', 'develops', 'breast', 'signs', 'breast', 'cancer', 'may', 'include', 'lump', 'change', 'breast', 'dimpling', 'fluid', 'coming', 'newly', 'inverted', 'red', 'scaly', 'patch', 'distant', 'spread', 'may', 'bone', 'swollen', 'lymph', 'shortness', 'yellow', 'risk', 'factors', 'developing', 'breast', 'cancer', 'include', 'lack', 'physical', 'drinking', 'hormone', 'replacement', 'therapy', 'early', 'age', 'first', 'children', 'late', 'older', 'prior', 'history', 'breast', 'family', 'cases', 'due', 'inherited', 'including', 'among', 'breast', 'cancer', 'commonly', 'develops', 'cells', 'lining', 'milk', 'ducts', 'lobules', 'supply', 'ducts', 'cancers', 'developing', 'ducts', 'known', 'ductal', 'developing', 'lobules', 'known', 'lobular', 'breast', 'ductal', 'carcinoma', 'develop', 'diagnosis', 'breast', 'cancer', 'confirmed', 'taking', 'biopsy', 'concerning', 'diagnosis', 'tests', 'done', 'determine', 'cancer', 'spread', 'beyond', 'breast', 'treatments', 'likely', 'balance', 'benefits', 'versus', 'harms', 'breast', 'cancer', 'screening', 'review', 'stated', 'unclear', 'screening', 'good', 'review', 'us', 'preventive', 'services', 'task', 'force', 'found', 'evidence', 'benefit', 'years', 'organization', 'recommends', 'screening', 'every', 'two', 'years', 'women', 'years', 'medications', 'tamoxifen', 'raloxifene', 'may', 'used', 'effort', 'prevent', 'breast', 'cancer', 'high', 'risk', 'developing', 'surgical', 'removal', 'breasts', 'another', 'preventative', 'measure', 'high', 'risk', 'diagnosed', 'number', 'treatments', 'may', 'including', 'radiation', 'hormonal', 'therapy', 'targeted', 'types', 'surgery', 'vary', 'surgery', 'breast', 'reconstruction', 'may', 'take', 'place', 'time', 'surgery', 'later', 'cancer', 'spread', 'parts', 'treatments', 'mostly', 'aimed', 'improving', 'quality', 'life', 'outcomes', 'breast', 'cancer', 'vary', 'depending', 'cancer', 'extent', 'survival', 'rates', 'developed', 'world', 'united', 'states', 'alive', 'least', 'developing', 'countries', 'survival', 'rates', 'breast', 'cancer', 'leading', 'type', 'cancer', 'accounting', 'resulted', 'million', 'new', 'cases', 'common', 'developed', 'countries', 'times', 'common', 'women', 'first', 'noticeable', 'symptom', 'breast', 'cancer', 'typically', 'lump', 'feels', 'different', 'rest', 'breast', 'breast', 'cancer', 'cases', 'discovered', 'woman', 'feels', 'earliest', 'breast', 'cancers', 'detected', 'lumps', 'found', 'lymph', 'nodes', 'located', 'armpits', 'can', 'also', 'indicate', 'breast', 'indications', 'breast', 'cancer', 'lump', 'may', 'include', 'thickening', 'different', 'breast', 'one', 'breast', 'becoming', 'larger', 'nipple', 'changing', 'position', 'shape', 'becoming', 'skin', 'puckering', 'rash', 'around', 'discharge', 'constant', 'pain', 'part', 'breast', 'swelling', 'beneath', 'armpit', 'around', 'pain', 'unreliable', 'tool', 'determining', 'presence', 'absence', 'breast', 'may', 'indicative', 'breast', 'health', 'inflammatory', 'breast', 'cancer', 'particular', 'type', 'breast', 'cancer', 'can', 'pose', 'substantial', 'diagnostic', 'symptoms', 'may', 'resemble', 'breast', 'inflammation', 'may', 'include', 'nipple', 'warmth', 'redness', 'throughout', 'well', 'texture', 'skin', 'referred', 'inflammatory', 'breast', 'cancer', 'present', 'lump', 'can', 'sometimes', 'delay', 'another', 'reported', 'symptom', 'complex', 'breast', 'cancer', 'disease', 'syndrome', 'presents', 'skin', 'changes', 'resembling', 'mild', 'flaking', 'nipple', 'disease', 'breast', 'symptoms', 'may', 'include', 'increased', 'may', 'also', 'discharge', 'approximately', 'half', 'women', 'diagnosed', 'disease', 'breast', 'also', 'lump', 'rare', 'initially', 'appears', 'fibroadenoma', 'fact', 'tumors', 'formed', 'within', 'stroma', 'breast', 'contain', 'glandular', 'well', 'stromal', 'tumors', 'staged', 'usual', 'classified', 'basis', 'appearance', 'microscope', 'breast', 'cancer', 'presents', 'metastatic', 'cancer', 'spread', 'beyond', 'original', 'symptoms', 'caused', 'metastatic', 'breast', 'cancer', 'will', 'depend', 'location', 'common', 'sites', 'metastasis', 'include', 'lung', 'unexplained', 'weight', 'loss', 'can', 'occasionally', 'signal', 'breast', 'can', 'symptoms', 'fevers', 'bone', 'joint', 'pains', 'can', 'sometimes', 'manifestations', 'metastatic', 'breast', 'can', 'jaundice', 'neurological', 'symptoms', 'called', 'meaning', 'manifestations', 'many', 'symptoms', 'breast', 'including', 'turn', 'represent', 'underlying', 'breast', 'fewer', 'benign', 'breast', 'diseases', 'mastitis', 'fibroadenoma', 'breast', 'common', 'causes', 'breast', 'disorder', 'risk', 'factors', 'can', 'divided', 'two', 'primary', 'risk', 'factors', 'breast', 'cancer', 'female', 'older', 'potential', 'risk', 'factors', 'include', 'lack', 'lack', 'higher', 'levels', 'certain', 'certain', 'dietary', 'one', 'study', 'indicates', 'exposure', 'light', 'risk', 'factor', 'development', 'breast', 'obesity', 'drinking', 'alcoholic', 'beverages', 'among', 'common', 'modifiable', 'risk', 'smoking', 'tobacco', 'appears', 'increase', 'risk', 'breast', 'greater', 'amount', 'smoked', 'earlier', 'life', 'smoking', 'higher', 'risk', 'increased', 'lack', 'physical', 'activity', 'linked', 'sitting', 'regularly', 'prolonged', 'periods', 'associated', 'higher', 'mortality', 'breast', 'risk', 'regular', 'though', 'association', 'use', 'hormonal', 'birth', 'control', 'development', 'breast', 'whether', 'oral', 'contraceptives', 'use', 'may', 'actually', 'cause', 'breast', 'cancer', 'matter', 'indeed', 'absolute', 'effect', 'clear', 'association', 'exists', 'newer', 'hormonal', 'birth', 'mutations', 'breast', 'cancer', 'susceptibility', 'family', 'history', 'breast', 'use', 'oral', 'contraceptives', 'appear', 'affect', 'risk', 'breast', 'association', 'breast', 'feeding', 'breast', 'cancer', 'clearly', 'studies', 'found', 'support', 'association', 'others', 'cancer', 'hypothesis', 'induced', 'abortion', 'increased', 'risk', 'developing', 'breast', 'hypothesis', 'subject', 'extensive', 'concluded', 'neither', 'miscarriages', 'abortions', 'associated', 'heightened', 'risk', 'breast', 'number', 'dietary', 'factors', 'linked', 'risk', 'breast', 'drinking', 'alcoholic', 'beverages', 'increases', 'risk', 'breast', 'even', 'relatively', 'low', 'three', 'drinks', 'per', 'moderate', 'risk', 'highest', 'among', 'heavy', 'dietary', 'factors', 'may', 'increase', 'risk', 'include', 'diet', 'high', 'cholesterol', 'dietary', 'iodine', 'deficiency', 'may', 'also', 'play', 'evidence', 'fiber', 'review', 'found', 'studies', 'trying', 'link', 'fiber', 'intake', 'breast', 'cancer', 'produced', 'mixed', 'tentative', 'association', 'low', 'fiber', 'intake', 'adolescence', 'breast', 'cancer', 'risk', 'factors', 'include', 'radiation', 'number', 'chemicals', 'also', 'including', 'organic', 'solvents', 'although', 'radiation', 'mammography', 'low', 'estimated', 'yearly', 'screening', 'years', 'age', 'will', 'cause', 'approximately', 'cases', 'fatal', 'breast', 'cancer', 'per', 'million', 'women', 'genetic', 'susceptibility', 'may', 'play', 'minor', 'role', 'genetics', 'believed', 'primary', 'cause', 'women', 'whose', 'mother', 'diagnosed', 'increased', 'risk', 'whose', 'mother', 'diagnosed', 'age', 'increased', 'risk', 'one', 'two', 'affected', 'risk', 'breast', 'cancer', 'age', 'subsequent', 'mortality', 'disease', 'first', 'degree', 'relative', 'disease', 'risk', 'breast', 'cancer', 'age', 'double', 'general', 'less', 'genetics', 'plays', 'significant', 'role', 'causing', 'hereditary', 'cancer', 'includes', 'carry', 'gene', 'mutations', 'account', 'total', 'genetic', 'influence', 'risk', 'breast', 'cancer', 'significant', 'mutations', 'include', 'said', 'four', 'distinct', 'types', 'breast', 'cancer', 'genetic', 'changes', 'lead', 'many', 'breast', 'changes', 'like', 'atypical', 'ductal', 'hyperplasia', 'lobular', 'carcinoma', 'found', 'benign', 'breast', 'conditions', 'fibrocystic', 'breast', 'correlated', 'increased', 'breast', 'cancer', 'diabetes', 'mellitus', 'might', 'also', 'increase', 'risk', 'breast', 'autoimmune', 'diseases', 'lupus', 'erythematosus', 'seem', 'also', 'increase', 'risk', 'acquisition', 'breast', 'breast', 'like', 'occurs', 'interaction', 'environmental', 'factor', 'susceptible', 'normal', 'cells', 'divide', 'many', 'times', 'needed', 'attach', 'cells', 'stay', 'place', 'cells', 'become', 'cancerous', 'lose', 'ability', 'stop', 'attach', 'stay', 'die', 'proper', 'normal', 'cells', 'will', 'commit', 'cell', 'suicide', 'cell', 'longer', 'protected', 'cell', 'suicide', 'several', 'protein', 'clusters', 'one', 'protective', 'pathways', 'another', 'sometimes', 'along', 'protective', 'pathways', 'mutated', 'way', 'turns', 'permanently', 'rendering', 'cell', 'incapable', 'suicide', 'longer', 'one', 'steps', 'causes', 'cancer', 'combination', 'protein', 'turns', 'pathway', 'cell', 'ready', 'programmed', 'cell', 'breast', 'gene', 'protein', 'pathway', 'stuck', 'cancer', 'cell', 'commit', 'mutations', 'can', 'lead', 'breast', 'cancer', 'linked', 'estrogen', 'abnormal', 'growth', 'factor', 'interaction', 'stromal', 'cells', 'epithelial', 'cells', 'can', 'facilitate', 'malignant', 'cell', 'breast', 'adipose', 'leads', 'increased', 'cell', 'proliferation', 'united', 'percent', 'people', 'breast', 'cancer', 'people', 'ovarian', 'cancer', 'relative', 'one', 'familial', 'tendency', 'develop', 'cancers', 'called', 'hereditary', 'cancer', 'best', 'known', 'confer', 'lifetime', 'risk', 'breast', 'cancer', 'percent', 'lifetime', 'risk', 'ovarian', 'cancer', 'mutations', 'associated', 'occur', 'mechanisms', 'correct', 'errors', 'mutations', 'either', 'inherited', 'acquired', 'allow', 'allow', 'uncontrolled', 'lack', 'metastasis', 'distant', 'strong', 'evidence', 'residual', 'risk', 'variation', 'goes', 'well', 'beyond', 'hereditary', 'gene', 'mutations', 'carrier', 'caused', 'risk', 'environmental', 'causes', 'triggers', 'breast', 'inherited', 'mutation', 'can', 'interfere', 'repair', 'dna', 'cross', 'dna', 'double', 'strand', 'breaks', 'functions', 'cause', 'dna', 'damage', 'dna', 'cross', 'double', 'strand', 'breaks', 'often', 'require', 'repairs', 'pathways', 'containing', 'mutations', 'account', 'percent', 'breast', 'say', 'cancer', 'may', 'inevitable', 'half', 'hereditary', 'cancer', 'syndromes', 'involve', 'unknown', 'directly', 'controls', 'expression', 'estrogen', 'receptor', 'associated', 'epithelial', 'loss', 'leads', 'loss', 'differentiation', 'poor', 'prognosis', 'due', 'cancer', 'cell', 'invasion', 'types', 'breast', 'cancer', 'easy', 'diagnose', 'microscopic', 'analysis', 'affected', 'area', 'types', 'breast', 'cancer', 'require', 'specialized', 'lab', 'two', 'commonly', 'used', 'screening', 'physical', 'examination', 'breasts', 'healthcare', 'provider', 'can', 'offer', 'approximate', 'likelihood', 'lump', 'may', 'also', 'detect', 'simple', 'examinations', 'healthcare', 'provider', 'can', 'remove', 'sample', 'fluid', 'lump', 'microscopic', 'analysis', 'procedure', 'known', 'fine', 'needle', 'fine', 'needle', 'aspiration', 'help', 'establish', 'needle', 'aspiration', 'may', 'performed', 'healthcare', 'office', 'clinic', 'using', 'local', 'finding', 'clear', 'fluid', 'makes', 'lump', 'highly', 'unlikely', 'bloody', 'fluid', 'may', 'sent', 'inspection', 'microscope', 'cancerous', 'physical', 'examination', 'can', 'used', 'diagnose', 'breast', 'cancer', 'good', 'degree', 'options', 'biopsy', 'include', 'core', 'biopsy', 'breast', 'procedures', 'section', 'breast', 'lump', 'excisional', 'entire', 'lump', 'often', 'results', 'physical', 'examination', 'healthcare', 'additional', 'tests', 'may', 'performed', 'special', 'circumstances', 'imaging', 'ultrasound', 'sufficient', 'warrant', 'excisional', 'biopsy', 'definitive', 'diagnostic', 'primary', 'treatment', 'breast', 'cancers', 'classified', 'several', 'prognosis', 'can', 'affect', 'treatment', 'description', 'breast', 'cancer', 'optimally', 'includes', 'women', 'can', 'reduce', 'risk', 'breast', 'cancer', 'maintaining', 'healthy', 'reducing', 'alcohol', 'increasing', 'physical', 'modifications', 'might', 'prevent', 'breast', 'cancers', 'benefits', 'moderate', 'exercise', 'brisk', 'walking', 'seen', 'age', 'groups', 'including', 'postmenopausal', 'high', 'levels', 'physical', 'activity', 'reduce', 'risk', 'breast', 'cancer', 'strategies', 'encourage', 'regular', 'physical', 'activity', 'reduce', 'obesity', 'also', 'reduced', 'risks', 'cardiovascular', 'disease', 'high', 'intake', 'citrus', 'fruit', 'associated', 'reduction', 'risk', 'breast', 'marine', 'fatty', 'acids', 'appear', 'reduce', 'high', 'consumption', 'foods', 'may', 'reduce', 'removal', 'breasts', 'cancer', 'diagnosed', 'suspicious', 'lump', 'lesion', 'appeared', 'procedure', 'known', 'prophylactic', 'bilateral', 'may', 'considered', 'people', 'associated', 'substantially', 'heightened', 'risk', 'eventual', 'diagnosis', 'breast', 'evidence', 'strong', 'enough', 'support', 'procedure', 'anyone', 'highest', 'brca', 'testing', 'recommended', 'high', 'family', 'risk', 'genetic', 'recommended', 'many', 'forms', 'changes', 'ranging', 'obviously', 'dangerous', 'effect', 'identifiable', 'changes', 'testing', 'person', 'particularly', 'likely', 'return', 'one', 'unclear', 'removing', 'second', 'breast', 'breast', 'cancer', 'one', 'selective', 'estrogen', 'receptor', 'reduce', 'risk', 'breast', 'cancer', 'increase', 'risk', 'thromboembolism', 'endometrial', 'overall', 'change', 'risk', 'thus', 'recommended', 'prevention', 'breast', 'cancer', 'women', 'average', 'risk', 'may', 'offered', 'high', 'benefit', 'breast', 'cancer', 'reduction', 'continues', 'least', 'five', 'years', 'stopping', 'course', 'treatment', 'breast', 'cancer', 'screening', 'refers', 'testing', 'women', 'breast', 'cancer', 'attempt', 'achieve', 'earlier', 'diagnosis', 'assumption', 'early', 'detection', 'will', 'improve', 'number', 'screening', 'tests', 'employed', 'including', 'clinical', 'self', 'breast', 'genetic', 'magnetic', 'resonance', 'clinical', 'self', 'breast', 'exam', 'involves', 'feeling', 'breast', 'lumps', 'clinical', 'breast', 'exams', 'performed', 'health', 'care', 'exams', 'performed', 'person', 'evidence', 'support', 'effectiveness', 'either', 'type', 'breast', 'time', 'lump', 'large', 'enough', 'found', 'likely', 'growing', 'several', 'years', 'thus', 'soon', 'large', 'enough', 'found', 'without', 'screening', 'breast', 'cancer', 'uses', 'examine', 'breast', 'uncharacteristic', 'masses', 'breast', 'compressed', 'technician', 'takes', 'multiple', 'general', 'mammogram', 'takes', 'entire', 'diagnostic', 'mammogram', 'focuses', 'specific', 'lump', 'area', 'number', 'national', 'bodies', 'recommend', 'breast', 'cancer', 'average', 'preventive', 'services', 'task', 'force', 'recommends', 'mammography', 'every', 'two', 'years', 'women', 'ages', 'council', 'europe', 'recommends', 'mammography', 'programs', 'using', 'screening', 'recommended', 'ages', 'frequency', 'task', 'force', 'reports', 'point', 'addition', 'unnecessary', 'surgery', 'risks', 'frequent', 'mammograms', 'include', 'small', 'significant', 'increase', 'breast', 'cancer', 'induced', 'collaboration', 'states', 'best', 'quality', 'evidence', 'neither', 'demonstrates', 'reduction', 'cancer', 'reduction', 'cause', 'mortality', 'screening', 'less', 'rigorous', 'trials', 'added', 'analysis', 'reduction', 'mortality', 'due', 'breast', 'cancer', 'decrease', 'deaths', 'breast', 'cancer', 'years', 'relative', 'decrease', 'breast', 'screening', 'years', 'results', 'increase', 'rates', 'per', 'half', 'will', 'least', 'one', 'falsely', 'positive', 'resulted', 'view', 'clear', 'whether', 'mammography', 'screening', 'good', 'states', 'due', 'recent', 'improvements', 'breast', 'cancer', 'risks', 'false', 'positives', 'breast', 'cancer', 'screening', 'leading', 'unnecessary', 'therefore', 'longer', 'seems', 'beneficial', 'attend', 'breast', 'cancer', 'whether', 'mri', 'screening', 'method', 'greater', 'harms', 'benefits', 'compared', 'standard', 'mammography', 'management', 'breast', 'cancer', 'depends', 'various', 'including', 'stage', 'cancer', 'treatments', 'aggressive', 'prognosis', 'worse', 'higher', 'risk', 'recurrence', 'cancer', 'following', 'breast', 'cancer', 'usually', 'treated', 'may', 'followed', 'chemotherapy', 'radiation', 'multidisciplinary', 'approach', 'hormone', 'cancers', 'often', 'treated', 'therapy', 'courses', 'several', 'monoclonal', 'may', 'administered', 'certain', 'cases', 'metastatic', 'advanced', 'stages', 'breast', 'surgery', 'involves', 'physical', 'removal', 'typically', 'along', 'surrounding', 'one', 'lymph', 'nodes', 'may', 'biopsied', 'increasingly', 'lymph', 'node', 'sampling', 'performed', 'sentinel', 'lymph', 'node', 'standard', 'surgeries', 'tumor', 'person', 'breast', 'reconstruction', 'type', 'plastic', 'may', 'performed', 'improve', 'appearance', 'treated', 'women', 'use', 'breast', 'prostheses', 'breast', 'choose', 'flat', 'nipple', 'prosthesis', 'can', 'used', 'time', 'following', 'drugs', 'used', 'addition', 'surgery', 'called', 'adjuvant', 'chemotherapy', 'types', 'therapy', 'prior', 'surgery', 'called', 'neoadjuvant', 'aspirin', 'may', 'reduce', 'mortality', 'breast', 'currently', 'three', 'main', 'groups', 'medications', 'used', 'adjuvant', 'breast', 'cancer', 'monoclonal', 'hormone', 'blocking', 'therapy', 'chemotherapy', 'monoclonal', 'antibodies', 'radiotherapy', 'given', 'surgery', 'region', 'tumor', 'bed', 'regional', 'lymph', 'microscopic', 'tumor', 'cells', 'may', 'may', 'also', 'beneficial', 'effect', 'tumor', 'radiation', 'therapy', 'can', 'delivered', 'external', 'beam', 'radiotherapy', 'brachytherapy', 'radiotherapy', 'given', 'operation', 'breast', 'radiation', 'can', 'also', 'given', 'time', 'operation', 'breast', 'radiation', 'can', 'reduce', 'risk', 'recurrence', 'reduction', 'delivered', 'correct', 'dose', 'considered', 'essential', 'breast', 'cancer', 'treated', 'removing', 'lump', 'wide', 'local', 'stage', 'breast', 'cancer', 'important', 'component', 'traditional', 'classification', 'methods', 'breast', 'greater', 'effect', 'prognosis', 'staging', 'takes', 'consideration', 'local', 'lymph', 'node', 'status', 'whether', 'metastatic', 'disease', 'higher', 'stage', 'poorer', 'stage', 'raised', 'disease', 'lymph', 'chest', 'skin', 'aggressiveness', 'cancer', 'stage', 'lowered', 'presence', 'zones', 'cell', 'behaviour', 'size', 'factor', 'staging', 'unless', 'cancer', 'ductal', 'carcinoma', 'situ', 'involving', 'entire', 'breast', 'will', 'still', 'stage', 'zero', 'consequently', 'excellent', 'prognosis', 'disease', 'free', 'survival', 'breast', 'cancer', 'grade', 'assessed', 'comparison', 'breast', 'cancer', 'cells', 'normal', 'breast', 'closer', 'normal', 'cancer', 'cells', 'slower', 'growth', 'better', 'cells', 'well', 'will', 'appear', 'will', 'divide', 'will', 'tend', 'well', 'differentiated', 'given', 'grade', 'moderate', 'grade', 'poor', 'undifferentiated', 'given', 'higher', 'grade', 'upon', 'scale', 'widely', 'used', 'system', 'younger', 'women', 'age', 'less', 'years', 'women', 'years', 'tend', 'poorer', 'prognosis', 'women', 'due', 'several', 'breasts', 'may', 'change', 'menstrual', 'may', 'nursing', 'may', 'unaware', 'changes', 'younger', 'women', 'usually', 'advanced', 'stage', 'may', 'also', 'biologic', 'factors', 'contributing', 'higher', 'risk', 'disease', 'recurrence', 'younger', 'women', 'breast', 'people', 'breast', 'cancer', 'experience', 'illness', 'factors', 'age', 'can', 'significant', 'impact', 'way', 'person', 'breast', 'cancer', 'women', 'positive', 'breast', 'cancer', 'must', 'issues', 'early', 'menopause', 'induced', 'many', 'chemotherapy', 'regimens', 'used', 'treat', 'breast', 'especially', 'use', 'hormones', 'counteract', 'ovarian', 'breast', 'cancer', 'common', 'invasive', 'cancer', 'affects', 'women', 'common', 'form', 'cancer', 'skin', 'cancers', 'generally', 'easily', 'cause', 'routinely', 'excluded', 'cancer', 'breast', 'cancer', 'invasive', 'cancers', 'women', 'female', 'comprised', 'cancers', 'diagnosed', 'making', 'common', 'female', 'breast', 'cancer', 'caused', 'deaths', 'cancer', 'deaths', 'women', 'cancer', 'deaths', 'men', 'women', 'lung', 'second', 'common', 'cause', 'death', 'caused', 'cancer', 'deaths', 'women', 'cancer', 'deaths', 'men', 'women', 'incidence', 'breast', 'cancer', 'varies', 'greatly', 'around', 'lowest', 'countries', 'greatest', 'twelve', 'world', 'annual', 'incidence', 'rates', 'per', 'women', 'eastern', 'south', 'central', 'north', 'western', 'south', 'central', 'eastern', 'southern', 'northern', 'western', 'north', 'number', 'cases', 'significantly', 'increased', 'since', 'phenomenon', 'partly', 'attributed', 'breast', 'cancer', 'strongly', 'related', 'age', 'breast', 'cancers', 'occurring', 'women', 'years', 'newly', 'diagnosed', 'cases', 'breast', 'cancer', 'registered', 'around', 'cases', 'women', 'age', 'older', 'based', 'million', 'women', 'affected', 'breast', 'united', 'incidence', 'breast', 'cancer', 'per', 'women', 'rose', 'around', 'cases', 'per', 'year', 'around', 'late', 'since', 'holding', 'steady', 'around', 'since', 'however', 'deaths', 'breast', 'cancer', 'per', 'women', 'rose', 'slightly', 'since', 'declined', 'steadily', 'breast', 'cancer', 'form', 'cancer', 'often', 'described', 'cancers', 'internal', 'organs', 'essentially', 'breast', 'felt', 'advanced', 'state', 'often', 'developed', 'fungating', 'tumor', 'become', 'necrotic', 'causing', 'tumor', 'appear', 'break', 'weeping', 'dark', 'oldest', 'discovered', 'evidence', 'breast', 'cancer', 'dates', 'back', 'sixth', 'study', 'remains', 'showed', 'typical', 'destructive', 'damage', 'due', 'metastatic', 'describes', 'cases', 'tumors', 'ulcers', 'breast', 'treated', 'writing', 'says', 'physicians', 'described', 'similar', 'cases', 'time', '17th', 'based', 'thus', 'believed', 'breast', 'cancer', 'generally', 'caused', 'imbalances', 'fluids', 'controlled', 'especially', 'excess', 'black', 'alternatively', 'seen', '18th', 'wide', 'variety', 'medical', 'explanations', 'including', 'lack', 'sexual', 'much', 'sexual', 'physical', 'injuries', 'breast', 'various', 'forms', 'lymphatic', 'either', 'internal', 'due', 'restrictive', '19th', 'surgeon', 'said', 'fear', 'cancer', 'caused', 'learned', 'example', 'accounted', 'breast', 'tendency', 'run', 'although', 'breast', 'cancer', 'known', 'uncommon', '19th', 'improvements', 'control', 'deadly', 'infectious', 'diseases', 'resulted', 'dramatic', 'increases', 'women', 'died', 'young', 'developed', 'breast', 'early', 'frequent', 'breastfeeding', 'probably', 'reduced', 'rate', 'breast', 'cancer', 'development', 'women', 'survive', 'middle', 'medicine', 'believed', 'cause', 'rather', 'surgery', 'carried', 'high', 'mortality', 'preferred', 'treatments', 'tended', 'pharmacological', 'rather', 'herbal', 'mineral', 'especially', 'involving', 'poison', 'relatively', 'mastectomy', 'breast', 'cancer', 'performed', 'least', 'early', 'ad', 'proposed', 'court', 'physician', 'doctors', 'achieved', 'greater', 'understanding', 'circulatory', 'system', '17th', 'link', 'breast', 'spread', 'lymph', 'nodes', 'french', 'surgeon', 'performed', 'total', 'mastectomies', 'included', 'removing', 'axillary', 'lymph', 'recognized', 'reduced', 'work', 'built', 'another', 'french', 'additionally', 'removed', 'pectoral', 'muscle', 'underlying', 'judged', 'greatly', 'improved', 'surgeon', 'bell', 'removal', 'entire', 'even', 'portion', 'successful', 'work', 'carried', 'started', 'performing', 'radical', 'mastectomies', 'helped', 'greatly', 'advances', 'general', 'surgical', 'aseptic', 'technique', 'radical', 'mastectomy', 'often', 'involved', 'removing', 'associated', 'lymph', 'underlying', 'chest', 'often', 'led', 'pain', 'seen', 'necessary', 'order', 'prevent', 'cancer', 'radical', 'survival', 'rates', 'surgery', 'raised', 'rate', 'extending', 'promoted', 'taking', 'even', 'survival', 'rates', 'proved', 'equal', 'radical', 'radical', 'mastectomies', 'remained', 'standard', 'care', 'america', 'often', 'followed', 'radiation', 'generally', 'adopted', 'one', 'reason', 'striking', 'difference', 'approach', 'may', 'structure', 'medical', 'descended', 'barber', 'held', 'less', 'surgeon', 'medical', 'far', 'women', 'less', 'one', 'percent', 'american', 'surgical', 'oncologists', 'breast', 'cancer', 'wards', 'medical', 'staff', 'half', 'american', 'health', 'insurance', 'companies', 'also', 'paid', 'surgeons', 'perform', 'radical', 'mastectomies', 'perform', 'breast', 'cancer', 'staging', 'systems', 'developed', 'new', 'understanding', 'metastasis', 'led', 'cancer', 'systemic', 'illness', 'well', 'localized', 'sparing', 'procedures', 'developed', 'proved', 'equally', 'chemotherapy', 'developed', 'world', 'war', 'prominent', 'women', 'died', 'breast', 'cancer', 'include', 'mother', 'mother', 'first', 'study', 'breast', 'cancer', 'done', 'published', 'comparative', 'study', 'breast', 'cancer', 'cases', 'controls', 'background', 'lifestyle', 'thousands', 'women', 'successfully', 'completed', 'standard', 'treatment', 'demanded', 'received', 'bone', 'marrow', 'thinking', 'lead', 'better', 'proved', 'completely', 'women', 'died', 'reports', 'health', 'study', 'conclusions', 'health', 'trial', 'conclusively', 'proved', 'hormone', 'replacement', 'therapy', 'significantly', 'increased', 'incidence', 'breast', '20th', 'breast', 'cancer', 'feared', 'discussed', 'little', 'safely', 'done', 'primitive', 'surgical', 'women', 'tended', 'suffer', 'silently', 'rather', 'seeking', 'surgery', 'survival', 'rates', 'women', 'began', 'raising', 'awareness', 'disease', 'possibility', 'successful', 'field', 'run', 'american', 'society', 'control', 'cancer', 'american', 'cancer', 'one', 'first', 'organized', 'first', 'support', 'called', 'began', 'providing', 'visits', 'women', 'survived', 'breast', 'breast', 'cancer', 'movement', 'developed', 'larger', 'movements', 'health', 'movement', '20th', 'series', 'political', 'educational', 'partly', 'inspired', 'socially', 'effective', 'aids', 'awareness', 'resulted', 'widespread', 'acceptance', 'second', 'opinions', 'less', 'invasive', 'surgical', 'support', 'advances', 'pink', 'ribbon', 'prominent', 'breast', 'cancer', 'pink', 'can', 'made', 'sometimes', 'sold', 'much', 'like', 'may', 'worn', 'honor', 'diagnosed', 'breast', 'identify', 'products', 'manufacturer', 'like', 'sell', 'interested', 'breast', 'pink', 'ribbon', 'associated', 'individual', 'faith', 'focus', 'emotionally', 'ultimate', 'vision', 'cure', 'breast', 'rather', 'path', 'current', 'knowledge', 'future', 'wearing', 'displaying', 'pink', 'ribbon', 'practice', 'kind', 'practical', 'positive', 'also', 'people', 'wear', 'pink', 'ribbon', 'show', 'good', 'will', 'towards', 'women', 'breast', 'practical', 'like', 'patient', 'rights', 'say', 'nature', 'pink', 'pink', 'consumption', 'society', 'lack', 'progress', 'preventing', 'breast', 'also', 'gender', 'women', 'breast', 'cancer', 'action', 'said', 'pink', 'promote', 'products', 'cause', 'breast', 'alcoholic', 'breast', 'cancer', 'also', 'known', 'pink', 'ribbon', 'set', 'values', 'surround', 'shape', 'breast', 'cancer', 'dominant', 'values', 'breast', 'cancer', 'breast', 'cancer', 'therapy', 'viewed', 'passage', 'rather', 'fit', 'woman', 'breast', 'cancer', 'needs', 'normalize', 'minimize', 'disruption', 'health', 'issues', 'cause', 'anyone', 'must', 'cultural', 'people', 'conform', 'model', 'given', 'social', 'case', 'cancer', 'women', 'model', 'culture', 'treating', 'adult', 'women', 'like', 'little', 'evidenced', 'pink', 'bears', 'given', 'adult', 'primary', 'purposes', 'goals', 'breast', 'cancer', 'culture', 'maintain', 'breast', 'dominance', 'health', 'promote', 'appearance', 'society', 'something', 'effective', 'breast', 'sustain', 'expand', 'financial', 'power', 'breast', 'cancer', 'compared', 'diseases', 'breast', 'cancer', 'receives', 'proportionately', 'greater', 'share', 'resources', 'mp', 'house', 'commons', 'united', 'party', 'group', 'cancer', 'stated', 'treatment', 'doubt', 'breast', 'cancer', 'get', 'better', 'treatment', 'terms', 'bed', 'facilities', 'doctors', 'breast', 'cancer', 'also', 'receives', 'significantly', 'media', 'coverage', 'equally', 'prevalent', 'study', 'prostate', 'showing', 'breast', 'cancer', 'stories', 'one', 'covering', 'cancer', 'ultimately', 'concern', 'favoring', 'breast', 'cancer', 'disproportionate', 'research', 'behalf', 'may', 'well', 'lives', 'partly', 'relatively', 'high', 'prevalence', 'survival', 'research', 'towards', 'breast', 'studied', 'little', 'except', 'women', 'breast', 'one', 'result', 'breast', 'high', 'visibility', 'results', 'can', 'sometimes', 'claim', 'one', 'eight', 'women', 'will', 'diagnosed', 'breast', 'cancer', 'claim', 'depends', 'assumption', 'woman', 'will', 'die', 'disease', 'age', 'obscures', 'ten', 'times', 'many', 'women', 'will', 'die', 'heart', 'disease', 'stroke', 'breast', 'emphasis', 'breast', 'cancer', 'screening', 'may', 'harming', 'women', 'unnecessary', 'diagnosed', 'breast', 'cancers', 'might', 'screening', 'mammography', 'efficiently', 'finds', 'asymptomatic', 'breast', 'cancers', 'even', 'serious', 'according', 'institute', 'health', 'policy', 'clinical', 'research', 'screening', 'mammography', 'taken', 'approach', 'says', 'best', 'test', 'one', 'finds', 'rather', 'one', 'finds', 'dangerous', 'breast', 'cancers', 'occur', 'pregnancy', 'rate', 'breast', 'cancers', 'women', 'breast', 'cancer', 'becomes', 'common', 'years', 'following', 'pregnancy', 'becomes', 'less', 'common', 'among', 'general', 'cancers', 'known', 'postpartum', 'breast', 'cancer', 'worse', 'outcomes', 'including', 'increased', 'risk', 'distant', 'spread', 'disease', 'cancers', 'found', 'shortly', 'pregnancy', 'appear', 'approximately', 'rate', 'cancers', 'women', 'similar', 'diagnosing', 'new', 'cancer', 'pregnant', 'woman', 'part', 'symptoms', 'commonly', 'assumed', 'normal', 'discomfort', 'associated', 'cancer', 'typically', 'discovered', 'somewhat', 'later', 'stage', 'average', 'many', 'pregnant', 'recently', 'pregnant', 'imaging', 'mris', 'resonance', 'ct', 'mammograms', 'fetal', 'considered', 'safe', 'pet', 'scans', 'treatment', 'generally', 'radiation', 'normally', 'avoided', 'especially', 'fetal', 'dose', 'might', 'exceed', 'treatments', 'postponed', 'birth', 'cancer', 'diagnosed', 'late', 'early', 'deliveries', 'speed', 'start', 'treatment', 'surgery', 'generally', 'considered', 'safe', 'especially', 'certain', 'chemotherapy', 'drugs', 'given', 'first', 'increase', 'risk', 'birth', 'defects', 'pregnancy', 'loss', 'abortions', 'elective', 'abortions', 'required', 'improve', 'likelihood', 'mother', 'surviving', 'radiation', 'treatments', 'may', 'interfere', 'ability', 'breastfeed', 'baby', 'reduces', 'ability', 'breast', 'produce', 'milk', 'increases', 'risk', 'chemotherapy', 'given', 'many', 'drugs', 'pass', 'breast', 'milk', 'harm', 'regarding', 'future', 'pregnancy', 'among', 'breast', 'cancer', 'often', 'fear', 'cancer', 'many', 'still', 'regard', 'pregnancy', 'represent', 'life', 'breast', 'cancer', 'birth', 'control', 'methods', 'used', 'methods', 'depot', 'medroxyprogesterone', 'iud', 'pills', 'poorly', 'investigated', 'possible', 'increased', 'risk', 'cancer', 'may', 'used', 'positive', 'effects', 'outweigh', 'possible', 'breast', 'cancer', 'recommended', 'first', 'consider', 'options', 'menopausal', 'bisphosphonates', 'selective', 'estrogen', 'receptor', 'vaginal', 'estrogen', 'local', 'studies', 'systemic', 'hormone', 'replacement', 'therapy', 'breast', 'cancer', 'generally', 'hormone', 'replacement', 'necessary', 'breast', 'therapy', 'estrogen', 'therapy', 'intrauterine', 'device', 'may', 'safer', 'options', 'combined', 'systemic', 'treatments', 'evaluated', 'includes', 'individual', 'combinations', 'surgical', 'radiation', 'techniques', 'investigations', 'include', 'new', 'types', 'targeted', 'cancer', 'latest', 'research', 'reported', 'annually', 'meetings', 'american', 'society', 'clinical', 'breast', 'cancer', 'oncology', 'conference', 'studies', 'reviewed', 'professional', 'formulated', 'guidelines', 'specific', 'treatment', 'groups', 'risk', 'also', 'studied', 'way', 'reduce', 'risk', 'breast', 'cancer', 'medications', 'related', 'vitamin', 'cryoablation', 'studied', 'see', 'substitute', 'lumpectomy', 'small', 'tentative', 'evidence', 'tumors', 'less', 'may', 'also', 'used', 'surgery', 'another', 'review', 'states', 'cryoablation', 'looks', 'early', 'breast', 'cancer', 'small', 'considerable', 'part', 'current', 'knowledge', 'breast', 'carcinomas', 'based', 'studies', 'performed', 'cell', 'lines', 'derived', 'breast', 'provide', 'source', 'homogenous', 'free', 'stromal', 'often', 'easily', 'cultured', 'simple', 'standard', 'first', 'breast', 'cancer', 'cell', 'line', 'established', 'since', 'despite', 'sustained', 'work', 'number', 'permanent', 'lines', 'obtained', 'strikingly', 'low', 'attempts', 'culture', 'breast', 'cancer', 'cell', 'lines', 'primary', 'tumors', 'largely', 'poor', 'efficiency', 'often', 'due', 'technical', 'difficulties', 'associated', 'extraction', 'viable', 'tumor', 'cells', 'surrounding', 'available', 'breast', 'cancer', 'cell', 'lines', 'issued', 'metastatic', 'mainly', 'pleural', 'effusions', 'provided', 'generally', 'large', 'numbers', 'viable', 'tumor', 'cells', 'little', 'contamination', 'tumor', 'stroma', 'many', 'currently', 'used', 'bcc', 'lines', 'established', 'late', 'namely', 'account', 'reporting', 'studies', 'mentioned', 'breast', 'cancer', 'cell', 'concluded', 'factors', 'implicated', 'breast', 'specifically', 'process', 'cell', 'motility', 'basis', 'metastasis', 'breast', 'carcinoma', 'inhibitor', 'cell', 'expression', 'increase', 'breast', 'cancer', 'cell', 'invasion', 'inhibits', 'expression', 'blunt', 'cell', 'useful', 'metabolic', 'markers', 'breast', 'cancer', 'estrogen', 'progesterone', 'used', 'predict', 'response', 'hormone', 'new', 'potentially', 'new', 'markers', 'breast', 'cancer', 'include', 'identify', 'people', 'high', 'risk', 'developing', 'breast', 'response', 'therapeutic', 'plasminogen', 'assessing']\n",
            "325\n"
          ]
        }
      ],
      "source": [
        "print(wikidocuments[0])\n",
        "print(len(wikidocuments))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcK34TJLO9yM"
      },
      "source": [
        "Create a list of lists. Each element list is a list of intersected words for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjmPcDD6PCY1"
      },
      "outputs": [],
      "source": [
        "notesdocuments=[] # a list of lists, each element is a list of intersected and filtered words for one note\n",
        "file3=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "line=file3.readline()\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  if line[0]=='codes:': # if this line is for code\n",
        "    line=file3.readline() # skip this line\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    if line[0]=='notes:': # if this line is for note\n",
        "      tempf=[]\n",
        "      line=file3.readline()\n",
        "      while line!='end!\\n': # keep reading until the end of the note\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for word in line:\n",
        "          if word in a3:\n",
        "            tempf.append(word)     \n",
        "        line=file3.readline()      \n",
        "      notesdocuments.append(tempf)\n",
        "  line=file3.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVRJqsmRN4Q"
      },
      "source": [
        "Print a sample element list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tcTvo8IRO4n",
        "outputId": "7024dcc4-a0f8-4440-9b13-fbad8327d281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['admission', 'date', 'discharge', 'date', 'date', 'birth', 'sex', 'm', 'service', 'medicine', 'allergies', 'patient', 'recorded', 'known', 'allergies', 'drugs', 'name', 'un', 'chief', 'complaint', 'fever', 'cough', 'weakness', 'major', 'surgical', 'invasive', 'procedure', 'rigid', 'bronchoscopy', 'endobronchial', 'debulking', 'squamous', 'cell', 'lung', 'mass', 'endotracheal', 'intubation', 'history', 'present', 'illness', 'mr', 'known', 'man', 'squamous', 'cell', 'lung', 'cancer', 'admitted', 'hypotension', 'fever', 'secondary', 'pneumonia', 'patient', 'back', 'pain', 'weight', 'loss', 'productive', 'cough', 'mild', 'hemoptysis', 'beginning', 'month', 'month', 'time', 'revealed', 'opacity', 'ct', 'revealed', 'mass', 'x', 'cm', 'concerning', 'malignancy', 'patient', 'underwent', 'flexible', 'bronchoscopy', 'endobronchial', 'biopsies', 'linear', 'endobronchial', 'ultrasound', 'lymph', 'node', 'biopsies', 'procedures', 'performed', 'dr', 'last', 'name', 'endobronchial', 'biopsies', 'revealed', 'squamous', 'cell', 'carcinoma', 'patient', 'presented', 'dr', 'first', 'name', 'office', 'worsening', 'cough', 'fever', 'weakness', 'office', 'sbp', 'breathing', 'ra', 'diminished', 'l', 'upper', 'lung', 'breath', 'sounds', 'per', 'pcp', 'revealed', 'new', 'infiltrate', 'peripheral', 'mass', 'lesions', 'likely', 'consistent', 'referred', 'pcp', 'time', 'done', 'also', 'showed', 'likely', 'patient', 'given', 'gram', 'iv', 'vancomycin', 'mg', 'sent', 'started', 'continued', 'hypotension', 'proven', 'fluid', 'responsive', 'breathing', 'l', 'oxygen', 'mass', 'thought', 'obstructing', 'airway', 'endobronchial', 'debulking', 'performed', 'prior', 'transfer', 'floor', 'suggested', 'consult', 'patients', 'procedure', 'noted', 'one', 'point', 'mechanical', 'lowered', 'patient', 'transferred', 'floor', 'saturation', 'past', 'medical', 'history', 'osteoarthritis', 'hip', 'social', 'history', 'lives', 'alone', 'always', 'lived', 'ma', 'works', 'post', 'office', 'letter', 'carrier', 'prior', 'employment', 'government', 'smoked', 'x', 'quit', 'drinks', 'family', 'history', 'prostate', 'cancer', 'father', 'deceased', 'hypertension', 'mother', 'alive', 'physical', 'exam', 'admission', 'vs', 't', 'bp', 'p', 'r', 'sat', 'nad', 'looks', 'comfortable', 'mm', 'slightly', 'dry', 'lesions', 'exudate', 'noted', 'neck', 'cervical', 'supraclavicular', 'cv', 'audible', 'lungs', 'focal', 'area', 'wheezing', 'rales', 'left', 'upper', 'lobe', 'scattered', 'left', 'lung', 'whole', 'right', 'lung', 'dullness', 'percussion', 'left', 'upper', 'lung', 'noted', 'soft', 'noted', 'pitting', 'edema', 'present', 'legs', 'bilaterally', 'shin', 'nontender', 'palpation', 'neuro', 'cns', 'intact', 'sensation', 'intact', 'discharge', 'vs', 'general', 'comfortable', 'appearing', 'nad', 'clear', 'cv', 'normal', 'breathing', 'minimal', 'diffuse', 'wheezing', 'throughout', 'worse', 'decreased', 'left', 'lung', 'abdominal', 'extremities', 'pulses', 'pertinent', 'results', 'admission', 'labs', 'blood', 'blood', 'blood', 'blood', 'blood', 'blood', 'ct', 'chest', 'contrast', 'enlarging', 'left', 'upper', 'lobe', 'mass', 'left', 'upper', 'bronchial', 'stenosis', 'level', 'stenosis', 'pulmonary', 'artery', 'less', 'posterior', 'bronchus', 'left', 'upper', 'lobe', 'pulmonary', 'artery', 'pneumonia', 'new', 'bilateral', 'small', 'pleural', 'effusions', 'new', 'left', 'basal', 'pneumonia', 'probably', 'aspiration', 'left', 'upper', 'lobe', 'contents', 'interval', 'results', 'mri', 'head', 'without', 'contrast', 'evidence', 'intracranial', 'metastatic', 'disease', 'ct', 'chest', 'without', 'contrast', 'progression', 'left', 'upper', 'lobe', 'atelectasis', 'consolidation', 'status', 'post', 'recent', 'debridement', 'gas', 'area', 'previously', 'seen', 'lesion', 'left', 'upper', 'lobe', 'bronchus', 'stenosis', 'left', 'upper', 'lobe', 'bronchus', 'however', 'mild', 'increase', 'segment', 'peripheral', 'dilated', 'bronchioles', 'progression', 'consolidation', 'left', 'lower', 'lobe', 'possibly', 'related', 'aspiration', 'part', 'compressive', 'atelectasis', 'due', 'enlarging', 'left', 'pleural', 'effusion', 'small', 'right', 'pleural', 'effusion', 'adjacent', 'atelectasis', 'unchanged', 'lymphadenopathy', 'pleural', 'fluid', 'analysis', 'organisms', 'fluid', 'culture', 'without', 'growth', 'anaerobic', 'culture', 'without', 'growth', 'fdg', 'tumor', 'imaging', 'fdg', 'avid', 'left', 'upper', 'lobe', 'consolidation', 'focally', 'increased', 'fdg', 'avidity', 'surrounding', 'narrowed', 'left', 'upper', 'lobe', 'bronchus', 'compatible', 'known', 'squamous', 'cell', 'carcinoma', 'collapse', 'consolidation', 'left', 'upper', 'lobe', 'fdg', 'avid', 'consolidation', 'left', 'lower', 'lobe', 'likely', 'pneumonia', 'aspiration', 'moderate', 'left', 'small', 'right', 'pleural', 'effusions', 'fdg', 'avid', 'mediastinal', 'lymphadenopathy', 'trace', 'free', 'fluid', 'pelvis', 'chest', 'ct', 'contrast', 'interval', 'improvement', 'left', 'upper', 'lobe', 'pneumonia', 'improved', 'anterior', 'aspect', 'left', 'upper', 'lobe', 'foci', 'gas', 'within', 'parenchyma', 'similar', 'appearance', 'potential', 'communication', 'left', 'upper', 'lobe', 'segmental', 'bronchus', 'collections', 'air', 'possible', 'known', 'mass', 'lesion', 'well', 'evaluated', 'due', 'surrounding', 'consolidation', 'slight', 'interval', 'increase', 'left', 'pleural', 'effusion', 'resolution', 'right', 'pleural', 'effusion', 'extensive', 'lymphadenopathy', 'unchanged', 'unclear', 'component', 'reactive', 'versus', 'involved', 'malignancy', 'ultrasound', 'doppler', 'lower', 'extremities', 'thrombosis', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'blood', 'cultures', 'repeated', 'times', 'admission', 'always', 'without', 'growth', 'chest', 'repeated', 'times', 'throughout', 'admission', 'demonstrated', 'left', 'upper', 'lobe', 'pneumonia', 'little', 'improvement', 'images', 'later', 'stay', 'revealed', 'almost', 'complete', 'white', 'left', 'lung', 'collapse', 'left', 'lower', 'lobe', 'detected', 'late', 'hospital', 'course', 'mediastinal', 'lymph', 'node', 'biopsy', 'lymph', 'node', 'biopsy', 'ad', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'ef', 'malignancy', 'identified', 'lymph', 'node', 'level', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'j', 'malignancy', 'identified', 'discharge', 'results', 'blood', 'blood', 'blood', 'brief', 'hospital', 'course', 'man', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'presented', 'fevers', 'leukocytosis', 'hypotension', 'setting', 'hypotension', 'patient', 'presented', 'systolic', 'blood', 'pressures', 'patient', 'sent', 'pressure', 'came', 'following', 'mental', 'status', 'never', 'compromised', 'lactate', 'patient', 'subsequently', 'transferred', 'general', 'medicine', 'floors', 'patient', 'continued', 'intermittent', 'hypotensive', 'episodes', 'stay', 'associated', 'severe', 'night', 'sweats', 'fevers', 'infectious', 'work', 'negative', 'multiple', 'fevers', 'attributed', 'tumor', 'started', 'scheduled', 'acetaminophen', 'continuous', 'ivf', 'patient', 'subsequent', 'episodes', 'hypotension', 'pneumonia', 'setting', 'known', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'radiographic', 'evidence', 'high', 'concern', 'pneumonia', 'patient', 'started', 'vancomycin', 'switched', 'interventional', 'performed', 'rigid', 'bronchoscopy', 'bulk', 'resection', 'antibiotics', 'switched', 'metronidazole', 'per', 'recommendations', 'patient', 'subsequently', 'began', 'radiation', 'therapy', 'effort', 'shrink', 'tumor', 'hopes', 'definitive', 'therapy', 'pneumonia', 'repeat', 'chest', 'throughout', 'admission', 'demonstrated', 'little', 'improvement', 'subsequent', 'collapse', 'left', 'lower', 'lobe', 'progression', 'left', 'upper', 'lobe', 'patient', 'antibiotics', 'discontinued', 'discharge', 'patient', 'completed', 'days', 'antibiotics', 'little', 'concern', 'infectious', 'etiology', 'fevers', 'per', 'left', 'upper', 'lobe', 'poorly', 'differentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'diagnosed', 'several', 'weeks', 'prior', 'admission', 'time', 'lymph', 'nodes', 'clear', 'brain', 'mri', 'showed', 'early', 'hospitalization', 'revealed', 'evidence', 'brain', 'metastases', 'pet', 'scan', 'revealed', 'mediastinal', 'lymph', 'nodes', 'suspicious', 'metastases', 'patient', 'subsequently', 'began', 'treatments', 'shrink', 'tumor', 'enough', 'treat', 'pneumonia', 'thoracic', 'surgery', 'consulted', 'performed', 'sampled', 'lymph', 'nodes', 'negative', 'malignant', 'cells', 'patient', 'discharged', 'instructions', 'follow', 'oncologist', 'well', 'chest', 'disease', 'center', 'pleural', 'effusion', 'pleural', 'fluid', 'negative', 'malignant', 'cells', 'thoracentesis', 'cultures', 'showing', 'empyema', 'exudative', 'parapneumonic', 'effusion', 'suspected', 'evidence', 'empyema', 'patient', 'treated', 'pneumonia', 'per', 'partial', 'occlusive', 'patient', 'mild', 'lower', 'extremity', 'edema', 'admission', 'ultrasound', 'study', 'revealed', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'patient', 'initially', 'treated', 'heparin', 'drip', 'due', 'concerns', 'renal', 'function', 'patient', 'eventually', 'treated', 'mg', 'subcutaneous', 'injection', 'twice', 'daily', 'discharged', 'home', 'instructions', 'continue', 'injections', 'thrombocytosis', 'likely', 'acute', 'phase', 'reactant', 'setting', 'ongoing', 'infection', 'inflammation', 'patients', 'platelet', 'counts', 'remained', 'elevated', 'throughout', 'admission', 'anemia', 'hematocrit', 'stable', 'throughout', 'admission', 'iron', 'studies', 'consistent', 'anemia', 'chronic', 'inflammation', 'medications', 'admission', 'none', 'discharge', 'medications', 'aerosol', 'inhalation', 'every', 'hours', 'needed', 'shortness', 'breath', 'wheezing', 'ml', 'syrup', 'mls', 'every', 'hours', 'needed', 'cough', 'weeks', 'ml', 'bottle', 'fluconazole', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'days', 'tablets', 'acetaminophen', 'mg', 'tablet', 'two', 'tablet', 'every', 'hours', 'tablets', 'ml', 'syringe', 'seventy', 'mg', 'subcutaneous', 'twice', 'day', 'syringes', 'lorazepam', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'anxiety', 'tablets', 'mg', 'tablet', 'one', 'tablet', 'daily', 'daily', 'tablets', 'morphine', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'pain', 'tablets', 'discharge', 'disposition', 'home', 'service', 'facility', 'care', 'greater', 'location', 'un', 'discharge', 'diagnosis', 'primary', 'diagnoses', 'pneumonia', 'diagnoses', 'squamous', 'cell', 'lung', 'cancer', 'discharge', 'condition', 'mental', 'status', 'clear', 'coherent', 'level', 'consciousness', 'alert', 'interactive', 'activity', 'status', 'ambulatory', 'independent', 'discharge', 'instructions', 'mr', 'known', 'pleasure', 'taking', 'care', 'admitted', 'hospital', 'pneumonia', 'fever', 'low', 'blood', 'pressure', 'also', 'known', 'hypotension', 'known', 'lung', 'cancer', 'obstructing', 'airways', 'leading', 'known', 'pneumonia', 'procedure', 'lung', 'cancer', 'attempt', 'open', 'airways', 'order', 'allow', 'pneumonia', 'resolve', 'received', 'antibiotics', 'will', 'need', 'continue', 'outpatient', 'hospital', 'began', 'workup', 'staging', 'lung', 'cancer', 'brain', 'mri', 'show', 'metastases', 'scheduled', 'scan', 'outpatient', 'will', 'need', 'follow', 'directions', 'oral', 'contrast', 'provided', 'prior', 'discharge', 'follow', 'dr', 'first', 'name', 'medical', 'oncology', 'dr', 'first', 'name', 'thoracic', 'surgery', 'dr', 'last', 'name', 'radiation', 'oncology', 'following', 'changes', 'made', 'medications', 'start', 'using', 'inhaler', 'every', 'hours', 'needed', 'shortness', 'breath', 'start', 'using', 'cough', 'syrup', 'ml', 'mouth', 'every', 'six', 'hours', 'needed', 'cough', 'start', 'using', 'fluconazole', 'mg', 'mouth', 'day', 'medication', 'sore', 'throat', 'will', 'need', 'take', 'days', 'start', 'taking', 'acetaminophen', 'mg', 'take', 'two', 'tablets', 'mouth', 'every', 'six', 'hours', 'needed', 'fever', 'exceed', 'tablets', 'per', 'day', 'start', 'using', 'mg', 'subcutaneous', 'injection', 'twice', 'day', 'medication', 'blood', 'clots', 'found', 'legs', 'start', 'using', 'lorazepam', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'anxiety', 'start', 'taking', 'mg', 'mouth', 'day', 'medication', 'helps', 'reflux', 'start', 'taking', 'morphine', 'sulfate', 'ir', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'pain', 'instructions', 'department', 'chest', 'disease', 'center', 'name', 'last', 'name', 'lf', 'first', 'initial', 'last', 'name', 'call', 'thoracic', 'oncology', 'program', 'schedule', 'upcoming', 'appointment', 'dr', 'first', 'name', 'days', 'hospital', 'discharge', 'call', 'office', 'number', 'listed', 'make', 'appointment', 'location', 'address', 'location', 'un', 'hospital', 'ward', 'name', 'location', 'un', 'numeric', 'phone', 'department', 'pulmonary', 'function', 'lab', 'pulmonary', 'function', 'lab', 'building', 'hospital', 'ward', 'name', 'building', 'ward', 'name', 'complex', 'location', 'un', 'east', 'best', 'main', 'department', 'radiology', 'building', 'cc', 'location', 'un', 'hospital', 'west', 'best', 'street', 'scan', 'department', 'radiology', 'location', 'hospital', 'ward', 'name', 'center', 'location', 'un', 'east', 'department', 'primary', 'care', 'name', 'dr', 'first', 'doctor', 'last', 'name', 'location', 'location', 'un', 'address', 'country', '3rd', 'fl', 'numeric', 'phone', 'completed']\n"
          ]
        }
      ],
      "source": [
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgP1wKEWSVV2"
      },
      "source": [
        "Set the sequence of words in the vocabulary matrix. Key is a word, and the value is the sequence/order in the vocabulary matrix.  \n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGwAnUihS5sY"
      },
      "outputs": [],
      "source": [
        "notesvocab={}\n",
        "for i in notesdocuments: # for each element (is a list) in list\n",
        "  for j in i: # for each word\n",
        "    if j.lower() not in notesvocab: # if a word is not in dict\n",
        "      # # set value of this word equal to current element (order of the token in matrix)\n",
        "      notesvocab[j.lower()]=len(notesvocab) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiUuPs8lTaow"
      },
      "source": [
        "Print sample key-value pairs (Compare with notesdocuments[0])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4bUiVMrTd67",
        "outputId": "33c2c6a8-b986-4dc9-8a0b-f543d1cf9b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "['admission', 'date', 'discharge', 'date', 'date', 'birth', 'sex', 'm', 'service', 'medicine', 'allergies', 'patient', 'recorded', 'known', 'allergies', 'drugs', 'name', 'un', 'chief', 'complaint', 'fever', 'cough', 'weakness', 'major', 'surgical', 'invasive', 'procedure', 'rigid', 'bronchoscopy', 'endobronchial', 'debulking', 'squamous', 'cell', 'lung', 'mass', 'endotracheal', 'intubation', 'history', 'present', 'illness', 'mr', 'known', 'man', 'squamous', 'cell', 'lung', 'cancer', 'admitted', 'hypotension', 'fever', 'secondary', 'pneumonia', 'patient', 'back', 'pain', 'weight', 'loss', 'productive', 'cough', 'mild', 'hemoptysis', 'beginning', 'month', 'month', 'time', 'revealed', 'opacity', 'ct', 'revealed', 'mass', 'x', 'cm', 'concerning', 'malignancy', 'patient', 'underwent', 'flexible', 'bronchoscopy', 'endobronchial', 'biopsies', 'linear', 'endobronchial', 'ultrasound', 'lymph', 'node', 'biopsies', 'procedures', 'performed', 'dr', 'last', 'name', 'endobronchial', 'biopsies', 'revealed', 'squamous', 'cell', 'carcinoma', 'patient', 'presented', 'dr', 'first', 'name', 'office', 'worsening', 'cough', 'fever', 'weakness', 'office', 'sbp', 'breathing', 'ra', 'diminished', 'l', 'upper', 'lung', 'breath', 'sounds', 'per', 'pcp', 'revealed', 'new', 'infiltrate', 'peripheral', 'mass', 'lesions', 'likely', 'consistent', 'referred', 'pcp', 'time', 'done', 'also', 'showed', 'likely', 'patient', 'given', 'gram', 'iv', 'vancomycin', 'mg', 'sent', 'started', 'continued', 'hypotension', 'proven', 'fluid', 'responsive', 'breathing', 'l', 'oxygen', 'mass', 'thought', 'obstructing', 'airway', 'endobronchial', 'debulking', 'performed', 'prior', 'transfer', 'floor', 'suggested', 'consult', 'patients', 'procedure', 'noted', 'one', 'point', 'mechanical', 'lowered', 'patient', 'transferred', 'floor', 'saturation', 'past', 'medical', 'history', 'osteoarthritis', 'hip', 'social', 'history', 'lives', 'alone', 'always', 'lived', 'ma', 'works', 'post', 'office', 'letter', 'carrier', 'prior', 'employment', 'government', 'smoked', 'x', 'quit', 'drinks', 'family', 'history', 'prostate', 'cancer', 'father', 'deceased', 'hypertension', 'mother', 'alive', 'physical', 'exam', 'admission', 'vs', 't', 'bp', 'p', 'r', 'sat', 'nad', 'looks', 'comfortable', 'mm', 'slightly', 'dry', 'lesions', 'exudate', 'noted', 'neck', 'cervical', 'supraclavicular', 'cv', 'audible', 'lungs', 'focal', 'area', 'wheezing', 'rales', 'left', 'upper', 'lobe', 'scattered', 'left', 'lung', 'whole', 'right', 'lung', 'dullness', 'percussion', 'left', 'upper', 'lung', 'noted', 'soft', 'noted', 'pitting', 'edema', 'present', 'legs', 'bilaterally', 'shin', 'nontender', 'palpation', 'neuro', 'cns', 'intact', 'sensation', 'intact', 'discharge', 'vs', 'general', 'comfortable', 'appearing', 'nad', 'clear', 'cv', 'normal', 'breathing', 'minimal', 'diffuse', 'wheezing', 'throughout', 'worse', 'decreased', 'left', 'lung', 'abdominal', 'extremities', 'pulses', 'pertinent', 'results', 'admission', 'labs', 'blood', 'blood', 'blood', 'blood', 'blood', 'blood', 'ct', 'chest', 'contrast', 'enlarging', 'left', 'upper', 'lobe', 'mass', 'left', 'upper', 'bronchial', 'stenosis', 'level', 'stenosis', 'pulmonary', 'artery', 'less', 'posterior', 'bronchus', 'left', 'upper', 'lobe', 'pulmonary', 'artery', 'pneumonia', 'new', 'bilateral', 'small', 'pleural', 'effusions', 'new', 'left', 'basal', 'pneumonia', 'probably', 'aspiration', 'left', 'upper', 'lobe', 'contents', 'interval', 'results', 'mri', 'head', 'without', 'contrast', 'evidence', 'intracranial', 'metastatic', 'disease', 'ct', 'chest', 'without', 'contrast', 'progression', 'left', 'upper', 'lobe', 'atelectasis', 'consolidation', 'status', 'post', 'recent', 'debridement', 'gas', 'area', 'previously', 'seen', 'lesion', 'left', 'upper', 'lobe', 'bronchus', 'stenosis', 'left', 'upper', 'lobe', 'bronchus', 'however', 'mild', 'increase', 'segment', 'peripheral', 'dilated', 'bronchioles', 'progression', 'consolidation', 'left', 'lower', 'lobe', 'possibly', 'related', 'aspiration', 'part', 'compressive', 'atelectasis', 'due', 'enlarging', 'left', 'pleural', 'effusion', 'small', 'right', 'pleural', 'effusion', 'adjacent', 'atelectasis', 'unchanged', 'lymphadenopathy', 'pleural', 'fluid', 'analysis', 'organisms', 'fluid', 'culture', 'without', 'growth', 'anaerobic', 'culture', 'without', 'growth', 'fdg', 'tumor', 'imaging', 'fdg', 'avid', 'left', 'upper', 'lobe', 'consolidation', 'focally', 'increased', 'fdg', 'avidity', 'surrounding', 'narrowed', 'left', 'upper', 'lobe', 'bronchus', 'compatible', 'known', 'squamous', 'cell', 'carcinoma', 'collapse', 'consolidation', 'left', 'upper', 'lobe', 'fdg', 'avid', 'consolidation', 'left', 'lower', 'lobe', 'likely', 'pneumonia', 'aspiration', 'moderate', 'left', 'small', 'right', 'pleural', 'effusions', 'fdg', 'avid', 'mediastinal', 'lymphadenopathy', 'trace', 'free', 'fluid', 'pelvis', 'chest', 'ct', 'contrast', 'interval', 'improvement', 'left', 'upper', 'lobe', 'pneumonia', 'improved', 'anterior', 'aspect', 'left', 'upper', 'lobe', 'foci', 'gas', 'within', 'parenchyma', 'similar', 'appearance', 'potential', 'communication', 'left', 'upper', 'lobe', 'segmental', 'bronchus', 'collections', 'air', 'possible', 'known', 'mass', 'lesion', 'well', 'evaluated', 'due', 'surrounding', 'consolidation', 'slight', 'interval', 'increase', 'left', 'pleural', 'effusion', 'resolution', 'right', 'pleural', 'effusion', 'extensive', 'lymphadenopathy', 'unchanged', 'unclear', 'component', 'reactive', 'versus', 'involved', 'malignancy', 'ultrasound', 'doppler', 'lower', 'extremities', 'thrombosis', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'blood', 'cultures', 'repeated', 'times', 'admission', 'always', 'without', 'growth', 'chest', 'repeated', 'times', 'throughout', 'admission', 'demonstrated', 'left', 'upper', 'lobe', 'pneumonia', 'little', 'improvement', 'images', 'later', 'stay', 'revealed', 'almost', 'complete', 'white', 'left', 'lung', 'collapse', 'left', 'lower', 'lobe', 'detected', 'late', 'hospital', 'course', 'mediastinal', 'lymph', 'node', 'biopsy', 'lymph', 'node', 'biopsy', 'ad', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'ef', 'malignancy', 'identified', 'lymph', 'node', 'level', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'j', 'malignancy', 'identified', 'discharge', 'results', 'blood', 'blood', 'blood', 'brief', 'hospital', 'course', 'man', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'presented', 'fevers', 'leukocytosis', 'hypotension', 'setting', 'hypotension', 'patient', 'presented', 'systolic', 'blood', 'pressures', 'patient', 'sent', 'pressure', 'came', 'following', 'mental', 'status', 'never', 'compromised', 'lactate', 'patient', 'subsequently', 'transferred', 'general', 'medicine', 'floors', 'patient', 'continued', 'intermittent', 'hypotensive', 'episodes', 'stay', 'associated', 'severe', 'night', 'sweats', 'fevers', 'infectious', 'work', 'negative', 'multiple', 'fevers', 'attributed', 'tumor', 'started', 'scheduled', 'acetaminophen', 'continuous', 'ivf', 'patient', 'subsequent', 'episodes', 'hypotension', 'pneumonia', 'setting', 'known', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'radiographic', 'evidence', 'high', 'concern', 'pneumonia', 'patient', 'started', 'vancomycin', 'switched', 'interventional', 'performed', 'rigid', 'bronchoscopy', 'bulk', 'resection', 'antibiotics', 'switched', 'metronidazole', 'per', 'recommendations', 'patient', 'subsequently', 'began', 'radiation', 'therapy', 'effort', 'shrink', 'tumor', 'hopes', 'definitive', 'therapy', 'pneumonia', 'repeat', 'chest', 'throughout', 'admission', 'demonstrated', 'little', 'improvement', 'subsequent', 'collapse', 'left', 'lower', 'lobe', 'progression', 'left', 'upper', 'lobe', 'patient', 'antibiotics', 'discontinued', 'discharge', 'patient', 'completed', 'days', 'antibiotics', 'little', 'concern', 'infectious', 'etiology', 'fevers', 'per', 'left', 'upper', 'lobe', 'poorly', 'differentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'diagnosed', 'several', 'weeks', 'prior', 'admission', 'time', 'lymph', 'nodes', 'clear', 'brain', 'mri', 'showed', 'early', 'hospitalization', 'revealed', 'evidence', 'brain', 'metastases', 'pet', 'scan', 'revealed', 'mediastinal', 'lymph', 'nodes', 'suspicious', 'metastases', 'patient', 'subsequently', 'began', 'treatments', 'shrink', 'tumor', 'enough', 'treat', 'pneumonia', 'thoracic', 'surgery', 'consulted', 'performed', 'sampled', 'lymph', 'nodes', 'negative', 'malignant', 'cells', 'patient', 'discharged', 'instructions', 'follow', 'oncologist', 'well', 'chest', 'disease', 'center', 'pleural', 'effusion', 'pleural', 'fluid', 'negative', 'malignant', 'cells', 'thoracentesis', 'cultures', 'showing', 'empyema', 'exudative', 'parapneumonic', 'effusion', 'suspected', 'evidence', 'empyema', 'patient', 'treated', 'pneumonia', 'per', 'partial', 'occlusive', 'patient', 'mild', 'lower', 'extremity', 'edema', 'admission', 'ultrasound', 'study', 'revealed', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'patient', 'initially', 'treated', 'heparin', 'drip', 'due', 'concerns', 'renal', 'function', 'patient', 'eventually', 'treated', 'mg', 'subcutaneous', 'injection', 'twice', 'daily', 'discharged', 'home', 'instructions', 'continue', 'injections', 'thrombocytosis', 'likely', 'acute', 'phase', 'reactant', 'setting', 'ongoing', 'infection', 'inflammation', 'patients', 'platelet', 'counts', 'remained', 'elevated', 'throughout', 'admission', 'anemia', 'hematocrit', 'stable', 'throughout', 'admission', 'iron', 'studies', 'consistent', 'anemia', 'chronic', 'inflammation', 'medications', 'admission', 'none', 'discharge', 'medications', 'aerosol', 'inhalation', 'every', 'hours', 'needed', 'shortness', 'breath', 'wheezing', 'ml', 'syrup', 'mls', 'every', 'hours', 'needed', 'cough', 'weeks', 'ml', 'bottle', 'fluconazole', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'days', 'tablets', 'acetaminophen', 'mg', 'tablet', 'two', 'tablet', 'every', 'hours', 'tablets', 'ml', 'syringe', 'seventy', 'mg', 'subcutaneous', 'twice', 'day', 'syringes', 'lorazepam', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'anxiety', 'tablets', 'mg', 'tablet', 'one', 'tablet', 'daily', 'daily', 'tablets', 'morphine', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'pain', 'tablets', 'discharge', 'disposition', 'home', 'service', 'facility', 'care', 'greater', 'location', 'un', 'discharge', 'diagnosis', 'primary', 'diagnoses', 'pneumonia', 'diagnoses', 'squamous', 'cell', 'lung', 'cancer', 'discharge', 'condition', 'mental', 'status', 'clear', 'coherent', 'level', 'consciousness', 'alert', 'interactive', 'activity', 'status', 'ambulatory', 'independent', 'discharge', 'instructions', 'mr', 'known', 'pleasure', 'taking', 'care', 'admitted', 'hospital', 'pneumonia', 'fever', 'low', 'blood', 'pressure', 'also', 'known', 'hypotension', 'known', 'lung', 'cancer', 'obstructing', 'airways', 'leading', 'known', 'pneumonia', 'procedure', 'lung', 'cancer', 'attempt', 'open', 'airways', 'order', 'allow', 'pneumonia', 'resolve', 'received', 'antibiotics', 'will', 'need', 'continue', 'outpatient', 'hospital', 'began', 'workup', 'staging', 'lung', 'cancer', 'brain', 'mri', 'show', 'metastases', 'scheduled', 'scan', 'outpatient', 'will', 'need', 'follow', 'directions', 'oral', 'contrast', 'provided', 'prior', 'discharge', 'follow', 'dr', 'first', 'name', 'medical', 'oncology', 'dr', 'first', 'name', 'thoracic', 'surgery', 'dr', 'last', 'name', 'radiation', 'oncology', 'following', 'changes', 'made', 'medications', 'start', 'using', 'inhaler', 'every', 'hours', 'needed', 'shortness', 'breath', 'start', 'using', 'cough', 'syrup', 'ml', 'mouth', 'every', 'six', 'hours', 'needed', 'cough', 'start', 'using', 'fluconazole', 'mg', 'mouth', 'day', 'medication', 'sore', 'throat', 'will', 'need', 'take', 'days', 'start', 'taking', 'acetaminophen', 'mg', 'take', 'two', 'tablets', 'mouth', 'every', 'six', 'hours', 'needed', 'fever', 'exceed', 'tablets', 'per', 'day', 'start', 'using', 'mg', 'subcutaneous', 'injection', 'twice', 'day', 'medication', 'blood', 'clots', 'found', 'legs', 'start', 'using', 'lorazepam', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'anxiety', 'start', 'taking', 'mg', 'mouth', 'day', 'medication', 'helps', 'reflux', 'start', 'taking', 'morphine', 'sulfate', 'ir', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'pain', 'instructions', 'department', 'chest', 'disease', 'center', 'name', 'last', 'name', 'lf', 'first', 'initial', 'last', 'name', 'call', 'thoracic', 'oncology', 'program', 'schedule', 'upcoming', 'appointment', 'dr', 'first', 'name', 'days', 'hospital', 'discharge', 'call', 'office', 'number', 'listed', 'make', 'appointment', 'location', 'address', 'location', 'un', 'hospital', 'ward', 'name', 'location', 'un', 'numeric', 'phone', 'department', 'pulmonary', 'function', 'lab', 'pulmonary', 'function', 'lab', 'building', 'hospital', 'ward', 'name', 'building', 'ward', 'name', 'complex', 'location', 'un', 'east', 'best', 'main', 'department', 'radiology', 'building', 'cc', 'location', 'un', 'hospital', 'west', 'best', 'street', 'scan', 'department', 'radiology', 'location', 'hospital', 'ward', 'name', 'center', 'location', 'un', 'east', 'department', 'primary', 'care', 'name', 'dr', 'first', 'doctor', 'last', 'name', 'location', 'location', 'un', 'address', 'country', '3rd', 'fl', 'numeric', 'phone', 'completed']\n"
          ]
        }
      ],
      "source": [
        "print(notesvocab[\"admission\"])\n",
        "print(notesvocab[\"date\"])\n",
        "print(notesvocab[\"discharge\"])\n",
        "print(notesvocab[\"birth\"])\n",
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RKZthpkV8g1"
      },
      "source": [
        "Create a list of string for Wiki document, and each element is a string for a Wiki document (including intersected words).\n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWcFW-QEV63H"
      },
      "outputs": [],
      "source": [
        "wikidata=[] # each element is a string, each string contains words for a wiki doc\n",
        "for i in wikidocuments:\n",
        "  temp=''\n",
        "  for j in i:\n",
        "    temp=temp+j+\" \"\n",
        "  wikidata.append(temp)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0UU5z_OWHe2"
      },
      "source": [
        "Print a sample string (Compare with wikidocuments[0])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fAfDRVWWK2u",
        "outputId": "0e959a85-747b-4178-ffe1-cdeb3c724eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breast cancer breast cancer cancer develops breast signs breast cancer may include lump change breast dimpling fluid coming newly inverted red scaly patch distant spread may bone swollen lymph shortness yellow risk factors developing breast cancer include lack physical drinking hormone replacement therapy early age first children late older prior history breast family cases due inherited including among breast cancer commonly develops cells lining milk ducts lobules supply ducts cancers developing ducts known ductal developing lobules known lobular breast ductal carcinoma develop diagnosis breast cancer confirmed taking biopsy concerning diagnosis tests done determine cancer spread beyond breast treatments likely balance benefits versus harms breast cancer screening review stated unclear screening good review us preventive services task force found evidence benefit years organization recommends screening every two years women years medications tamoxifen raloxifene may used effort prevent breast cancer high risk developing surgical removal breasts another preventative measure high risk diagnosed number treatments may including radiation hormonal therapy targeted types surgery vary surgery breast reconstruction may take place time surgery later cancer spread parts treatments mostly aimed improving quality life outcomes breast cancer vary depending cancer extent survival rates developed world united states alive least developing countries survival rates breast cancer leading type cancer accounting resulted million new cases common developed countries times common women first noticeable symptom breast cancer typically lump feels different rest breast breast cancer cases discovered woman feels earliest breast cancers detected lumps found lymph nodes located armpits can also indicate breast indications breast cancer lump may include thickening different breast one breast becoming larger nipple changing position shape becoming skin puckering rash around discharge constant pain part breast swelling beneath armpit around pain unreliable tool determining presence absence breast may indicative breast health inflammatory breast cancer particular type breast cancer can pose substantial diagnostic symptoms may resemble breast inflammation may include nipple warmth redness throughout well texture skin referred inflammatory breast cancer present lump can sometimes delay another reported symptom complex breast cancer disease syndrome presents skin changes resembling mild flaking nipple disease breast symptoms may include increased may also discharge approximately half women diagnosed disease breast also lump rare initially appears fibroadenoma fact tumors formed within stroma breast contain glandular well stromal tumors staged usual classified basis appearance microscope breast cancer presents metastatic cancer spread beyond original symptoms caused metastatic breast cancer will depend location common sites metastasis include lung unexplained weight loss can occasionally signal breast can symptoms fevers bone joint pains can sometimes manifestations metastatic breast can jaundice neurological symptoms called meaning manifestations many symptoms breast including turn represent underlying breast fewer benign breast diseases mastitis fibroadenoma breast common causes breast disorder risk factors can divided two primary risk factors breast cancer female older potential risk factors include lack lack higher levels certain certain dietary one study indicates exposure light risk factor development breast obesity drinking alcoholic beverages among common modifiable risk smoking tobacco appears increase risk breast greater amount smoked earlier life smoking higher risk increased lack physical activity linked sitting regularly prolonged periods associated higher mortality breast risk regular though association use hormonal birth control development breast whether oral contraceptives use may actually cause breast cancer matter indeed absolute effect clear association exists newer hormonal birth mutations breast cancer susceptibility family history breast use oral contraceptives appear affect risk breast association breast feeding breast cancer clearly studies found support association others cancer hypothesis induced abortion increased risk developing breast hypothesis subject extensive concluded neither miscarriages abortions associated heightened risk breast number dietary factors linked risk breast drinking alcoholic beverages increases risk breast even relatively low three drinks per moderate risk highest among heavy dietary factors may increase risk include diet high cholesterol dietary iodine deficiency may also play evidence fiber review found studies trying link fiber intake breast cancer produced mixed tentative association low fiber intake adolescence breast cancer risk factors include radiation number chemicals also including organic solvents although radiation mammography low estimated yearly screening years age will cause approximately cases fatal breast cancer per million women genetic susceptibility may play minor role genetics believed primary cause women whose mother diagnosed increased risk whose mother diagnosed age increased risk one two affected risk breast cancer age subsequent mortality disease first degree relative disease risk breast cancer age double general less genetics plays significant role causing hereditary cancer includes carry gene mutations account total genetic influence risk breast cancer significant mutations include said four distinct types breast cancer genetic changes lead many breast changes like atypical ductal hyperplasia lobular carcinoma found benign breast conditions fibrocystic breast correlated increased breast cancer diabetes mellitus might also increase risk breast autoimmune diseases lupus erythematosus seem also increase risk acquisition breast breast like occurs interaction environmental factor susceptible normal cells divide many times needed attach cells stay place cells become cancerous lose ability stop attach stay die proper normal cells will commit cell suicide cell longer protected cell suicide several protein clusters one protective pathways another sometimes along protective pathways mutated way turns permanently rendering cell incapable suicide longer one steps causes cancer combination protein turns pathway cell ready programmed cell breast gene protein pathway stuck cancer cell commit mutations can lead breast cancer linked estrogen abnormal growth factor interaction stromal cells epithelial cells can facilitate malignant cell breast adipose leads increased cell proliferation united percent people breast cancer people ovarian cancer relative one familial tendency develop cancers called hereditary cancer best known confer lifetime risk breast cancer percent lifetime risk ovarian cancer mutations associated occur mechanisms correct errors mutations either inherited acquired allow allow uncontrolled lack metastasis distant strong evidence residual risk variation goes well beyond hereditary gene mutations carrier caused risk environmental causes triggers breast inherited mutation can interfere repair dna cross dna double strand breaks functions cause dna damage dna cross double strand breaks often require repairs pathways containing mutations account percent breast say cancer may inevitable half hereditary cancer syndromes involve unknown directly controls expression estrogen receptor associated epithelial loss leads loss differentiation poor prognosis due cancer cell invasion types breast cancer easy diagnose microscopic analysis affected area types breast cancer require specialized lab two commonly used screening physical examination breasts healthcare provider can offer approximate likelihood lump may also detect simple examinations healthcare provider can remove sample fluid lump microscopic analysis procedure known fine needle fine needle aspiration help establish needle aspiration may performed healthcare office clinic using local finding clear fluid makes lump highly unlikely bloody fluid may sent inspection microscope cancerous physical examination can used diagnose breast cancer good degree options biopsy include core biopsy breast procedures section breast lump excisional entire lump often results physical examination healthcare additional tests may performed special circumstances imaging ultrasound sufficient warrant excisional biopsy definitive diagnostic primary treatment breast cancers classified several prognosis can affect treatment description breast cancer optimally includes women can reduce risk breast cancer maintaining healthy reducing alcohol increasing physical modifications might prevent breast cancers benefits moderate exercise brisk walking seen age groups including postmenopausal high levels physical activity reduce risk breast cancer strategies encourage regular physical activity reduce obesity also reduced risks cardiovascular disease high intake citrus fruit associated reduction risk breast marine fatty acids appear reduce high consumption foods may reduce removal breasts cancer diagnosed suspicious lump lesion appeared procedure known prophylactic bilateral may considered people associated substantially heightened risk eventual diagnosis breast evidence strong enough support procedure anyone highest brca testing recommended high family risk genetic recommended many forms changes ranging obviously dangerous effect identifiable changes testing person particularly likely return one unclear removing second breast breast cancer one selective estrogen receptor reduce risk breast cancer increase risk thromboembolism endometrial overall change risk thus recommended prevention breast cancer women average risk may offered high benefit breast cancer reduction continues least five years stopping course treatment breast cancer screening refers testing women breast cancer attempt achieve earlier diagnosis assumption early detection will improve number screening tests employed including clinical self breast genetic magnetic resonance clinical self breast exam involves feeling breast lumps clinical breast exams performed health care exams performed person evidence support effectiveness either type breast time lump large enough found likely growing several years thus soon large enough found without screening breast cancer uses examine breast uncharacteristic masses breast compressed technician takes multiple general mammogram takes entire diagnostic mammogram focuses specific lump area number national bodies recommend breast cancer average preventive services task force recommends mammography every two years women ages council europe recommends mammography programs using screening recommended ages frequency task force reports point addition unnecessary surgery risks frequent mammograms include small significant increase breast cancer induced collaboration states best quality evidence neither demonstrates reduction cancer reduction cause mortality screening less rigorous trials added analysis reduction mortality due breast cancer decrease deaths breast cancer years relative decrease breast screening years results increase rates per half will least one falsely positive resulted view clear whether mammography screening good states due recent improvements breast cancer risks false positives breast cancer screening leading unnecessary therefore longer seems beneficial attend breast cancer whether mri screening method greater harms benefits compared standard mammography management breast cancer depends various including stage cancer treatments aggressive prognosis worse higher risk recurrence cancer following breast cancer usually treated may followed chemotherapy radiation multidisciplinary approach hormone cancers often treated therapy courses several monoclonal may administered certain cases metastatic advanced stages breast surgery involves physical removal typically along surrounding one lymph nodes may biopsied increasingly lymph node sampling performed sentinel lymph node standard surgeries tumor person breast reconstruction type plastic may performed improve appearance treated women use breast prostheses breast choose flat nipple prosthesis can used time following drugs used addition surgery called adjuvant chemotherapy types therapy prior surgery called neoadjuvant aspirin may reduce mortality breast currently three main groups medications used adjuvant breast cancer monoclonal hormone blocking therapy chemotherapy monoclonal antibodies radiotherapy given surgery region tumor bed regional lymph microscopic tumor cells may may also beneficial effect tumor radiation therapy can delivered external beam radiotherapy brachytherapy radiotherapy given operation breast radiation can also given time operation breast radiation can reduce risk recurrence reduction delivered correct dose considered essential breast cancer treated removing lump wide local stage breast cancer important component traditional classification methods breast greater effect prognosis staging takes consideration local lymph node status whether metastatic disease higher stage poorer stage raised disease lymph chest skin aggressiveness cancer stage lowered presence zones cell behaviour size factor staging unless cancer ductal carcinoma situ involving entire breast will still stage zero consequently excellent prognosis disease free survival breast cancer grade assessed comparison breast cancer cells normal breast closer normal cancer cells slower growth better cells well will appear will divide will tend well differentiated given grade moderate grade poor undifferentiated given higher grade upon scale widely used system younger women age less years women years tend poorer prognosis women due several breasts may change menstrual may nursing may unaware changes younger women usually advanced stage may also biologic factors contributing higher risk disease recurrence younger women breast people breast cancer experience illness factors age can significant impact way person breast cancer women positive breast cancer must issues early menopause induced many chemotherapy regimens used treat breast especially use hormones counteract ovarian breast cancer common invasive cancer affects women common form cancer skin cancers generally easily cause routinely excluded cancer breast cancer invasive cancers women female comprised cancers diagnosed making common female breast cancer caused deaths cancer deaths women cancer deaths men women lung second common cause death caused cancer deaths women cancer deaths men women incidence breast cancer varies greatly around lowest countries greatest twelve world annual incidence rates per women eastern south central north western south central eastern southern northern western north number cases significantly increased since phenomenon partly attributed breast cancer strongly related age breast cancers occurring women years newly diagnosed cases breast cancer registered around cases women age older based million women affected breast united incidence breast cancer per women rose around cases per year around late since holding steady around since however deaths breast cancer per women rose slightly since declined steadily breast cancer form cancer often described cancers internal organs essentially breast felt advanced state often developed fungating tumor become necrotic causing tumor appear break weeping dark oldest discovered evidence breast cancer dates back sixth study remains showed typical destructive damage due metastatic describes cases tumors ulcers breast treated writing says physicians described similar cases time 17th based thus believed breast cancer generally caused imbalances fluids controlled especially excess black alternatively seen 18th wide variety medical explanations including lack sexual much sexual physical injuries breast various forms lymphatic either internal due restrictive 19th surgeon said fear cancer caused learned example accounted breast tendency run although breast cancer known uncommon 19th improvements control deadly infectious diseases resulted dramatic increases women died young developed breast early frequent breastfeeding probably reduced rate breast cancer development women survive middle medicine believed cause rather surgery carried high mortality preferred treatments tended pharmacological rather herbal mineral especially involving poison relatively mastectomy breast cancer performed least early ad proposed court physician doctors achieved greater understanding circulatory system 17th link breast spread lymph nodes french surgeon performed total mastectomies included removing axillary lymph recognized reduced work built another french additionally removed pectoral muscle underlying judged greatly improved surgeon bell removal entire even portion successful work carried started performing radical mastectomies helped greatly advances general surgical aseptic technique radical mastectomy often involved removing associated lymph underlying chest often led pain seen necessary order prevent cancer radical survival rates surgery raised rate extending promoted taking even survival rates proved equal radical radical mastectomies remained standard care america often followed radiation generally adopted one reason striking difference approach may structure medical descended barber held less surgeon medical far women less one percent american surgical oncologists breast cancer wards medical staff half american health insurance companies also paid surgeons perform radical mastectomies perform breast cancer staging systems developed new understanding metastasis led cancer systemic illness well localized sparing procedures developed proved equally chemotherapy developed world war prominent women died breast cancer include mother mother first study breast cancer done published comparative study breast cancer cases controls background lifestyle thousands women successfully completed standard treatment demanded received bone marrow thinking lead better proved completely women died reports health study conclusions health trial conclusively proved hormone replacement therapy significantly increased incidence breast 20th breast cancer feared discussed little safely done primitive surgical women tended suffer silently rather seeking surgery survival rates women began raising awareness disease possibility successful field run american society control cancer american cancer one first organized first support called began providing visits women survived breast breast cancer movement developed larger movements health movement 20th series political educational partly inspired socially effective aids awareness resulted widespread acceptance second opinions less invasive surgical support advances pink ribbon prominent breast cancer pink can made sometimes sold much like may worn honor diagnosed breast identify products manufacturer like sell interested breast pink ribbon associated individual faith focus emotionally ultimate vision cure breast rather path current knowledge future wearing displaying pink ribbon practice kind practical positive also people wear pink ribbon show good will towards women breast practical like patient rights say nature pink pink consumption society lack progress preventing breast also gender women breast cancer action said pink promote products cause breast alcoholic breast cancer also known pink ribbon set values surround shape breast cancer dominant values breast cancer breast cancer therapy viewed passage rather fit woman breast cancer needs normalize minimize disruption health issues cause anyone must cultural people conform model given social case cancer women model culture treating adult women like little evidenced pink bears given adult primary purposes goals breast cancer culture maintain breast dominance health promote appearance society something effective breast sustain expand financial power breast cancer compared diseases breast cancer receives proportionately greater share resources mp house commons united party group cancer stated treatment doubt breast cancer get better treatment terms bed facilities doctors breast cancer also receives significantly media coverage equally prevalent study prostate showing breast cancer stories one covering cancer ultimately concern favoring breast cancer disproportionate research behalf may well lives partly relatively high prevalence survival research towards breast studied little except women breast one result breast high visibility results can sometimes claim one eight women will diagnosed breast cancer claim depends assumption woman will die disease age obscures ten times many women will die heart disease stroke breast emphasis breast cancer screening may harming women unnecessary diagnosed breast cancers might screening mammography efficiently finds asymptomatic breast cancers even serious according institute health policy clinical research screening mammography taken approach says best test one finds rather one finds dangerous breast cancers occur pregnancy rate breast cancers women breast cancer becomes common years following pregnancy becomes less common among general cancers known postpartum breast cancer worse outcomes including increased risk distant spread disease cancers found shortly pregnancy appear approximately rate cancers women similar diagnosing new cancer pregnant woman part symptoms commonly assumed normal discomfort associated cancer typically discovered somewhat later stage average many pregnant recently pregnant imaging mris resonance ct mammograms fetal considered safe pet scans treatment generally radiation normally avoided especially fetal dose might exceed treatments postponed birth cancer diagnosed late early deliveries speed start treatment surgery generally considered safe especially certain chemotherapy drugs given first increase risk birth defects pregnancy loss abortions elective abortions required improve likelihood mother surviving radiation treatments may interfere ability breastfeed baby reduces ability breast produce milk increases risk chemotherapy given many drugs pass breast milk harm regarding future pregnancy among breast cancer often fear cancer many still regard pregnancy represent life breast cancer birth control methods used methods depot medroxyprogesterone iud pills poorly investigated possible increased risk cancer may used positive effects outweigh possible breast cancer recommended first consider options menopausal bisphosphonates selective estrogen receptor vaginal estrogen local studies systemic hormone replacement therapy breast cancer generally hormone replacement necessary breast therapy estrogen therapy intrauterine device may safer options combined systemic treatments evaluated includes individual combinations surgical radiation techniques investigations include new types targeted cancer latest research reported annually meetings american society clinical breast cancer oncology conference studies reviewed professional formulated guidelines specific treatment groups risk also studied way reduce risk breast cancer medications related vitamin cryoablation studied see substitute lumpectomy small tentative evidence tumors less may also used surgery another review states cryoablation looks early breast cancer small considerable part current knowledge breast carcinomas based studies performed cell lines derived breast provide source homogenous free stromal often easily cultured simple standard first breast cancer cell line established since despite sustained work number permanent lines obtained strikingly low attempts culture breast cancer cell lines primary tumors largely poor efficiency often due technical difficulties associated extraction viable tumor cells surrounding available breast cancer cell lines issued metastatic mainly pleural effusions provided generally large numbers viable tumor cells little contamination tumor stroma many currently used bcc lines established late namely account reporting studies mentioned breast cancer cell concluded factors implicated breast specifically process cell motility basis metastasis breast carcinoma inhibitor cell expression increase breast cancer cell invasion inhibits expression blunt cell useful metabolic markers breast cancer estrogen progesterone used predict response hormone new potentially new markers breast cancer include identify people high risk developing breast response therapeutic plasminogen assessing \n",
            "['breast', 'cancer', 'breast', 'cancer', 'cancer', 'develops', 'breast', 'signs', 'breast', 'cancer', 'may', 'include', 'lump', 'change', 'breast', 'dimpling', 'fluid', 'coming', 'newly', 'inverted', 'red', 'scaly', 'patch', 'distant', 'spread', 'may', 'bone', 'swollen', 'lymph', 'shortness', 'yellow', 'risk', 'factors', 'developing', 'breast', 'cancer', 'include', 'lack', 'physical', 'drinking', 'hormone', 'replacement', 'therapy', 'early', 'age', 'first', 'children', 'late', 'older', 'prior', 'history', 'breast', 'family', 'cases', 'due', 'inherited', 'including', 'among', 'breast', 'cancer', 'commonly', 'develops', 'cells', 'lining', 'milk', 'ducts', 'lobules', 'supply', 'ducts', 'cancers', 'developing', 'ducts', 'known', 'ductal', 'developing', 'lobules', 'known', 'lobular', 'breast', 'ductal', 'carcinoma', 'develop', 'diagnosis', 'breast', 'cancer', 'confirmed', 'taking', 'biopsy', 'concerning', 'diagnosis', 'tests', 'done', 'determine', 'cancer', 'spread', 'beyond', 'breast', 'treatments', 'likely', 'balance', 'benefits', 'versus', 'harms', 'breast', 'cancer', 'screening', 'review', 'stated', 'unclear', 'screening', 'good', 'review', 'us', 'preventive', 'services', 'task', 'force', 'found', 'evidence', 'benefit', 'years', 'organization', 'recommends', 'screening', 'every', 'two', 'years', 'women', 'years', 'medications', 'tamoxifen', 'raloxifene', 'may', 'used', 'effort', 'prevent', 'breast', 'cancer', 'high', 'risk', 'developing', 'surgical', 'removal', 'breasts', 'another', 'preventative', 'measure', 'high', 'risk', 'diagnosed', 'number', 'treatments', 'may', 'including', 'radiation', 'hormonal', 'therapy', 'targeted', 'types', 'surgery', 'vary', 'surgery', 'breast', 'reconstruction', 'may', 'take', 'place', 'time', 'surgery', 'later', 'cancer', 'spread', 'parts', 'treatments', 'mostly', 'aimed', 'improving', 'quality', 'life', 'outcomes', 'breast', 'cancer', 'vary', 'depending', 'cancer', 'extent', 'survival', 'rates', 'developed', 'world', 'united', 'states', 'alive', 'least', 'developing', 'countries', 'survival', 'rates', 'breast', 'cancer', 'leading', 'type', 'cancer', 'accounting', 'resulted', 'million', 'new', 'cases', 'common', 'developed', 'countries', 'times', 'common', 'women', 'first', 'noticeable', 'symptom', 'breast', 'cancer', 'typically', 'lump', 'feels', 'different', 'rest', 'breast', 'breast', 'cancer', 'cases', 'discovered', 'woman', 'feels', 'earliest', 'breast', 'cancers', 'detected', 'lumps', 'found', 'lymph', 'nodes', 'located', 'armpits', 'can', 'also', 'indicate', 'breast', 'indications', 'breast', 'cancer', 'lump', 'may', 'include', 'thickening', 'different', 'breast', 'one', 'breast', 'becoming', 'larger', 'nipple', 'changing', 'position', 'shape', 'becoming', 'skin', 'puckering', 'rash', 'around', 'discharge', 'constant', 'pain', 'part', 'breast', 'swelling', 'beneath', 'armpit', 'around', 'pain', 'unreliable', 'tool', 'determining', 'presence', 'absence', 'breast', 'may', 'indicative', 'breast', 'health', 'inflammatory', 'breast', 'cancer', 'particular', 'type', 'breast', 'cancer', 'can', 'pose', 'substantial', 'diagnostic', 'symptoms', 'may', 'resemble', 'breast', 'inflammation', 'may', 'include', 'nipple', 'warmth', 'redness', 'throughout', 'well', 'texture', 'skin', 'referred', 'inflammatory', 'breast', 'cancer', 'present', 'lump', 'can', 'sometimes', 'delay', 'another', 'reported', 'symptom', 'complex', 'breast', 'cancer', 'disease', 'syndrome', 'presents', 'skin', 'changes', 'resembling', 'mild', 'flaking', 'nipple', 'disease', 'breast', 'symptoms', 'may', 'include', 'increased', 'may', 'also', 'discharge', 'approximately', 'half', 'women', 'diagnosed', 'disease', 'breast', 'also', 'lump', 'rare', 'initially', 'appears', 'fibroadenoma', 'fact', 'tumors', 'formed', 'within', 'stroma', 'breast', 'contain', 'glandular', 'well', 'stromal', 'tumors', 'staged', 'usual', 'classified', 'basis', 'appearance', 'microscope', 'breast', 'cancer', 'presents', 'metastatic', 'cancer', 'spread', 'beyond', 'original', 'symptoms', 'caused', 'metastatic', 'breast', 'cancer', 'will', 'depend', 'location', 'common', 'sites', 'metastasis', 'include', 'lung', 'unexplained', 'weight', 'loss', 'can', 'occasionally', 'signal', 'breast', 'can', 'symptoms', 'fevers', 'bone', 'joint', 'pains', 'can', 'sometimes', 'manifestations', 'metastatic', 'breast', 'can', 'jaundice', 'neurological', 'symptoms', 'called', 'meaning', 'manifestations', 'many', 'symptoms', 'breast', 'including', 'turn', 'represent', 'underlying', 'breast', 'fewer', 'benign', 'breast', 'diseases', 'mastitis', 'fibroadenoma', 'breast', 'common', 'causes', 'breast', 'disorder', 'risk', 'factors', 'can', 'divided', 'two', 'primary', 'risk', 'factors', 'breast', 'cancer', 'female', 'older', 'potential', 'risk', 'factors', 'include', 'lack', 'lack', 'higher', 'levels', 'certain', 'certain', 'dietary', 'one', 'study', 'indicates', 'exposure', 'light', 'risk', 'factor', 'development', 'breast', 'obesity', 'drinking', 'alcoholic', 'beverages', 'among', 'common', 'modifiable', 'risk', 'smoking', 'tobacco', 'appears', 'increase', 'risk', 'breast', 'greater', 'amount', 'smoked', 'earlier', 'life', 'smoking', 'higher', 'risk', 'increased', 'lack', 'physical', 'activity', 'linked', 'sitting', 'regularly', 'prolonged', 'periods', 'associated', 'higher', 'mortality', 'breast', 'risk', 'regular', 'though', 'association', 'use', 'hormonal', 'birth', 'control', 'development', 'breast', 'whether', 'oral', 'contraceptives', 'use', 'may', 'actually', 'cause', 'breast', 'cancer', 'matter', 'indeed', 'absolute', 'effect', 'clear', 'association', 'exists', 'newer', 'hormonal', 'birth', 'mutations', 'breast', 'cancer', 'susceptibility', 'family', 'history', 'breast', 'use', 'oral', 'contraceptives', 'appear', 'affect', 'risk', 'breast', 'association', 'breast', 'feeding', 'breast', 'cancer', 'clearly', 'studies', 'found', 'support', 'association', 'others', 'cancer', 'hypothesis', 'induced', 'abortion', 'increased', 'risk', 'developing', 'breast', 'hypothesis', 'subject', 'extensive', 'concluded', 'neither', 'miscarriages', 'abortions', 'associated', 'heightened', 'risk', 'breast', 'number', 'dietary', 'factors', 'linked', 'risk', 'breast', 'drinking', 'alcoholic', 'beverages', 'increases', 'risk', 'breast', 'even', 'relatively', 'low', 'three', 'drinks', 'per', 'moderate', 'risk', 'highest', 'among', 'heavy', 'dietary', 'factors', 'may', 'increase', 'risk', 'include', 'diet', 'high', 'cholesterol', 'dietary', 'iodine', 'deficiency', 'may', 'also', 'play', 'evidence', 'fiber', 'review', 'found', 'studies', 'trying', 'link', 'fiber', 'intake', 'breast', 'cancer', 'produced', 'mixed', 'tentative', 'association', 'low', 'fiber', 'intake', 'adolescence', 'breast', 'cancer', 'risk', 'factors', 'include', 'radiation', 'number', 'chemicals', 'also', 'including', 'organic', 'solvents', 'although', 'radiation', 'mammography', 'low', 'estimated', 'yearly', 'screening', 'years', 'age', 'will', 'cause', 'approximately', 'cases', 'fatal', 'breast', 'cancer', 'per', 'million', 'women', 'genetic', 'susceptibility', 'may', 'play', 'minor', 'role', 'genetics', 'believed', 'primary', 'cause', 'women', 'whose', 'mother', 'diagnosed', 'increased', 'risk', 'whose', 'mother', 'diagnosed', 'age', 'increased', 'risk', 'one', 'two', 'affected', 'risk', 'breast', 'cancer', 'age', 'subsequent', 'mortality', 'disease', 'first', 'degree', 'relative', 'disease', 'risk', 'breast', 'cancer', 'age', 'double', 'general', 'less', 'genetics', 'plays', 'significant', 'role', 'causing', 'hereditary', 'cancer', 'includes', 'carry', 'gene', 'mutations', 'account', 'total', 'genetic', 'influence', 'risk', 'breast', 'cancer', 'significant', 'mutations', 'include', 'said', 'four', 'distinct', 'types', 'breast', 'cancer', 'genetic', 'changes', 'lead', 'many', 'breast', 'changes', 'like', 'atypical', 'ductal', 'hyperplasia', 'lobular', 'carcinoma', 'found', 'benign', 'breast', 'conditions', 'fibrocystic', 'breast', 'correlated', 'increased', 'breast', 'cancer', 'diabetes', 'mellitus', 'might', 'also', 'increase', 'risk', 'breast', 'autoimmune', 'diseases', 'lupus', 'erythematosus', 'seem', 'also', 'increase', 'risk', 'acquisition', 'breast', 'breast', 'like', 'occurs', 'interaction', 'environmental', 'factor', 'susceptible', 'normal', 'cells', 'divide', 'many', 'times', 'needed', 'attach', 'cells', 'stay', 'place', 'cells', 'become', 'cancerous', 'lose', 'ability', 'stop', 'attach', 'stay', 'die', 'proper', 'normal', 'cells', 'will', 'commit', 'cell', 'suicide', 'cell', 'longer', 'protected', 'cell', 'suicide', 'several', 'protein', 'clusters', 'one', 'protective', 'pathways', 'another', 'sometimes', 'along', 'protective', 'pathways', 'mutated', 'way', 'turns', 'permanently', 'rendering', 'cell', 'incapable', 'suicide', 'longer', 'one', 'steps', 'causes', 'cancer', 'combination', 'protein', 'turns', 'pathway', 'cell', 'ready', 'programmed', 'cell', 'breast', 'gene', 'protein', 'pathway', 'stuck', 'cancer', 'cell', 'commit', 'mutations', 'can', 'lead', 'breast', 'cancer', 'linked', 'estrogen', 'abnormal', 'growth', 'factor', 'interaction', 'stromal', 'cells', 'epithelial', 'cells', 'can', 'facilitate', 'malignant', 'cell', 'breast', 'adipose', 'leads', 'increased', 'cell', 'proliferation', 'united', 'percent', 'people', 'breast', 'cancer', 'people', 'ovarian', 'cancer', 'relative', 'one', 'familial', 'tendency', 'develop', 'cancers', 'called', 'hereditary', 'cancer', 'best', 'known', 'confer', 'lifetime', 'risk', 'breast', 'cancer', 'percent', 'lifetime', 'risk', 'ovarian', 'cancer', 'mutations', 'associated', 'occur', 'mechanisms', 'correct', 'errors', 'mutations', 'either', 'inherited', 'acquired', 'allow', 'allow', 'uncontrolled', 'lack', 'metastasis', 'distant', 'strong', 'evidence', 'residual', 'risk', 'variation', 'goes', 'well', 'beyond', 'hereditary', 'gene', 'mutations', 'carrier', 'caused', 'risk', 'environmental', 'causes', 'triggers', 'breast', 'inherited', 'mutation', 'can', 'interfere', 'repair', 'dna', 'cross', 'dna', 'double', 'strand', 'breaks', 'functions', 'cause', 'dna', 'damage', 'dna', 'cross', 'double', 'strand', 'breaks', 'often', 'require', 'repairs', 'pathways', 'containing', 'mutations', 'account', 'percent', 'breast', 'say', 'cancer', 'may', 'inevitable', 'half', 'hereditary', 'cancer', 'syndromes', 'involve', 'unknown', 'directly', 'controls', 'expression', 'estrogen', 'receptor', 'associated', 'epithelial', 'loss', 'leads', 'loss', 'differentiation', 'poor', 'prognosis', 'due', 'cancer', 'cell', 'invasion', 'types', 'breast', 'cancer', 'easy', 'diagnose', 'microscopic', 'analysis', 'affected', 'area', 'types', 'breast', 'cancer', 'require', 'specialized', 'lab', 'two', 'commonly', 'used', 'screening', 'physical', 'examination', 'breasts', 'healthcare', 'provider', 'can', 'offer', 'approximate', 'likelihood', 'lump', 'may', 'also', 'detect', 'simple', 'examinations', 'healthcare', 'provider', 'can', 'remove', 'sample', 'fluid', 'lump', 'microscopic', 'analysis', 'procedure', 'known', 'fine', 'needle', 'fine', 'needle', 'aspiration', 'help', 'establish', 'needle', 'aspiration', 'may', 'performed', 'healthcare', 'office', 'clinic', 'using', 'local', 'finding', 'clear', 'fluid', 'makes', 'lump', 'highly', 'unlikely', 'bloody', 'fluid', 'may', 'sent', 'inspection', 'microscope', 'cancerous', 'physical', 'examination', 'can', 'used', 'diagnose', 'breast', 'cancer', 'good', 'degree', 'options', 'biopsy', 'include', 'core', 'biopsy', 'breast', 'procedures', 'section', 'breast', 'lump', 'excisional', 'entire', 'lump', 'often', 'results', 'physical', 'examination', 'healthcare', 'additional', 'tests', 'may', 'performed', 'special', 'circumstances', 'imaging', 'ultrasound', 'sufficient', 'warrant', 'excisional', 'biopsy', 'definitive', 'diagnostic', 'primary', 'treatment', 'breast', 'cancers', 'classified', 'several', 'prognosis', 'can', 'affect', 'treatment', 'description', 'breast', 'cancer', 'optimally', 'includes', 'women', 'can', 'reduce', 'risk', 'breast', 'cancer', 'maintaining', 'healthy', 'reducing', 'alcohol', 'increasing', 'physical', 'modifications', 'might', 'prevent', 'breast', 'cancers', 'benefits', 'moderate', 'exercise', 'brisk', 'walking', 'seen', 'age', 'groups', 'including', 'postmenopausal', 'high', 'levels', 'physical', 'activity', 'reduce', 'risk', 'breast', 'cancer', 'strategies', 'encourage', 'regular', 'physical', 'activity', 'reduce', 'obesity', 'also', 'reduced', 'risks', 'cardiovascular', 'disease', 'high', 'intake', 'citrus', 'fruit', 'associated', 'reduction', 'risk', 'breast', 'marine', 'fatty', 'acids', 'appear', 'reduce', 'high', 'consumption', 'foods', 'may', 'reduce', 'removal', 'breasts', 'cancer', 'diagnosed', 'suspicious', 'lump', 'lesion', 'appeared', 'procedure', 'known', 'prophylactic', 'bilateral', 'may', 'considered', 'people', 'associated', 'substantially', 'heightened', 'risk', 'eventual', 'diagnosis', 'breast', 'evidence', 'strong', 'enough', 'support', 'procedure', 'anyone', 'highest', 'brca', 'testing', 'recommended', 'high', 'family', 'risk', 'genetic', 'recommended', 'many', 'forms', 'changes', 'ranging', 'obviously', 'dangerous', 'effect', 'identifiable', 'changes', 'testing', 'person', 'particularly', 'likely', 'return', 'one', 'unclear', 'removing', 'second', 'breast', 'breast', 'cancer', 'one', 'selective', 'estrogen', 'receptor', 'reduce', 'risk', 'breast', 'cancer', 'increase', 'risk', 'thromboembolism', 'endometrial', 'overall', 'change', 'risk', 'thus', 'recommended', 'prevention', 'breast', 'cancer', 'women', 'average', 'risk', 'may', 'offered', 'high', 'benefit', 'breast', 'cancer', 'reduction', 'continues', 'least', 'five', 'years', 'stopping', 'course', 'treatment', 'breast', 'cancer', 'screening', 'refers', 'testing', 'women', 'breast', 'cancer', 'attempt', 'achieve', 'earlier', 'diagnosis', 'assumption', 'early', 'detection', 'will', 'improve', 'number', 'screening', 'tests', 'employed', 'including', 'clinical', 'self', 'breast', 'genetic', 'magnetic', 'resonance', 'clinical', 'self', 'breast', 'exam', 'involves', 'feeling', 'breast', 'lumps', 'clinical', 'breast', 'exams', 'performed', 'health', 'care', 'exams', 'performed', 'person', 'evidence', 'support', 'effectiveness', 'either', 'type', 'breast', 'time', 'lump', 'large', 'enough', 'found', 'likely', 'growing', 'several', 'years', 'thus', 'soon', 'large', 'enough', 'found', 'without', 'screening', 'breast', 'cancer', 'uses', 'examine', 'breast', 'uncharacteristic', 'masses', 'breast', 'compressed', 'technician', 'takes', 'multiple', 'general', 'mammogram', 'takes', 'entire', 'diagnostic', 'mammogram', 'focuses', 'specific', 'lump', 'area', 'number', 'national', 'bodies', 'recommend', 'breast', 'cancer', 'average', 'preventive', 'services', 'task', 'force', 'recommends', 'mammography', 'every', 'two', 'years', 'women', 'ages', 'council', 'europe', 'recommends', 'mammography', 'programs', 'using', 'screening', 'recommended', 'ages', 'frequency', 'task', 'force', 'reports', 'point', 'addition', 'unnecessary', 'surgery', 'risks', 'frequent', 'mammograms', 'include', 'small', 'significant', 'increase', 'breast', 'cancer', 'induced', 'collaboration', 'states', 'best', 'quality', 'evidence', 'neither', 'demonstrates', 'reduction', 'cancer', 'reduction', 'cause', 'mortality', 'screening', 'less', 'rigorous', 'trials', 'added', 'analysis', 'reduction', 'mortality', 'due', 'breast', 'cancer', 'decrease', 'deaths', 'breast', 'cancer', 'years', 'relative', 'decrease', 'breast', 'screening', 'years', 'results', 'increase', 'rates', 'per', 'half', 'will', 'least', 'one', 'falsely', 'positive', 'resulted', 'view', 'clear', 'whether', 'mammography', 'screening', 'good', 'states', 'due', 'recent', 'improvements', 'breast', 'cancer', 'risks', 'false', 'positives', 'breast', 'cancer', 'screening', 'leading', 'unnecessary', 'therefore', 'longer', 'seems', 'beneficial', 'attend', 'breast', 'cancer', 'whether', 'mri', 'screening', 'method', 'greater', 'harms', 'benefits', 'compared', 'standard', 'mammography', 'management', 'breast', 'cancer', 'depends', 'various', 'including', 'stage', 'cancer', 'treatments', 'aggressive', 'prognosis', 'worse', 'higher', 'risk', 'recurrence', 'cancer', 'following', 'breast', 'cancer', 'usually', 'treated', 'may', 'followed', 'chemotherapy', 'radiation', 'multidisciplinary', 'approach', 'hormone', 'cancers', 'often', 'treated', 'therapy', 'courses', 'several', 'monoclonal', 'may', 'administered', 'certain', 'cases', 'metastatic', 'advanced', 'stages', 'breast', 'surgery', 'involves', 'physical', 'removal', 'typically', 'along', 'surrounding', 'one', 'lymph', 'nodes', 'may', 'biopsied', 'increasingly', 'lymph', 'node', 'sampling', 'performed', 'sentinel', 'lymph', 'node', 'standard', 'surgeries', 'tumor', 'person', 'breast', 'reconstruction', 'type', 'plastic', 'may', 'performed', 'improve', 'appearance', 'treated', 'women', 'use', 'breast', 'prostheses', 'breast', 'choose', 'flat', 'nipple', 'prosthesis', 'can', 'used', 'time', 'following', 'drugs', 'used', 'addition', 'surgery', 'called', 'adjuvant', 'chemotherapy', 'types', 'therapy', 'prior', 'surgery', 'called', 'neoadjuvant', 'aspirin', 'may', 'reduce', 'mortality', 'breast', 'currently', 'three', 'main', 'groups', 'medications', 'used', 'adjuvant', 'breast', 'cancer', 'monoclonal', 'hormone', 'blocking', 'therapy', 'chemotherapy', 'monoclonal', 'antibodies', 'radiotherapy', 'given', 'surgery', 'region', 'tumor', 'bed', 'regional', 'lymph', 'microscopic', 'tumor', 'cells', 'may', 'may', 'also', 'beneficial', 'effect', 'tumor', 'radiation', 'therapy', 'can', 'delivered', 'external', 'beam', 'radiotherapy', 'brachytherapy', 'radiotherapy', 'given', 'operation', 'breast', 'radiation', 'can', 'also', 'given', 'time', 'operation', 'breast', 'radiation', 'can', 'reduce', 'risk', 'recurrence', 'reduction', 'delivered', 'correct', 'dose', 'considered', 'essential', 'breast', 'cancer', 'treated', 'removing', 'lump', 'wide', 'local', 'stage', 'breast', 'cancer', 'important', 'component', 'traditional', 'classification', 'methods', 'breast', 'greater', 'effect', 'prognosis', 'staging', 'takes', 'consideration', 'local', 'lymph', 'node', 'status', 'whether', 'metastatic', 'disease', 'higher', 'stage', 'poorer', 'stage', 'raised', 'disease', 'lymph', 'chest', 'skin', 'aggressiveness', 'cancer', 'stage', 'lowered', 'presence', 'zones', 'cell', 'behaviour', 'size', 'factor', 'staging', 'unless', 'cancer', 'ductal', 'carcinoma', 'situ', 'involving', 'entire', 'breast', 'will', 'still', 'stage', 'zero', 'consequently', 'excellent', 'prognosis', 'disease', 'free', 'survival', 'breast', 'cancer', 'grade', 'assessed', 'comparison', 'breast', 'cancer', 'cells', 'normal', 'breast', 'closer', 'normal', 'cancer', 'cells', 'slower', 'growth', 'better', 'cells', 'well', 'will', 'appear', 'will', 'divide', 'will', 'tend', 'well', 'differentiated', 'given', 'grade', 'moderate', 'grade', 'poor', 'undifferentiated', 'given', 'higher', 'grade', 'upon', 'scale', 'widely', 'used', 'system', 'younger', 'women', 'age', 'less', 'years', 'women', 'years', 'tend', 'poorer', 'prognosis', 'women', 'due', 'several', 'breasts', 'may', 'change', 'menstrual', 'may', 'nursing', 'may', 'unaware', 'changes', 'younger', 'women', 'usually', 'advanced', 'stage', 'may', 'also', 'biologic', 'factors', 'contributing', 'higher', 'risk', 'disease', 'recurrence', 'younger', 'women', 'breast', 'people', 'breast', 'cancer', 'experience', 'illness', 'factors', 'age', 'can', 'significant', 'impact', 'way', 'person', 'breast', 'cancer', 'women', 'positive', 'breast', 'cancer', 'must', 'issues', 'early', 'menopause', 'induced', 'many', 'chemotherapy', 'regimens', 'used', 'treat', 'breast', 'especially', 'use', 'hormones', 'counteract', 'ovarian', 'breast', 'cancer', 'common', 'invasive', 'cancer', 'affects', 'women', 'common', 'form', 'cancer', 'skin', 'cancers', 'generally', 'easily', 'cause', 'routinely', 'excluded', 'cancer', 'breast', 'cancer', 'invasive', 'cancers', 'women', 'female', 'comprised', 'cancers', 'diagnosed', 'making', 'common', 'female', 'breast', 'cancer', 'caused', 'deaths', 'cancer', 'deaths', 'women', 'cancer', 'deaths', 'men', 'women', 'lung', 'second', 'common', 'cause', 'death', 'caused', 'cancer', 'deaths', 'women', 'cancer', 'deaths', 'men', 'women', 'incidence', 'breast', 'cancer', 'varies', 'greatly', 'around', 'lowest', 'countries', 'greatest', 'twelve', 'world', 'annual', 'incidence', 'rates', 'per', 'women', 'eastern', 'south', 'central', 'north', 'western', 'south', 'central', 'eastern', 'southern', 'northern', 'western', 'north', 'number', 'cases', 'significantly', 'increased', 'since', 'phenomenon', 'partly', 'attributed', 'breast', 'cancer', 'strongly', 'related', 'age', 'breast', 'cancers', 'occurring', 'women', 'years', 'newly', 'diagnosed', 'cases', 'breast', 'cancer', 'registered', 'around', 'cases', 'women', 'age', 'older', 'based', 'million', 'women', 'affected', 'breast', 'united', 'incidence', 'breast', 'cancer', 'per', 'women', 'rose', 'around', 'cases', 'per', 'year', 'around', 'late', 'since', 'holding', 'steady', 'around', 'since', 'however', 'deaths', 'breast', 'cancer', 'per', 'women', 'rose', 'slightly', 'since', 'declined', 'steadily', 'breast', 'cancer', 'form', 'cancer', 'often', 'described', 'cancers', 'internal', 'organs', 'essentially', 'breast', 'felt', 'advanced', 'state', 'often', 'developed', 'fungating', 'tumor', 'become', 'necrotic', 'causing', 'tumor', 'appear', 'break', 'weeping', 'dark', 'oldest', 'discovered', 'evidence', 'breast', 'cancer', 'dates', 'back', 'sixth', 'study', 'remains', 'showed', 'typical', 'destructive', 'damage', 'due', 'metastatic', 'describes', 'cases', 'tumors', 'ulcers', 'breast', 'treated', 'writing', 'says', 'physicians', 'described', 'similar', 'cases', 'time', '17th', 'based', 'thus', 'believed', 'breast', 'cancer', 'generally', 'caused', 'imbalances', 'fluids', 'controlled', 'especially', 'excess', 'black', 'alternatively', 'seen', '18th', 'wide', 'variety', 'medical', 'explanations', 'including', 'lack', 'sexual', 'much', 'sexual', 'physical', 'injuries', 'breast', 'various', 'forms', 'lymphatic', 'either', 'internal', 'due', 'restrictive', '19th', 'surgeon', 'said', 'fear', 'cancer', 'caused', 'learned', 'example', 'accounted', 'breast', 'tendency', 'run', 'although', 'breast', 'cancer', 'known', 'uncommon', '19th', 'improvements', 'control', 'deadly', 'infectious', 'diseases', 'resulted', 'dramatic', 'increases', 'women', 'died', 'young', 'developed', 'breast', 'early', 'frequent', 'breastfeeding', 'probably', 'reduced', 'rate', 'breast', 'cancer', 'development', 'women', 'survive', 'middle', 'medicine', 'believed', 'cause', 'rather', 'surgery', 'carried', 'high', 'mortality', 'preferred', 'treatments', 'tended', 'pharmacological', 'rather', 'herbal', 'mineral', 'especially', 'involving', 'poison', 'relatively', 'mastectomy', 'breast', 'cancer', 'performed', 'least', 'early', 'ad', 'proposed', 'court', 'physician', 'doctors', 'achieved', 'greater', 'understanding', 'circulatory', 'system', '17th', 'link', 'breast', 'spread', 'lymph', 'nodes', 'french', 'surgeon', 'performed', 'total', 'mastectomies', 'included', 'removing', 'axillary', 'lymph', 'recognized', 'reduced', 'work', 'built', 'another', 'french', 'additionally', 'removed', 'pectoral', 'muscle', 'underlying', 'judged', 'greatly', 'improved', 'surgeon', 'bell', 'removal', 'entire', 'even', 'portion', 'successful', 'work', 'carried', 'started', 'performing', 'radical', 'mastectomies', 'helped', 'greatly', 'advances', 'general', 'surgical', 'aseptic', 'technique', 'radical', 'mastectomy', 'often', 'involved', 'removing', 'associated', 'lymph', 'underlying', 'chest', 'often', 'led', 'pain', 'seen', 'necessary', 'order', 'prevent', 'cancer', 'radical', 'survival', 'rates', 'surgery', 'raised', 'rate', 'extending', 'promoted', 'taking', 'even', 'survival', 'rates', 'proved', 'equal', 'radical', 'radical', 'mastectomies', 'remained', 'standard', 'care', 'america', 'often', 'followed', 'radiation', 'generally', 'adopted', 'one', 'reason', 'striking', 'difference', 'approach', 'may', 'structure', 'medical', 'descended', 'barber', 'held', 'less', 'surgeon', 'medical', 'far', 'women', 'less', 'one', 'percent', 'american', 'surgical', 'oncologists', 'breast', 'cancer', 'wards', 'medical', 'staff', 'half', 'american', 'health', 'insurance', 'companies', 'also', 'paid', 'surgeons', 'perform', 'radical', 'mastectomies', 'perform', 'breast', 'cancer', 'staging', 'systems', 'developed', 'new', 'understanding', 'metastasis', 'led', 'cancer', 'systemic', 'illness', 'well', 'localized', 'sparing', 'procedures', 'developed', 'proved', 'equally', 'chemotherapy', 'developed', 'world', 'war', 'prominent', 'women', 'died', 'breast', 'cancer', 'include', 'mother', 'mother', 'first', 'study', 'breast', 'cancer', 'done', 'published', 'comparative', 'study', 'breast', 'cancer', 'cases', 'controls', 'background', 'lifestyle', 'thousands', 'women', 'successfully', 'completed', 'standard', 'treatment', 'demanded', 'received', 'bone', 'marrow', 'thinking', 'lead', 'better', 'proved', 'completely', 'women', 'died', 'reports', 'health', 'study', 'conclusions', 'health', 'trial', 'conclusively', 'proved', 'hormone', 'replacement', 'therapy', 'significantly', 'increased', 'incidence', 'breast', '20th', 'breast', 'cancer', 'feared', 'discussed', 'little', 'safely', 'done', 'primitive', 'surgical', 'women', 'tended', 'suffer', 'silently', 'rather', 'seeking', 'surgery', 'survival', 'rates', 'women', 'began', 'raising', 'awareness', 'disease', 'possibility', 'successful', 'field', 'run', 'american', 'society', 'control', 'cancer', 'american', 'cancer', 'one', 'first', 'organized', 'first', 'support', 'called', 'began', 'providing', 'visits', 'women', 'survived', 'breast', 'breast', 'cancer', 'movement', 'developed', 'larger', 'movements', 'health', 'movement', '20th', 'series', 'political', 'educational', 'partly', 'inspired', 'socially', 'effective', 'aids', 'awareness', 'resulted', 'widespread', 'acceptance', 'second', 'opinions', 'less', 'invasive', 'surgical', 'support', 'advances', 'pink', 'ribbon', 'prominent', 'breast', 'cancer', 'pink', 'can', 'made', 'sometimes', 'sold', 'much', 'like', 'may', 'worn', 'honor', 'diagnosed', 'breast', 'identify', 'products', 'manufacturer', 'like', 'sell', 'interested', 'breast', 'pink', 'ribbon', 'associated', 'individual', 'faith', 'focus', 'emotionally', 'ultimate', 'vision', 'cure', 'breast', 'rather', 'path', 'current', 'knowledge', 'future', 'wearing', 'displaying', 'pink', 'ribbon', 'practice', 'kind', 'practical', 'positive', 'also', 'people', 'wear', 'pink', 'ribbon', 'show', 'good', 'will', 'towards', 'women', 'breast', 'practical', 'like', 'patient', 'rights', 'say', 'nature', 'pink', 'pink', 'consumption', 'society', 'lack', 'progress', 'preventing', 'breast', 'also', 'gender', 'women', 'breast', 'cancer', 'action', 'said', 'pink', 'promote', 'products', 'cause', 'breast', 'alcoholic', 'breast', 'cancer', 'also', 'known', 'pink', 'ribbon', 'set', 'values', 'surround', 'shape', 'breast', 'cancer', 'dominant', 'values', 'breast', 'cancer', 'breast', 'cancer', 'therapy', 'viewed', 'passage', 'rather', 'fit', 'woman', 'breast', 'cancer', 'needs', 'normalize', 'minimize', 'disruption', 'health', 'issues', 'cause', 'anyone', 'must', 'cultural', 'people', 'conform', 'model', 'given', 'social', 'case', 'cancer', 'women', 'model', 'culture', 'treating', 'adult', 'women', 'like', 'little', 'evidenced', 'pink', 'bears', 'given', 'adult', 'primary', 'purposes', 'goals', 'breast', 'cancer', 'culture', 'maintain', 'breast', 'dominance', 'health', 'promote', 'appearance', 'society', 'something', 'effective', 'breast', 'sustain', 'expand', 'financial', 'power', 'breast', 'cancer', 'compared', 'diseases', 'breast', 'cancer', 'receives', 'proportionately', 'greater', 'share', 'resources', 'mp', 'house', 'commons', 'united', 'party', 'group', 'cancer', 'stated', 'treatment', 'doubt', 'breast', 'cancer', 'get', 'better', 'treatment', 'terms', 'bed', 'facilities', 'doctors', 'breast', 'cancer', 'also', 'receives', 'significantly', 'media', 'coverage', 'equally', 'prevalent', 'study', 'prostate', 'showing', 'breast', 'cancer', 'stories', 'one', 'covering', 'cancer', 'ultimately', 'concern', 'favoring', 'breast', 'cancer', 'disproportionate', 'research', 'behalf', 'may', 'well', 'lives', 'partly', 'relatively', 'high', 'prevalence', 'survival', 'research', 'towards', 'breast', 'studied', 'little', 'except', 'women', 'breast', 'one', 'result', 'breast', 'high', 'visibility', 'results', 'can', 'sometimes', 'claim', 'one', 'eight', 'women', 'will', 'diagnosed', 'breast', 'cancer', 'claim', 'depends', 'assumption', 'woman', 'will', 'die', 'disease', 'age', 'obscures', 'ten', 'times', 'many', 'women', 'will', 'die', 'heart', 'disease', 'stroke', 'breast', 'emphasis', 'breast', 'cancer', 'screening', 'may', 'harming', 'women', 'unnecessary', 'diagnosed', 'breast', 'cancers', 'might', 'screening', 'mammography', 'efficiently', 'finds', 'asymptomatic', 'breast', 'cancers', 'even', 'serious', 'according', 'institute', 'health', 'policy', 'clinical', 'research', 'screening', 'mammography', 'taken', 'approach', 'says', 'best', 'test', 'one', 'finds', 'rather', 'one', 'finds', 'dangerous', 'breast', 'cancers', 'occur', 'pregnancy', 'rate', 'breast', 'cancers', 'women', 'breast', 'cancer', 'becomes', 'common', 'years', 'following', 'pregnancy', 'becomes', 'less', 'common', 'among', 'general', 'cancers', 'known', 'postpartum', 'breast', 'cancer', 'worse', 'outcomes', 'including', 'increased', 'risk', 'distant', 'spread', 'disease', 'cancers', 'found', 'shortly', 'pregnancy', 'appear', 'approximately', 'rate', 'cancers', 'women', 'similar', 'diagnosing', 'new', 'cancer', 'pregnant', 'woman', 'part', 'symptoms', 'commonly', 'assumed', 'normal', 'discomfort', 'associated', 'cancer', 'typically', 'discovered', 'somewhat', 'later', 'stage', 'average', 'many', 'pregnant', 'recently', 'pregnant', 'imaging', 'mris', 'resonance', 'ct', 'mammograms', 'fetal', 'considered', 'safe', 'pet', 'scans', 'treatment', 'generally', 'radiation', 'normally', 'avoided', 'especially', 'fetal', 'dose', 'might', 'exceed', 'treatments', 'postponed', 'birth', 'cancer', 'diagnosed', 'late', 'early', 'deliveries', 'speed', 'start', 'treatment', 'surgery', 'generally', 'considered', 'safe', 'especially', 'certain', 'chemotherapy', 'drugs', 'given', 'first', 'increase', 'risk', 'birth', 'defects', 'pregnancy', 'loss', 'abortions', 'elective', 'abortions', 'required', 'improve', 'likelihood', 'mother', 'surviving', 'radiation', 'treatments', 'may', 'interfere', 'ability', 'breastfeed', 'baby', 'reduces', 'ability', 'breast', 'produce', 'milk', 'increases', 'risk', 'chemotherapy', 'given', 'many', 'drugs', 'pass', 'breast', 'milk', 'harm', 'regarding', 'future', 'pregnancy', 'among', 'breast', 'cancer', 'often', 'fear', 'cancer', 'many', 'still', 'regard', 'pregnancy', 'represent', 'life', 'breast', 'cancer', 'birth', 'control', 'methods', 'used', 'methods', 'depot', 'medroxyprogesterone', 'iud', 'pills', 'poorly', 'investigated', 'possible', 'increased', 'risk', 'cancer', 'may', 'used', 'positive', 'effects', 'outweigh', 'possible', 'breast', 'cancer', 'recommended', 'first', 'consider', 'options', 'menopausal', 'bisphosphonates', 'selective', 'estrogen', 'receptor', 'vaginal', 'estrogen', 'local', 'studies', 'systemic', 'hormone', 'replacement', 'therapy', 'breast', 'cancer', 'generally', 'hormone', 'replacement', 'necessary', 'breast', 'therapy', 'estrogen', 'therapy', 'intrauterine', 'device', 'may', 'safer', 'options', 'combined', 'systemic', 'treatments', 'evaluated', 'includes', 'individual', 'combinations', 'surgical', 'radiation', 'techniques', 'investigations', 'include', 'new', 'types', 'targeted', 'cancer', 'latest', 'research', 'reported', 'annually', 'meetings', 'american', 'society', 'clinical', 'breast', 'cancer', 'oncology', 'conference', 'studies', 'reviewed', 'professional', 'formulated', 'guidelines', 'specific', 'treatment', 'groups', 'risk', 'also', 'studied', 'way', 'reduce', 'risk', 'breast', 'cancer', 'medications', 'related', 'vitamin', 'cryoablation', 'studied', 'see', 'substitute', 'lumpectomy', 'small', 'tentative', 'evidence', 'tumors', 'less', 'may', 'also', 'used', 'surgery', 'another', 'review', 'states', 'cryoablation', 'looks', 'early', 'breast', 'cancer', 'small', 'considerable', 'part', 'current', 'knowledge', 'breast', 'carcinomas', 'based', 'studies', 'performed', 'cell', 'lines', 'derived', 'breast', 'provide', 'source', 'homogenous', 'free', 'stromal', 'often', 'easily', 'cultured', 'simple', 'standard', 'first', 'breast', 'cancer', 'cell', 'line', 'established', 'since', 'despite', 'sustained', 'work', 'number', 'permanent', 'lines', 'obtained', 'strikingly', 'low', 'attempts', 'culture', 'breast', 'cancer', 'cell', 'lines', 'primary', 'tumors', 'largely', 'poor', 'efficiency', 'often', 'due', 'technical', 'difficulties', 'associated', 'extraction', 'viable', 'tumor', 'cells', 'surrounding', 'available', 'breast', 'cancer', 'cell', 'lines', 'issued', 'metastatic', 'mainly', 'pleural', 'effusions', 'provided', 'generally', 'large', 'numbers', 'viable', 'tumor', 'cells', 'little', 'contamination', 'tumor', 'stroma', 'many', 'currently', 'used', 'bcc', 'lines', 'established', 'late', 'namely', 'account', 'reporting', 'studies', 'mentioned', 'breast', 'cancer', 'cell', 'concluded', 'factors', 'implicated', 'breast', 'specifically', 'process', 'cell', 'motility', 'basis', 'metastasis', 'breast', 'carcinoma', 'inhibitor', 'cell', 'expression', 'increase', 'breast', 'cancer', 'cell', 'invasion', 'inhibits', 'expression', 'blunt', 'cell', 'useful', 'metabolic', 'markers', 'breast', 'cancer', 'estrogen', 'progesterone', 'used', 'predict', 'response', 'hormone', 'new', 'potentially', 'new', 'markers', 'breast', 'cancer', 'include', 'identify', 'people', 'high', 'risk', 'developing', 'breast', 'response', 'therapeutic', 'plasminogen', 'assessing']\n"
          ]
        }
      ],
      "source": [
        "print(wikidata[0])\n",
        "print(wikidocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te_XDGn_VFlV"
      },
      "source": [
        "Create a list of string for clinical notes, and each element is a string for a note (including intersected words).  \n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsxeKIhJVEI8"
      },
      "outputs": [],
      "source": [
        "notedata=[] # each element is a string, each string contains words for a note\n",
        "for i in notesdocuments: # for each element (list) in list\n",
        "  temp=''\n",
        "  for j in i: # for each word\n",
        "    temp=temp+j+\" \" # create a string, words for a note separated by a space\n",
        "  notedata.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRoisHIrVlYS"
      },
      "source": [
        "Print a sample string (Compare with notesdocuments[0])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D9I-pHfVoOq",
        "outputId": "e93d59aa-82a1-4369-e8b1-4d1f09ad6797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "admission date discharge date date birth sex m service medicine allergies patient recorded known allergies drugs name un chief complaint fever cough weakness major surgical invasive procedure rigid bronchoscopy endobronchial debulking squamous cell lung mass endotracheal intubation history present illness mr known man squamous cell lung cancer admitted hypotension fever secondary pneumonia patient back pain weight loss productive cough mild hemoptysis beginning month month time revealed opacity ct revealed mass x cm concerning malignancy patient underwent flexible bronchoscopy endobronchial biopsies linear endobronchial ultrasound lymph node biopsies procedures performed dr last name endobronchial biopsies revealed squamous cell carcinoma patient presented dr first name office worsening cough fever weakness office sbp breathing ra diminished l upper lung breath sounds per pcp revealed new infiltrate peripheral mass lesions likely consistent referred pcp time done also showed likely patient given gram iv vancomycin mg sent started continued hypotension proven fluid responsive breathing l oxygen mass thought obstructing airway endobronchial debulking performed prior transfer floor suggested consult patients procedure noted one point mechanical lowered patient transferred floor saturation past medical history osteoarthritis hip social history lives alone always lived ma works post office letter carrier prior employment government smoked x quit drinks family history prostate cancer father deceased hypertension mother alive physical exam admission vs t bp p r sat nad looks comfortable mm slightly dry lesions exudate noted neck cervical supraclavicular cv audible lungs focal area wheezing rales left upper lobe scattered left lung whole right lung dullness percussion left upper lung noted soft noted pitting edema present legs bilaterally shin nontender palpation neuro cns intact sensation intact discharge vs general comfortable appearing nad clear cv normal breathing minimal diffuse wheezing throughout worse decreased left lung abdominal extremities pulses pertinent results admission labs blood blood blood blood blood blood ct chest contrast enlarging left upper lobe mass left upper bronchial stenosis level stenosis pulmonary artery less posterior bronchus left upper lobe pulmonary artery pneumonia new bilateral small pleural effusions new left basal pneumonia probably aspiration left upper lobe contents interval results mri head without contrast evidence intracranial metastatic disease ct chest without contrast progression left upper lobe atelectasis consolidation status post recent debridement gas area previously seen lesion left upper lobe bronchus stenosis left upper lobe bronchus however mild increase segment peripheral dilated bronchioles progression consolidation left lower lobe possibly related aspiration part compressive atelectasis due enlarging left pleural effusion small right pleural effusion adjacent atelectasis unchanged lymphadenopathy pleural fluid analysis organisms fluid culture without growth anaerobic culture without growth fdg tumor imaging fdg avid left upper lobe consolidation focally increased fdg avidity surrounding narrowed left upper lobe bronchus compatible known squamous cell carcinoma collapse consolidation left upper lobe fdg avid consolidation left lower lobe likely pneumonia aspiration moderate left small right pleural effusions fdg avid mediastinal lymphadenopathy trace free fluid pelvis chest ct contrast interval improvement left upper lobe pneumonia improved anterior aspect left upper lobe foci gas within parenchyma similar appearance potential communication left upper lobe segmental bronchus collections air possible known mass lesion well evaluated due surrounding consolidation slight interval increase left pleural effusion resolution right pleural effusion extensive lymphadenopathy unchanged unclear component reactive versus involved malignancy ultrasound doppler lower extremities thrombosis bilateral right left posterior tibialis veins blood cultures repeated times admission always without growth chest repeated times throughout admission demonstrated left upper lobe pneumonia little improvement images later stay revealed almost complete white left lung collapse left lower lobe detected late hospital course mediastinal lymph node biopsy lymph node biopsy ad malignancy identified lymph node biopsy ef malignancy identified lymph node level biopsy malignancy identified lymph node biopsy malignancy identified lymph node biopsy j malignancy identified discharge results blood blood blood brief hospital course man left upper lobe squamous cell carcinoma presented fevers leukocytosis hypotension setting hypotension patient presented systolic blood pressures patient sent pressure came following mental status never compromised lactate patient subsequently transferred general medicine floors patient continued intermittent hypotensive episodes stay associated severe night sweats fevers infectious work negative multiple fevers attributed tumor started scheduled acetaminophen continuous ivf patient subsequent episodes hypotension pneumonia setting known left upper lobe squamous cell carcinoma radiographic evidence high concern pneumonia patient started vancomycin switched interventional performed rigid bronchoscopy bulk resection antibiotics switched metronidazole per recommendations patient subsequently began radiation therapy effort shrink tumor hopes definitive therapy pneumonia repeat chest throughout admission demonstrated little improvement subsequent collapse left lower lobe progression left upper lobe patient antibiotics discontinued discharge patient completed days antibiotics little concern infectious etiology fevers per left upper lobe poorly differentiated squamous cell carcinoma patient diagnosed several weeks prior admission time lymph nodes clear brain mri showed early hospitalization revealed evidence brain metastases pet scan revealed mediastinal lymph nodes suspicious metastases patient subsequently began treatments shrink tumor enough treat pneumonia thoracic surgery consulted performed sampled lymph nodes negative malignant cells patient discharged instructions follow oncologist well chest disease center pleural effusion pleural fluid negative malignant cells thoracentesis cultures showing empyema exudative parapneumonic effusion suspected evidence empyema patient treated pneumonia per partial occlusive patient mild lower extremity edema admission ultrasound study revealed bilateral right left posterior tibialis veins patient initially treated heparin drip due concerns renal function patient eventually treated mg subcutaneous injection twice daily discharged home instructions continue injections thrombocytosis likely acute phase reactant setting ongoing infection inflammation patients platelet counts remained elevated throughout admission anemia hematocrit stable throughout admission iron studies consistent anemia chronic inflammation medications admission none discharge medications aerosol inhalation every hours needed shortness breath wheezing ml syrup mls every hours needed cough weeks ml bottle fluconazole mg tablet one tablet every hours days tablets acetaminophen mg tablet two tablet every hours tablets ml syringe seventy mg subcutaneous twice day syringes lorazepam mg tablet one tablet every hours needed anxiety tablets mg tablet one tablet daily daily tablets morphine mg tablet one tablet every hours needed pain tablets discharge disposition home service facility care greater location un discharge diagnosis primary diagnoses pneumonia diagnoses squamous cell lung cancer discharge condition mental status clear coherent level consciousness alert interactive activity status ambulatory independent discharge instructions mr known pleasure taking care admitted hospital pneumonia fever low blood pressure also known hypotension known lung cancer obstructing airways leading known pneumonia procedure lung cancer attempt open airways order allow pneumonia resolve received antibiotics will need continue outpatient hospital began workup staging lung cancer brain mri show metastases scheduled scan outpatient will need follow directions oral contrast provided prior discharge follow dr first name medical oncology dr first name thoracic surgery dr last name radiation oncology following changes made medications start using inhaler every hours needed shortness breath start using cough syrup ml mouth every six hours needed cough start using fluconazole mg mouth day medication sore throat will need take days start taking acetaminophen mg take two tablets mouth every six hours needed fever exceed tablets per day start using mg subcutaneous injection twice day medication blood clots found legs start using lorazepam mg mouth every four hours needed anxiety start taking mg mouth day medication helps reflux start taking morphine sulfate ir mg mouth every four hours needed pain instructions department chest disease center name last name lf first initial last name call thoracic oncology program schedule upcoming appointment dr first name days hospital discharge call office number listed make appointment location address location un hospital ward name location un numeric phone department pulmonary function lab pulmonary function lab building hospital ward name building ward name complex location un east best main department radiology building cc location un hospital west best street scan department radiology location hospital ward name center location un east department primary care name dr first doctor last name location location un address country 3rd fl numeric phone completed \n",
            "['admission', 'date', 'discharge', 'date', 'date', 'birth', 'sex', 'm', 'service', 'medicine', 'allergies', 'patient', 'recorded', 'known', 'allergies', 'drugs', 'name', 'un', 'chief', 'complaint', 'fever', 'cough', 'weakness', 'major', 'surgical', 'invasive', 'procedure', 'rigid', 'bronchoscopy', 'endobronchial', 'debulking', 'squamous', 'cell', 'lung', 'mass', 'endotracheal', 'intubation', 'history', 'present', 'illness', 'mr', 'known', 'man', 'squamous', 'cell', 'lung', 'cancer', 'admitted', 'hypotension', 'fever', 'secondary', 'pneumonia', 'patient', 'back', 'pain', 'weight', 'loss', 'productive', 'cough', 'mild', 'hemoptysis', 'beginning', 'month', 'month', 'time', 'revealed', 'opacity', 'ct', 'revealed', 'mass', 'x', 'cm', 'concerning', 'malignancy', 'patient', 'underwent', 'flexible', 'bronchoscopy', 'endobronchial', 'biopsies', 'linear', 'endobronchial', 'ultrasound', 'lymph', 'node', 'biopsies', 'procedures', 'performed', 'dr', 'last', 'name', 'endobronchial', 'biopsies', 'revealed', 'squamous', 'cell', 'carcinoma', 'patient', 'presented', 'dr', 'first', 'name', 'office', 'worsening', 'cough', 'fever', 'weakness', 'office', 'sbp', 'breathing', 'ra', 'diminished', 'l', 'upper', 'lung', 'breath', 'sounds', 'per', 'pcp', 'revealed', 'new', 'infiltrate', 'peripheral', 'mass', 'lesions', 'likely', 'consistent', 'referred', 'pcp', 'time', 'done', 'also', 'showed', 'likely', 'patient', 'given', 'gram', 'iv', 'vancomycin', 'mg', 'sent', 'started', 'continued', 'hypotension', 'proven', 'fluid', 'responsive', 'breathing', 'l', 'oxygen', 'mass', 'thought', 'obstructing', 'airway', 'endobronchial', 'debulking', 'performed', 'prior', 'transfer', 'floor', 'suggested', 'consult', 'patients', 'procedure', 'noted', 'one', 'point', 'mechanical', 'lowered', 'patient', 'transferred', 'floor', 'saturation', 'past', 'medical', 'history', 'osteoarthritis', 'hip', 'social', 'history', 'lives', 'alone', 'always', 'lived', 'ma', 'works', 'post', 'office', 'letter', 'carrier', 'prior', 'employment', 'government', 'smoked', 'x', 'quit', 'drinks', 'family', 'history', 'prostate', 'cancer', 'father', 'deceased', 'hypertension', 'mother', 'alive', 'physical', 'exam', 'admission', 'vs', 't', 'bp', 'p', 'r', 'sat', 'nad', 'looks', 'comfortable', 'mm', 'slightly', 'dry', 'lesions', 'exudate', 'noted', 'neck', 'cervical', 'supraclavicular', 'cv', 'audible', 'lungs', 'focal', 'area', 'wheezing', 'rales', 'left', 'upper', 'lobe', 'scattered', 'left', 'lung', 'whole', 'right', 'lung', 'dullness', 'percussion', 'left', 'upper', 'lung', 'noted', 'soft', 'noted', 'pitting', 'edema', 'present', 'legs', 'bilaterally', 'shin', 'nontender', 'palpation', 'neuro', 'cns', 'intact', 'sensation', 'intact', 'discharge', 'vs', 'general', 'comfortable', 'appearing', 'nad', 'clear', 'cv', 'normal', 'breathing', 'minimal', 'diffuse', 'wheezing', 'throughout', 'worse', 'decreased', 'left', 'lung', 'abdominal', 'extremities', 'pulses', 'pertinent', 'results', 'admission', 'labs', 'blood', 'blood', 'blood', 'blood', 'blood', 'blood', 'ct', 'chest', 'contrast', 'enlarging', 'left', 'upper', 'lobe', 'mass', 'left', 'upper', 'bronchial', 'stenosis', 'level', 'stenosis', 'pulmonary', 'artery', 'less', 'posterior', 'bronchus', 'left', 'upper', 'lobe', 'pulmonary', 'artery', 'pneumonia', 'new', 'bilateral', 'small', 'pleural', 'effusions', 'new', 'left', 'basal', 'pneumonia', 'probably', 'aspiration', 'left', 'upper', 'lobe', 'contents', 'interval', 'results', 'mri', 'head', 'without', 'contrast', 'evidence', 'intracranial', 'metastatic', 'disease', 'ct', 'chest', 'without', 'contrast', 'progression', 'left', 'upper', 'lobe', 'atelectasis', 'consolidation', 'status', 'post', 'recent', 'debridement', 'gas', 'area', 'previously', 'seen', 'lesion', 'left', 'upper', 'lobe', 'bronchus', 'stenosis', 'left', 'upper', 'lobe', 'bronchus', 'however', 'mild', 'increase', 'segment', 'peripheral', 'dilated', 'bronchioles', 'progression', 'consolidation', 'left', 'lower', 'lobe', 'possibly', 'related', 'aspiration', 'part', 'compressive', 'atelectasis', 'due', 'enlarging', 'left', 'pleural', 'effusion', 'small', 'right', 'pleural', 'effusion', 'adjacent', 'atelectasis', 'unchanged', 'lymphadenopathy', 'pleural', 'fluid', 'analysis', 'organisms', 'fluid', 'culture', 'without', 'growth', 'anaerobic', 'culture', 'without', 'growth', 'fdg', 'tumor', 'imaging', 'fdg', 'avid', 'left', 'upper', 'lobe', 'consolidation', 'focally', 'increased', 'fdg', 'avidity', 'surrounding', 'narrowed', 'left', 'upper', 'lobe', 'bronchus', 'compatible', 'known', 'squamous', 'cell', 'carcinoma', 'collapse', 'consolidation', 'left', 'upper', 'lobe', 'fdg', 'avid', 'consolidation', 'left', 'lower', 'lobe', 'likely', 'pneumonia', 'aspiration', 'moderate', 'left', 'small', 'right', 'pleural', 'effusions', 'fdg', 'avid', 'mediastinal', 'lymphadenopathy', 'trace', 'free', 'fluid', 'pelvis', 'chest', 'ct', 'contrast', 'interval', 'improvement', 'left', 'upper', 'lobe', 'pneumonia', 'improved', 'anterior', 'aspect', 'left', 'upper', 'lobe', 'foci', 'gas', 'within', 'parenchyma', 'similar', 'appearance', 'potential', 'communication', 'left', 'upper', 'lobe', 'segmental', 'bronchus', 'collections', 'air', 'possible', 'known', 'mass', 'lesion', 'well', 'evaluated', 'due', 'surrounding', 'consolidation', 'slight', 'interval', 'increase', 'left', 'pleural', 'effusion', 'resolution', 'right', 'pleural', 'effusion', 'extensive', 'lymphadenopathy', 'unchanged', 'unclear', 'component', 'reactive', 'versus', 'involved', 'malignancy', 'ultrasound', 'doppler', 'lower', 'extremities', 'thrombosis', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'blood', 'cultures', 'repeated', 'times', 'admission', 'always', 'without', 'growth', 'chest', 'repeated', 'times', 'throughout', 'admission', 'demonstrated', 'left', 'upper', 'lobe', 'pneumonia', 'little', 'improvement', 'images', 'later', 'stay', 'revealed', 'almost', 'complete', 'white', 'left', 'lung', 'collapse', 'left', 'lower', 'lobe', 'detected', 'late', 'hospital', 'course', 'mediastinal', 'lymph', 'node', 'biopsy', 'lymph', 'node', 'biopsy', 'ad', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'ef', 'malignancy', 'identified', 'lymph', 'node', 'level', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', 'biopsy', 'j', 'malignancy', 'identified', 'discharge', 'results', 'blood', 'blood', 'blood', 'brief', 'hospital', 'course', 'man', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'presented', 'fevers', 'leukocytosis', 'hypotension', 'setting', 'hypotension', 'patient', 'presented', 'systolic', 'blood', 'pressures', 'patient', 'sent', 'pressure', 'came', 'following', 'mental', 'status', 'never', 'compromised', 'lactate', 'patient', 'subsequently', 'transferred', 'general', 'medicine', 'floors', 'patient', 'continued', 'intermittent', 'hypotensive', 'episodes', 'stay', 'associated', 'severe', 'night', 'sweats', 'fevers', 'infectious', 'work', 'negative', 'multiple', 'fevers', 'attributed', 'tumor', 'started', 'scheduled', 'acetaminophen', 'continuous', 'ivf', 'patient', 'subsequent', 'episodes', 'hypotension', 'pneumonia', 'setting', 'known', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'radiographic', 'evidence', 'high', 'concern', 'pneumonia', 'patient', 'started', 'vancomycin', 'switched', 'interventional', 'performed', 'rigid', 'bronchoscopy', 'bulk', 'resection', 'antibiotics', 'switched', 'metronidazole', 'per', 'recommendations', 'patient', 'subsequently', 'began', 'radiation', 'therapy', 'effort', 'shrink', 'tumor', 'hopes', 'definitive', 'therapy', 'pneumonia', 'repeat', 'chest', 'throughout', 'admission', 'demonstrated', 'little', 'improvement', 'subsequent', 'collapse', 'left', 'lower', 'lobe', 'progression', 'left', 'upper', 'lobe', 'patient', 'antibiotics', 'discontinued', 'discharge', 'patient', 'completed', 'days', 'antibiotics', 'little', 'concern', 'infectious', 'etiology', 'fevers', 'per', 'left', 'upper', 'lobe', 'poorly', 'differentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'diagnosed', 'several', 'weeks', 'prior', 'admission', 'time', 'lymph', 'nodes', 'clear', 'brain', 'mri', 'showed', 'early', 'hospitalization', 'revealed', 'evidence', 'brain', 'metastases', 'pet', 'scan', 'revealed', 'mediastinal', 'lymph', 'nodes', 'suspicious', 'metastases', 'patient', 'subsequently', 'began', 'treatments', 'shrink', 'tumor', 'enough', 'treat', 'pneumonia', 'thoracic', 'surgery', 'consulted', 'performed', 'sampled', 'lymph', 'nodes', 'negative', 'malignant', 'cells', 'patient', 'discharged', 'instructions', 'follow', 'oncologist', 'well', 'chest', 'disease', 'center', 'pleural', 'effusion', 'pleural', 'fluid', 'negative', 'malignant', 'cells', 'thoracentesis', 'cultures', 'showing', 'empyema', 'exudative', 'parapneumonic', 'effusion', 'suspected', 'evidence', 'empyema', 'patient', 'treated', 'pneumonia', 'per', 'partial', 'occlusive', 'patient', 'mild', 'lower', 'extremity', 'edema', 'admission', 'ultrasound', 'study', 'revealed', 'bilateral', 'right', 'left', 'posterior', 'tibialis', 'veins', 'patient', 'initially', 'treated', 'heparin', 'drip', 'due', 'concerns', 'renal', 'function', 'patient', 'eventually', 'treated', 'mg', 'subcutaneous', 'injection', 'twice', 'daily', 'discharged', 'home', 'instructions', 'continue', 'injections', 'thrombocytosis', 'likely', 'acute', 'phase', 'reactant', 'setting', 'ongoing', 'infection', 'inflammation', 'patients', 'platelet', 'counts', 'remained', 'elevated', 'throughout', 'admission', 'anemia', 'hematocrit', 'stable', 'throughout', 'admission', 'iron', 'studies', 'consistent', 'anemia', 'chronic', 'inflammation', 'medications', 'admission', 'none', 'discharge', 'medications', 'aerosol', 'inhalation', 'every', 'hours', 'needed', 'shortness', 'breath', 'wheezing', 'ml', 'syrup', 'mls', 'every', 'hours', 'needed', 'cough', 'weeks', 'ml', 'bottle', 'fluconazole', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'days', 'tablets', 'acetaminophen', 'mg', 'tablet', 'two', 'tablet', 'every', 'hours', 'tablets', 'ml', 'syringe', 'seventy', 'mg', 'subcutaneous', 'twice', 'day', 'syringes', 'lorazepam', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'anxiety', 'tablets', 'mg', 'tablet', 'one', 'tablet', 'daily', 'daily', 'tablets', 'morphine', 'mg', 'tablet', 'one', 'tablet', 'every', 'hours', 'needed', 'pain', 'tablets', 'discharge', 'disposition', 'home', 'service', 'facility', 'care', 'greater', 'location', 'un', 'discharge', 'diagnosis', 'primary', 'diagnoses', 'pneumonia', 'diagnoses', 'squamous', 'cell', 'lung', 'cancer', 'discharge', 'condition', 'mental', 'status', 'clear', 'coherent', 'level', 'consciousness', 'alert', 'interactive', 'activity', 'status', 'ambulatory', 'independent', 'discharge', 'instructions', 'mr', 'known', 'pleasure', 'taking', 'care', 'admitted', 'hospital', 'pneumonia', 'fever', 'low', 'blood', 'pressure', 'also', 'known', 'hypotension', 'known', 'lung', 'cancer', 'obstructing', 'airways', 'leading', 'known', 'pneumonia', 'procedure', 'lung', 'cancer', 'attempt', 'open', 'airways', 'order', 'allow', 'pneumonia', 'resolve', 'received', 'antibiotics', 'will', 'need', 'continue', 'outpatient', 'hospital', 'began', 'workup', 'staging', 'lung', 'cancer', 'brain', 'mri', 'show', 'metastases', 'scheduled', 'scan', 'outpatient', 'will', 'need', 'follow', 'directions', 'oral', 'contrast', 'provided', 'prior', 'discharge', 'follow', 'dr', 'first', 'name', 'medical', 'oncology', 'dr', 'first', 'name', 'thoracic', 'surgery', 'dr', 'last', 'name', 'radiation', 'oncology', 'following', 'changes', 'made', 'medications', 'start', 'using', 'inhaler', 'every', 'hours', 'needed', 'shortness', 'breath', 'start', 'using', 'cough', 'syrup', 'ml', 'mouth', 'every', 'six', 'hours', 'needed', 'cough', 'start', 'using', 'fluconazole', 'mg', 'mouth', 'day', 'medication', 'sore', 'throat', 'will', 'need', 'take', 'days', 'start', 'taking', 'acetaminophen', 'mg', 'take', 'two', 'tablets', 'mouth', 'every', 'six', 'hours', 'needed', 'fever', 'exceed', 'tablets', 'per', 'day', 'start', 'using', 'mg', 'subcutaneous', 'injection', 'twice', 'day', 'medication', 'blood', 'clots', 'found', 'legs', 'start', 'using', 'lorazepam', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'anxiety', 'start', 'taking', 'mg', 'mouth', 'day', 'medication', 'helps', 'reflux', 'start', 'taking', 'morphine', 'sulfate', 'ir', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'pain', 'instructions', 'department', 'chest', 'disease', 'center', 'name', 'last', 'name', 'lf', 'first', 'initial', 'last', 'name', 'call', 'thoracic', 'oncology', 'program', 'schedule', 'upcoming', 'appointment', 'dr', 'first', 'name', 'days', 'hospital', 'discharge', 'call', 'office', 'number', 'listed', 'make', 'appointment', 'location', 'address', 'location', 'un', 'hospital', 'ward', 'name', 'location', 'un', 'numeric', 'phone', 'department', 'pulmonary', 'function', 'lab', 'pulmonary', 'function', 'lab', 'building', 'hospital', 'ward', 'name', 'building', 'ward', 'name', 'complex', 'location', 'un', 'east', 'best', 'main', 'department', 'radiology', 'building', 'cc', 'location', 'un', 'hospital', 'west', 'best', 'street', 'scan', 'department', 'radiology', 'location', 'hospital', 'ward', 'name', 'center', 'location', 'un', 'east', 'department', 'primary', 'care', 'name', 'dr', 'first', 'doctor', 'last', 'name', 'location', 'location', 'un', 'address', 'country', '3rd', 'fl', 'numeric', 'phone', 'completed']\n"
          ]
        }
      ],
      "source": [
        "print(notedata[0])\n",
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-zZsHsfZETc"
      },
      "source": [
        "Create 2 word matrices (intersected words). One is for Wiki documents, and the other is for clinical notes.  \n",
        "**notevec.npy**: A matrix of intersected words for clinical notes.  \n",
        "**wikivec.npy**: A matrix of intersected words for Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBfUhADjZVha"
      },
      "outputs": [],
      "source": [
        "# create a matrix of token counts\n",
        "vect = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "# transfer list of string to matrix, if a word exists in a string, set value to 1\n",
        "binaryn = vect.fit_transform(notedata)\n",
        "binaryn=binaryn.A # Return self as an ndarray object.\n",
        "binaryn=np.array(binaryn,dtype=float)\n",
        "\n",
        "vect2 = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "binaryk = vect2.fit_transform(wikidata)\n",
        "binaryk=binaryk.A\n",
        "binaryk=np.array(binaryk,dtype=float)\n",
        "\n",
        "\n",
        "np.save('notevec',binaryn) # save numpy array as a file\n",
        "np.save('wikivec',binaryk) # save numpy array as a file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDe_Hdt2Zy7s"
      },
      "source": [
        "Print the shape of the created matrices.  \n",
        "For the matrix for clinical notes, the size of 1st dimension is the number of notes, and the size of the 2nd dimension is the number of intersected words.  \n",
        "For the matrix for Wiki documents, the size of 1st dimension is the number of Wiki documents, and the size of the 2nd dimension is the number of intersected words.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alNBP-KwZ3L7",
        "outputId": "7a8c29e1-cc81-4b1e-8e5d-b9ab6b68f58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the matrix for clinical notes (notevec) = (52722, 12173)\n",
            "The shape of the matrix for wiki docs (wikivec) = (325, 12173)\n"
          ]
        }
      ],
      "source": [
        "print(f\"The shape of the matrix for clinical notes (notevec) = {binaryn.shape}\")\n",
        "print(f\"The shape of the matrix for wiki docs (wikivec) = {binaryk.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4Bc7NWJ6TuA",
        "outputId": "440d3cad-4106-4508-d671-b97c98416f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 04m 32s\n"
          ]
        }
      ],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA3PKA3o-zIW"
      },
      "source": [
        "# Data Pre-processing 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmglWv6s9Jhc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMqsHWScDco"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dMkHxzFeEGD"
      },
      "source": [
        "Create 2 dictionaries for the ICD-9 codes for Wiki documents.  \n",
        "For the 1st dictionary, the key is a ICD-9 code which exists in Wiki documents, and the value is 1.\n",
        "For the 2nd dictionary, the key is a ICD-9 code which exists in Wiki documents, and the value is the list which contain the sequence number of Wiki documents for this ICD-9 code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNU5O0HNcLOB"
      },
      "outputs": [],
      "source": [
        "wikivoc={}\n",
        "codewiki=defaultdict(list)\n",
        "\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline()\n",
        "count=0\n",
        "while line:\n",
        "  if line[0:4]=='XXXd': # read the start of a wiki doc\n",
        "    line=line.strip('\\n')\n",
        "    codes=line.split()\n",
        "    for code in codes:\n",
        "      if code[0:2]=='d_': # if it is a icd code\n",
        "        codewiki[code].append(count) # save the index of wiki doc to list for code\n",
        "        wikivoc[code]=1 # set value of code to 1\n",
        "    count=count+1\n",
        "  line=file2.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI6vsnX0gObv"
      },
      "source": [
        "Print the 2 dictionaries.  \n",
        "Print the number of ICD-9 codes in Wiki documents (**not all of them can be found in clinical notes**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TCFIlvqcQ4B",
        "outputId": "b7f864c0-38d2-4627-891d-f1b1d9f5a995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_174': 1, 'd_175': 1, 'd_130': 1, 'd_540': 1, 'd_541': 1, 'd_542': 1, 'd_200': 1, 'd_357': 1, 'd_614': 1, 'd_615': 1, 'd_616': 1, 'd_191': 1, 'd_027': 1, 'd_323': 1, 'd_060': 1, 'd_136': 1, 'd_330': 1, 'd_331': 1, 'd_272': 1, 'd_715': 1, 'd_458': 1, 'd_076': 1, 'd_313': 1, 'd_147': 1, 'd_241': 1, 'd_314': 1, 'd_410': 1, 'd_740': 1, 'd_155': 1, 'd_695': 1, 'd_420': 1, 'd_345': 1, 'd_421': 1, 'd_268': 1, 'd_651': 1, 'd_551': 1, 'd_552': 1, 'd_553': 1, 'd_365': 1, 'd_084': 1, 'd_346': 1, 'd_203': 1, 'd_512': 1, 'd_860': 1, 'd_574': 1, 'd_586': 1, 'd_214': 1, 'd_135': 1, 'd_209': 1, 'd_511': 1, 'd_287': 1, 'd_056': 1, 'd_033': 1, 'd_101': 1, 'd_463': 1, 'd_578': 1, 'd_708': 1, 'd_359': 1, 'd_072': 1, 'd_207': 1, 'd_208': 1, 'd_811': 1, 'd_286': 1, 'd_087': 1, 'd_176': 1, 'd_094': 1, 'd_480': 1, 'd_347': 1, 'd_690': 1, 'd_694': 1, 'd_702': 1, 'd_707': 1, 'd_341': 1, 'd_193': 1, 'd_332': 1, 'd_134': 1, 'd_610': 1, 'd_260': 1, 'd_361': 1, 'd_579': 1, 'd_401': 1, 'd_309': 1, 'd_693': 1, 'd_035': 1, 'd_601': 1, 'd_274': 1, 'd_278': 1, 'd_749': 1, 'd_073': 1, 'd_387': 1, 'd_150': 1, 'd_986': 1, 'd_565': 1, 'd_720': 1, 'd_462': 1, 'd_472': 1, 'd_071': 1, 'd_050': 1, 'd_854': 1, 'd_487': 1, 'd_850': 1, 'd_502': 1, 'd_503': 1, 'd_505': 1, 'd_840': 1, 'd_685': 1, 'd_115': 1, 'd_660': 1, 'd_851': 1, 'd_325': 1, 'd_451': 1, 'd_369': 1, 'd_524': 1, 'd_368': 1, 'd_466': 1, 'd_025': 1, 'd_024': 1, 'd_018': 1, 'd_020': 1, 'd_045': 1, 'd_138': 1, 'd_291': 1, 'd_292': 1, 'd_294': 1, 'd_297': 1, 'd_302': 1, 'd_344': 1, 'd_338': 1, 'd_698': 1, 'd_952': 1, 'd_090': 1, 'd_366': 1, 'd_185': 1, 'd_460': 1, 'd_600': 1, 'd_131': 1, 'd_034': 1, 'd_684': 1, 'd_004': 1, 'd_721': 1, 'd_038': 1, 'd_312': 1, 'd_315': 1, 'd_110': 1, 'd_691': 1, 'd_023': 1, 'd_501': 1, 'd_157': 1, 'd_411': 1, 'd_414': 1, 'd_725': 1, 'd_405': 1, 'd_500': 1, 'd_402': 1, 'd_481': 1, 'd_452': 1, 'd_081': 1, 'd_102': 1, 'd_984': 1, 'd_583': 1, 'd_186': 1, 'd_052': 1, 'd_370': 1, 'd_026': 1, 'd_111': 1, 'd_156': 1, 'd_998': 1, 'd_488': 1, 'd_053': 1, 'd_063': 1, 'd_206': 1, 'd_305': 1, 'd_126': 1, 'd_767': 1, 'd_245': 1, 'd_042': 1, 'd_043': 1, 'd_044': 1, 'd_342': 1, 'd_734': 1, 'd_455': 1, 'd_606': 1, 'd_628': 1, 'd_630': 1, 'd_535': 1, 'd_683': 1, 'd_692': 1, 'd_125': 1, 'd_243': 1, 'd_494': 1, 'd_296': 1, 'd_051': 1, 'd_483': 1, 'd_486': 1, 'd_180': 1, 'd_392': 1, 'd_422': 1, 'd_681': 1, 'd_682': 1, 'd_262': 1, 'd_263': 1, 'd_634': 1, 'd_567': 1, 'd_057': 1, 'd_711': 1, 'd_716': 1, 'd_427': 1, 'd_039': 1, 'd_054': 1, 'd_571': 1, 'd_301': 1, 'd_281': 1, 'd_771': 1, 'd_391': 1, 'd_398': 1, 'd_477': 1, 'd_514': 1, 'd_140': 1, 'd_141': 1, 'd_142': 1, 'd_143': 1, 'd_144': 1, 'd_145': 1, 'd_146': 1, 'd_183': 1, 'd_572': 1, 'd_082': 1, 'd_083': 1, 'd_580': 1, 'd_490': 1, 'd_201': 1, 'd_645': 1, 'd_598': 1, 'd_515': 1, 'd_300': 1, 'd_327': 1, 'd_340': 1, 'd_389': 1, 'd_311': 1, 'd_001': 1, 'd_112': 1, 'd_510': 1, 'd_099': 1, 'd_153': 1, 'd_619': 1, 'd_722': 1, 'd_741': 1, 'd_055': 1, 'd_032': 1, 'd_122': 1, 'd_098': 1, 'd_128': 1, 'd_333': 1, 'd_657': 1, 'd_383': 1, 'd_386': 1, 'd_228': 1, 'd_021': 1, 'd_680': 1, 'd_430': 1, 'd_584': 1, 'd_585': 1, 'd_475': 1, 'd_285': 1, 'd_124': 1, 'd_758': 1, 'd_491': 1, 'd_492': 1, 'd_496': 1, 'd_432': 1, 'd_465': 1, 'd_482': 1, 'd_700': 1, 'd_436': 1, 'd_437': 1, 'd_438': 1, 'd_428': 1, 'd_820': 1, 'd_036': 1, 'd_604': 1, 'd_280': 1, 'd_730': 1, 'd_378': 1, 'd_282': 1, 'd_283': 1, 'd_317': 1, 'd_256': 1, 'd_120': 1, 'd_188': 1, 'd_075': 1, 'd_696': 1, 'd_516': 1, 'd_199': 1, 'd_454': 1, 'd_456': 1, 'd_595': 1, 'd_002': 1, 'd_435': 1, 'd_597': 1, 'd_731': 1, 'd_086': 1, 'd_426': 1, 'd_100': 1, 'd_133': 1, 'd_726': 1, 'd_303': 1, 'd_632': 1, 'd_635': 1, 'd_636': 1, 'd_637': 1, 'd_638': 1, 'd_539': 1, 'd_653': 1, 'd_425': 1, 'd_080': 1, 'd_493': 1, 'd_030': 1, 'd_037': 1, 'd_250': 1, 'd_010': 1, 'd_011': 1, 'd_137': 1, 'd_461': 1, 'd_473': 1, 'd_091': 1, 'd_092': 1, 'd_093': 1, 'd_095': 1, 'd_096': 1, 'd_097': 1, 'd_267': 1, 'd_666': 1, 'd_161': 1, 'd_671': 1, 'd_672': 1, 'd_674': 1, 'd_677': 1, 'd_304': 1, 'd_218': 1, 'd_714': 1, 'd_603': 1, 'd_769': 1, 'd_798': 1, 'd_531': 1, 'd_532': 1, 'd_533': 1, 'd_555': 1, 'd_172': 1, 'd_173': 1, 'd_413': 1, 'd_151': 1, 'd_581': 1, 'd_269': 1, 'd_444': 1, 'd_320': 1, 'd_321': 1, 'd_322': 1, 'd_006': 1, 'd_550': 1, 'd_464': 1, 'd_476': 1, 'd_732': 1, 'd_190': 1, 'd_022': 1, 'd_061': 1, 'd_591': 1, 'd_261': 1, 'd_354': 1, 'd_355': 1, 'd_070': 1, 'd_085': 1, 'd_605': 1, 'd_284': 1, 'd_768': 1, 'd_326': 1, 'd_382': 1, 'd_523': 1, 'd_403': 1, 'd_114': 1, 'd_453': 1, 'd_808': 1, 'd_712': 1, 'd_431': 1, 'd_198': 1, 'd_259': 1, 'd_713': 1, 'd_103': 1, 'd_308': 1, 'd_470': 1}\n",
            "defaultdict(<class 'list'>, {'d_174': [0], 'd_175': [0], 'd_130': [1], 'd_540': [2], 'd_541': [2], 'd_542': [2], 'd_200': [3], 'd_357': [4], 'd_614': [4], 'd_615': [4], 'd_616': [4], 'd_191': [5], 'd_027': [6], 'd_323': [7], 'd_060': [8], 'd_136': [9], 'd_330': [10], 'd_331': [10], 'd_272': [11], 'd_715': [12], 'd_458': [13], 'd_076': [14], 'd_313': [15], 'd_147': [16], 'd_241': [17], 'd_314': [18], 'd_410': [19], 'd_740': [20], 'd_155': [21], 'd_695': [22], 'd_420': [23], 'd_345': [24], 'd_421': [25], 'd_268': [26], 'd_651': [27], 'd_551': [28], 'd_552': [28], 'd_553': [28], 'd_365': [29], 'd_084': [30], 'd_346': [31], 'd_203': [32], 'd_512': [33], 'd_860': [33], 'd_574': [34], 'd_586': [35], 'd_214': [36], 'd_135': [37], 'd_209': [38], 'd_511': [39], 'd_287': [40], 'd_056': [41], 'd_033': [42], 'd_101': [43], 'd_463': [43], 'd_578': [44], 'd_708': [45], 'd_359': [46], 'd_072': [47, 214], 'd_207': [48], 'd_208': [48], 'd_811': [49], 'd_286': [50], 'd_087': [51], 'd_176': [52], 'd_094': [53], 'd_480': [54], 'd_347': [55], 'd_690': [56], 'd_694': [56], 'd_702': [56], 'd_707': [56], 'd_341': [57], 'd_193': [58], 'd_332': [59], 'd_134': [60], 'd_610': [61], 'd_260': [62], 'd_361': [63], 'd_579': [64], 'd_401': [65], 'd_309': [66], 'd_693': [67], 'd_035': [68], 'd_601': [69], 'd_274': [70], 'd_278': [71], 'd_749': [72], 'd_073': [73], 'd_387': [74], 'd_150': [75], 'd_986': [76], 'd_565': [77], 'd_720': [78], 'd_462': [79], 'd_472': [79], 'd_071': [80], 'd_050': [81], 'd_854': [82], 'd_487': [83], 'd_850': [84], 'd_502': [85], 'd_503': [85], 'd_505': [85], 'd_840': [86], 'd_685': [87], 'd_115': [88], 'd_660': [89], 'd_851': [90], 'd_325': [91], 'd_451': [91], 'd_369': [92], 'd_524': [93], 'd_368': [94], 'd_466': [95], 'd_025': [96], 'd_024': [97], 'd_018': [98], 'd_020': [99], 'd_045': [100], 'd_138': [100], 'd_291': [101], 'd_292': [101], 'd_294': [101], 'd_297': [102], 'd_302': [103], 'd_344': [104], 'd_338': [105], 'd_698': [106, 125], 'd_952': [107], 'd_090': [108], 'd_366': [109], 'd_185': [110], 'd_460': [111], 'd_600': [112], 'd_131': [113], 'd_034': [114], 'd_684': [115], 'd_004': [116], 'd_721': [117], 'd_038': [118], 'd_312': [119], 'd_315': [120], 'd_110': [121], 'd_691': [122], 'd_023': [123], 'd_501': [124], 'd_157': [126], 'd_411': [127], 'd_414': [127], 'd_725': [128], 'd_405': [129], 'd_500': [130], 'd_402': [131], 'd_481': [132], 'd_452': [133], 'd_081': [134], 'd_102': [135], 'd_984': [136], 'd_583': [137], 'd_186': [138], 'd_052': [139], 'd_370': [140], 'd_026': [141], 'd_111': [142], 'd_156': [143], 'd_998': [144], 'd_488': [145], 'd_053': [146], 'd_063': [147], 'd_206': [148], 'd_305': [149, 250], 'd_126': [150], 'd_767': [151], 'd_245': [152], 'd_042': [153], 'd_043': [153], 'd_044': [153], 'd_342': [154], 'd_734': [155], 'd_455': [156], 'd_606': [157], 'd_628': [157], 'd_630': [158], 'd_535': [159], 'd_683': [160], 'd_692': [161], 'd_125': [162], 'd_243': [163], 'd_494': [164], 'd_296': [165], 'd_051': [166], 'd_483': [167], 'd_486': [167], 'd_180': [168], 'd_392': [169], 'd_422': [170], 'd_681': [171], 'd_682': [171], 'd_262': [172], 'd_263': [172], 'd_634': [173], 'd_567': [174], 'd_057': [175], 'd_711': [176], 'd_716': [176], 'd_427': [177], 'd_039': [178], 'd_054': [179], 'd_571': [180], 'd_301': [181], 'd_281': [182], 'd_771': [183], 'd_391': [184], 'd_398': [184], 'd_477': [185], 'd_514': [186], 'd_140': [187], 'd_141': [187], 'd_142': [187], 'd_143': [187], 'd_144': [187], 'd_145': [187], 'd_146': [187], 'd_183': [188], 'd_572': [189], 'd_082': [190], 'd_083': [190], 'd_580': [191], 'd_490': [192], 'd_201': [193], 'd_645': [194], 'd_598': [195], 'd_515': [196], 'd_300': [197], 'd_327': [198], 'd_340': [199], 'd_389': [200], 'd_311': [201], 'd_001': [202], 'd_112': [203], 'd_510': [204], 'd_099': [205], 'd_153': [206], 'd_619': [207], 'd_722': [208], 'd_741': [209], 'd_055': [210], 'd_032': [211], 'd_122': [212], 'd_098': [213], 'd_128': [215], 'd_333': [216], 'd_657': [217], 'd_383': [218], 'd_386': [219, 221], 'd_228': [220], 'd_021': [222], 'd_680': [223], 'd_430': [224], 'd_584': [225], 'd_585': [226], 'd_475': [227], 'd_285': [228], 'd_124': [229], 'd_758': [230], 'd_491': [231], 'd_492': [231], 'd_496': [231], 'd_432': [232], 'd_465': [233], 'd_482': [234], 'd_700': [235], 'd_436': [236], 'd_437': [236], 'd_438': [236], 'd_428': [237], 'd_820': [238], 'd_036': [239], 'd_604': [240], 'd_280': [241], 'd_730': [242], 'd_378': [243], 'd_282': [244], 'd_283': [244], 'd_317': [245], 'd_256': [246], 'd_120': [247], 'd_188': [248], 'd_075': [249], 'd_696': [251], 'd_516': [252], 'd_199': [253], 'd_454': [254], 'd_456': [254], 'd_595': [255], 'd_002': [256], 'd_435': [257], 'd_597': [258], 'd_731': [259], 'd_086': [260], 'd_426': [261], 'd_100': [262], 'd_133': [263], 'd_726': [264], 'd_303': [265], 'd_632': [266], 'd_635': [266], 'd_636': [266], 'd_637': [266], 'd_638': [266], 'd_539': [267], 'd_653': [268], 'd_425': [269], 'd_080': [270], 'd_493': [271], 'd_030': [272], 'd_037': [273], 'd_250': [274], 'd_010': [275], 'd_011': [275], 'd_137': [275], 'd_461': [276], 'd_473': [276], 'd_091': [277], 'd_092': [277], 'd_093': [277], 'd_095': [277], 'd_096': [277], 'd_097': [277], 'd_267': [278], 'd_666': [279], 'd_161': [280], 'd_671': [281], 'd_672': [281], 'd_674': [281], 'd_677': [281], 'd_304': [282], 'd_218': [283], 'd_714': [284], 'd_603': [285], 'd_769': [286], 'd_798': [287], 'd_531': [288], 'd_532': [288], 'd_533': [288], 'd_555': [289], 'd_172': [290], 'd_173': [290], 'd_413': [291], 'd_151': [292], 'd_581': [293], 'd_269': [294], 'd_444': [295], 'd_320': [296], 'd_321': [296], 'd_322': [296], 'd_006': [297], 'd_550': [298], 'd_464': [299], 'd_476': [299], 'd_732': [300], 'd_190': [301], 'd_022': [302], 'd_061': [303], 'd_591': [304], 'd_261': [305], 'd_354': [306], 'd_355': [306], 'd_070': [307], 'd_085': [308], 'd_605': [309], 'd_284': [310], 'd_768': [311], 'd_326': [312], 'd_382': [312], 'd_523': [313], 'd_403': [314], 'd_114': [315], 'd_453': [316], 'd_808': [317], 'd_712': [318], 'd_431': [319], 'd_198': [320], 'd_259': [321], 'd_713': [321], 'd_103': [322], 'd_308': [323], 'd_470': [324]})\n",
            "Total number of codes = 389\n"
          ]
        }
      ],
      "source": [
        "print(wikivoc)\n",
        "print(codewiki)\n",
        "print(f\"Total number of codes = {len(wikivoc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXmyJz6chflJ"
      },
      "source": [
        "Each of the following 4 ICD-9 codes appears in 2 Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZR9TGFAhDM2",
        "outputId": "35262f9e-4e60-478b-d2ae-ceb2a91f655e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47, 214]\n",
            "[106, 125]\n",
            "[149, 250]\n",
            "[219, 221]\n"
          ]
        }
      ],
      "source": [
        "print(codewiki['d_072'])\n",
        "print(codewiki['d_698'])\n",
        "print(codewiki['d_305'])\n",
        "print(codewiki['d_386'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXlRQZ4khr9V"
      },
      "source": [
        "For training purpose, each Wiki document can have more than one ICD-9 code, but each ICD-9 code can appear in only one Wiki document.  \n",
        "Correct the 4 ICD-9 codes above, and for each of these 4 codes, down-select one Wiki document.  \n",
        "**wikivoc.npy**: A matrix of ICD-9 codes which appear in Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFSJFFcriWRC"
      },
      "outputs": [],
      "source": [
        "codewiki['d_072']=[214]\n",
        "codewiki['d_698']=[125]\n",
        "codewiki['d_305']=[250]\n",
        "codewiki['d_386']=[219]\n",
        "\n",
        "np.save('wikivoc',wikivoc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJq3pVJOZHO"
      },
      "source": [
        "Prepare feature and label for the deep learning model.  \n",
        "Feature: A list of string. Each string is one clinical note.  \n",
        "Label: A list of string. Each string is the ICD-9 codes for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3pmjJZOTEX"
      },
      "outputs": [],
      "source": [
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "feature=[]\n",
        "label=[]\n",
        "\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  \n",
        "  if line[0]=='codes:':\n",
        "    temp=line[1:] # read the codes of that node\n",
        "    label.append(temp) # add the code to list for label\n",
        "    line=filec.readline()\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    if line[0]=='notes:':\n",
        "      tempf=[]\n",
        "      line=filec.readline()\n",
        "      \n",
        "      while line!='end!\\n': # read the notes until end\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        tempf=tempf+line\n",
        "        line=filec.readline()\n",
        "      feature.append(tempf) # add list of words to list of feature\n",
        "  line=filec.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QN-HcplPzUM"
      },
      "source": [
        "Print a sample feature.  \n",
        "Print a sample label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UI622nTP3yu",
        "outputId": "4f5f21d0-8cd6-45b2-ea0d-bd6e7efe26ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['admission', 'date', 'discharge', 'date', 'date', 'birth', 'sex', 'm', 'service', 'medicine', 'allergies', 'patient', 'recorded', 'known', 'allergies', 'drugs', 'attendinglast', 'name', 'un', 'chief', 'complaint', 'fever', 'cough', 'weakness', 'major', 'surgical', 'invasive', 'procedure', 'rigid', 'bronchoscopy', 'endobronchial', 'debulking', 'squamous', 'cell', 'lung', 'mass', 'endotracheal', 'intubation', 'history', 'present', 'illness', 'mr', 'known', 'lastname', 'yo', 'man', 'w', 'recentlydiagnosed', 'lul', 'poorlydifferentiated', 'squamous', 'cell', 'lung', 'cancer', 'admitted', 'hypotension', 'fever', 'secondary', 'postobstructive', 'pneumonia', 'patient', 'back', 'pain', 'weight', 'loss', 'productive', 'cough', 'mild', 'hemoptysis', 'beginning', 'month', 'month', 'time', 'revealed', 'lul', 'opacity', 'fu', 'ct', 'thorax', 'revealed', 'lul', 'mass', 'x', 'cm', 'concerning', 'malignancy', 'patient', 'underwent', 'flexible', 'bronchoscopy', 'endobronchial', 'biopsies', 'linear', 'endobronchial', 'ultrasound', 'lymph', 'node', 'biopsies', 'procedures', 'performed', 'dr', 'last', 'name', 'stitle', 'endobronchial', 'biopsies', 'revealed', 'poorlydifferentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'presented', 'pcps', 'dr', 'first', 'name', 'stitle', 'angels', 'office', 'worsening', 'cough', 'fever', 'weakness', 'vitals', 'pcps', 'office', 'sbp', 'tachycardic', 'breathing', 'ra', 'diminished', 'l', 'upper', 'lung', 'breath', 'sounds', 'per', 'pcp', 'name10', 'nameis', 'revealed', 'new', 'infiltrate', 'peripheral', 'mass', 'lesions', 'likely', 'consistent', 'pna', 'referred', 'ed', 'pcp', 'time', 'ed', 'vitals', '2l', 'monthyear', 'done', 'ed', 'also', 'showed', 'likely', 'postobstructive', 'pna', 'patient', 'given', '3l', 'ns', 'gram', 'iv', 'vancomycin', 'mg', 'po', 'levofloxacin', 'sent', 'micu', 'started', 'ampicillinsulbactam', 'unasyn', 'continued', 'levofloxacin', 'micu', 'hypotension', 'stablized', 'proven', 'fluid', 'responsive', 'breathing', 'comfortably', 'l', 'oxygen', 'lul', 'mass', 'thought', 'obstructing', 'airway', 'endobronchial', 'debulking', 'performed', 'afternoon', 'prior', 'transfer', 'micu', 'floor', 'suggested', 'ip', 'consult', 'patients', 'procedure', 'cb', 'hypoventilation', 'paco2', 'noted', 'one', 'point', 'mechanical', 'pacu', 'lowered', 'paco2', 'mid50s', 'patient', 'extubated', 'transferred', 'floor', 'o2', 'saturation', 'past', 'medical', 'history', 'hypercholesterolemia', 'osteoarthritis', 'hip', 'social', 'history', 'lives', 'alone', 'always', 'lived', 'ma', 'works', 'post', 'office', 'letter', 'carrier', 'prior', 'employment', 'government', 'contractor', 'smoked', 'ppd', 'x', 'yrs', 'quit', 'drinks', 'beers', 'family', 'history', 'prostate', 'cancer', 'father', 'deceased', 'hypertension', 'mother', 'alive', 'physical', 'exam', 'admission', 'vs', 't', 'bp', 'p', 'r', 'sat', '97ra', 'gen', 'nad', 'aaox3', 'looks', 'comfortable', 'heent', 'mm', 'slightly', 'dry', 'lesions', 'exudate', 'noted', 'neck', 'cervical', 'supraclavicular', 'lad', 'cv', 'rrr', 's1s2', 'mrg', 'audible', 'lungs', 'focal', 'area', 'wheezing', 'rales', 'left', 'upper', 'lobe', 'scattered', 'wheezes', 'left', 'lung', 'whole', 'right', 'lung', 'ctab', 'wrr', 'dullness', 'percussion', 'left', 'upper', 'lung', 'noted', 'egophony', 'abd', 'bs', 'normoactive', 'soft', 'ntnd', 'hepatosplenomegaly', 'noted', 'ext', 'wwp', 'pitting', 'edema', 'present', 'legs', 'bilaterally', 'halfway', 'shin', 'nontender', 'palpation', 'neuro', 'aaox3', 'cns', 'iixii', 'intact', 'sensation', 'intact', 'lt', 'discharge', 'vs', 'hr89', 'bp14080', 'rr18', 'general', 'comfortable', 'appearing', 'nad', 'heent', 'op', 'clear', 'cv', 'rrr', 'normal', 's1', 's1', 'mrg', 'resp', 'unlabored', 'breathing', 'minimal', 'diffuse', 'endexpiratory', 'wheezing', 'throughout', 'worse', 'lul', 'decreased', 'bs', 'left', 'lung', 'abdominal', 'sntnd', 'extremities', 'pulses', 'pertinent', 'results', 'admission', 'labs', '0545pm', 'blood', 'wbc228', 'rbc427', 'hgb120', 'hct363', 'mcv85', 'mch281', 'mchc330', 'rdw136', 'plt', 'ct309', '0545pm', 'blood', 'neuts91', 'bands2', 'lymphs2', 'monos5', 'eos0', 'baso0', 'atyps0', 'metas0', 'myelos0', '0545pm', 'blood', 'pt158', 'ptt314', 'inrpt14', '0545pm', 'blood', 'glucose114', 'urean20', 'creat12', 'na138', 'k42', 'cl99', 'hco326', 'angap17', '0545pm', 'blood', 'alt21', 'ast19', 'alkphos118', 'totbili31', '0928pm', 'blood', 'calcium77', 'phos38', 'mg18', 'ct', 'chest', 'contrast', 'enlarging', 'left', 'upper', 'lobe', 'mass', 'left', 'upper', 'bronchial', 'stenosis', 'level', 'stenosis', 'pulmonary', 'artery', 'less', '11mm', 'posterior', 'bronchus', 'encasement', 'left', 'upper', 'lobe', 'pulmonary', 'artery', 'postobstructive', 'pneumonia', 'new', 'bilateral', 'small', 'pleural', 'effusions', 'new', 'left', 'basal', 'pneumonia', 'probably', 'aspiration', 'left', 'upper', 'lobe', 'contents', 'interval', 'results', 'mri', 'head', 'without', 'contrast', 'evidence', 'intracranial', 'metastatic', 'disease', 'ct', 'chest', 'without', 'contrast', 'progression', 'left', 'upper', 'lobe', 'atelectasis', 'consolidation', 'status', 'post', 'recent', 'debridement', 'gas', 'area', 'previously', 'seen', 'lesion', 'reopening', 'left', 'upper', 'lobe', 'bronchus', 'stenosis', 'left', 'upper', 'lobe', 'bronchus', 'however', 'mild', 'increase', 'aeration', 'apicoposterior', 'segment', 'peripheral', 'dilated', 'bronchioles', 'progression', 'groundglass', 'consolidation', 'left', 'lower', 'lobe', 'possibly', 'related', 'aspiration', 'part', 'compressive', 'atelectasis', 'due', 'enlarging', 'left', 'pleural', 'effusion', 'small', 'right', 'pleural', 'effusion', 'adjacent', 'atelectasis', 'unchanged', 'lymphadenopathy', 'pleural', 'fluid', 'analysis', 'pmns', 'organisms', 'fluid', 'culture', 'without', 'growth', 'anaerobic', 'culture', 'without', 'growth', 'fdg', 'tumor', 'imaging', 'fdg', 'avid', 'left', 'upper', 'lobe', 'consolidation', 'focally', 'increased', 'fdg', 'avidity', 'surrounding', 'narrowed', 'left', 'upper', 'lobe', 'bronchus', 'compatible', 'known', 'squamous', 'cell', 'carcinoma', 'postobstructive', 'collapse', 'consolidation', 'left', 'upper', 'lobe', 'fdg', 'avid', 'consolidation', 'left', 'lower', 'lobe', 'lingula', 'likely', 'pneumonia', 'aspiration', 'moderate', 'left', 'small', 'right', 'pleural', 'effusions', 'fdg', 'avid', 'mediastinal', 'lymphadenopathy', 'trace', 'free', 'fluid', 'pelvis', 'chest', 'ct', 'contrast', 'interval', 'improvement', 'left', 'upper', 'lobe', 'postobstructive', 'pneumonia', 'improved', 'aeration', 'anterior', 'aspect', 'left', 'upper', 'lobe', 'foci', 'gas', 'within', 'consolidated', 'parenchyma', 'similar', 'appearance', 'potential', 'communication', 'left', 'upper', 'lobe', 'segmental', 'bronchus', 'collections', 'air', 'possible', 'known', 'mass', 'lesion', 'well', 'evaluated', 'due', 'surrounding', 'consolidation', 'slight', 'interval', 'increase', 'left', 'pleural', 'effusion', 'resolution', 'right', 'pleural', 'effusion', 'extensive', 'lymphadenopathy', 'unchanged', 'unclear', 'component', 'reactive', 'versus', 'involved', 'malignancy', 'ultrasound', 'doppler', 'lower', 'extremities', 'nonocclusive', 'thrombosis', 'bilateral', 'popliteal', 'right', 'peroneal', 'left', 'posterior', 'tibialis', 'veins', 'blood', 'cultures', 'repeated', 'times', 'admission', 'always', 'without', 'growth', 'chest', 'xrays', 'repeated', 'times', 'throughout', 'admission', 'demonstrated', 'left', 'upper', 'lobe', 'postobstructive', 'pneumonia', 'little', 'improvement', 'images', 'later', 'stay', 'revealed', 'almost', 'complete', 'white', 'left', 'lung', 'collapse', 'left', 'lower', 'lobe', 'detected', 'late', 'hospital', 'course', 'mediastinal', 'lymph', 'node', 'biopsy', 'lymph', 'node', '4r', 'biopsy', 'ad', 'malignancy', 'identified', 'lymph', 'node', '4l', 'biopsy', 'ef', 'malignancy', 'identified', 'lymph', 'node', 'level', 'biopsy', 'gh', 'malignancy', 'identified', 'lymph', 'node', '2r', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', '2l', 'biopsy', 'j', 'malignancy', 'identified', 'discharge', 'results', '0720am', 'blood', 'wbc91', 'rbc366', 'hgb99', 'hct311', 'mcv85', 'mch270', 'mchc317', 'rdw157', 'plt', 'ct470', '0720am', 'blood', 'glucose86', 'urean5', 'creat07', 'na140', 'k38', 'cl101', 'hco332', 'angap11', '0720am', 'blood', 'albumin24', 'calcium80', 'phos36', 'mg23', 'brief', 'hospital', 'course', 'yo', 'man', 'left', 'upper', 'lobe', 'poorlydifferentiated', 'squamous', 'cell', 'carcinoma', 'presented', 'fevers', 'leukocytosis', 'hypotension', 'setting', 'lul', 'postobstructive', 'pna', 'hypotension', 'patient', 'presented', 'systolic', 'blood', 'pressures', '80s', 'patient', 'sent', 'micu', 'pressure', 'came', '100s', 'following', '4l', 'ns', 'mental', 'status', 'never', 'compromised', 'lactate', 'patient', 'subsequently', 'transferred', 'general', 'medicine', 'floors', 'patient', 'continued', 'intermittent', 'hypotensive', 'episodes', 'stay', 'associated', 'severe', 'night', 'sweats', 'fevers', 'infectious', 'work', 'negative', 'multiple', 'occassions', 'fevers', 'attributed', 'tumor', 'started', 'scheduled', 'acetaminophen', 'continuous', 'ivf', 'patient', 'subsequent', 'episodes', 'hypotension', 'postobstructive', 'pneumonia', 'setting', 'known', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'radiographic', 'evidence', 'high', 'concern', 'postobstructive', 'pneumonia', 'patient', 'started', 'vancomycin', 'levofloxacin', 'ed', 'switched', 'ampicillinsulbactam', 'micu', 'interventional', 'pulmonology', 'performed', 'rigid', 'bronchoscopy', 'bulk', 'resection', 'antibiotics', 'switched', 'levofloxacin', 'metronidazole', 'per', 'ids', 'recommendations', 'patient', 'subsequently', 'began', 'radiation', 'therapy', 'effort', 'shrink', 'tumor', 'hopes', 'definitive', 'therapy', 'pneumonia', 'repeat', 'chest', 'xrays', 'throughout', 'admission', 'demonstrated', 'little', 'improvement', 'postobstructive', 'pnemonia', 'subsequent', 'collapse', 'left', 'lower', 'lobe', 'progression', 'left', 'upper', 'lobe', 'patient', 'transitioned', 'levofloxacin', 'flagyl', 'antibiotics', 'discontinued', 'discharge', 'patient', 'completed', 'days', 'antibiotics', 'little', 'concern', 'infectious', 'etiology', 'fevers', 'per', 'left', 'upper', 'lobe', 'poorly', 'differentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'diagnosed', 'several', 'weeks', 'prior', 'admission', 'time', 'lymph', 'nodes', 'clear', 'brain', 'mri', 'showed', 'early', 'hospitalization', 'revealed', 'evidence', 'brain', 'metastases', 'pet', 'scan', 'revealed', 'mediastinal', 'lymph', 'nodes', 'suspicious', 'metastases', 'patient', 'subsequently', 'began', 'xrt', 'treatments', 'shrink', 'tumor', 'enough', 'treat', 'postobstructive', 'pneumonia', 'thoracic', 'surgery', 'consulted', 'performed', 'mediastinoscopy', 'sampled', 'lymph', 'nodes', 'negative', 'malignant', 'cells', 'patient', 'discharged', 'instructions', 'follow', 'oncologist', 'well', 'chest', 'disease', 'center', 'pleural', 'effusion', 'pleural', 'fluid', 'negative', 'malignant', 'cells', 'thoracentesis', 'cultures', 'showing', 'empyema', 'exudative', 'parapneumonic', 'effusion', 'suspected', 'evidence', 'empyema', 'patient', 'treated', 'pneumonia', 'per', 'partial', 'occlusive', 'thromboses', 'patient', 'mild', 'rightsided', 'lower', 'extremity', 'edema', 'midway', 'admission', 'ultrasound', 'study', 'dopplers', 'revealed', 'nonocclusive', 'thromboses', 'bilateral', 'popliteal', 'right', 'peroneal', 'left', 'posterior', 'tibialis', 'veins', 'patient', 'initially', 'treated', 'heparin', 'drip', 'due', 'concerns', 'renal', 'function', 'patient', 'eventually', 'treated', 'enoxaparin', 'mg', 'subcutaneous', 'injection', 'twice', 'daily', 'discharged', 'home', 'instructions', 'continue', 'injections', 'thrombocytosis', 'likely', 'acute', 'phase', 'reactant', 'setting', 'ongoing', 'infection', 'inflammation', 'patients', 'platelet', 'counts', 'remained', 'elevated', 'throughout', 'admission', 'anemia', 'hematocrit', 'stable', 'throughout', 'admission', 'iron', 'studies', 'consistent', 'anemia', 'chronic', 'inflammation', 'medications', 'admission', 'none', 'discharge', 'medications', 'combivent', 'mcgactuation', 'aerosol', 'sig', 'inhalation', 'every', 'hours', 'needed', 'shortness', 'breath', 'wheezing', 'disp1', 'refills0', 'codeineguaifenesin', 'mg5', 'ml', 'syrup', 'sig', 'mls', 'po', 'q6h', 'every', 'hours', 'needed', 'cough', 'weeks', 'disp1', 'ml', 'bottle', 'refills1', 'fluconazole', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q24h', 'every', 'hours', 'days', 'disp8', 'tablets', 'refills0', 'acetaminophen', 'mg', 'tablet', 'sig', 'two', 'tablet', 'po', 'q6h', 'every', 'hours', 'disp120', 'tablets', 'refills0', 'lovenox', 'mg08', 'ml', 'syringe', 'sig', 'seventy', 'mg', 'subcutaneous', 'twice', 'day', 'disp10', 'syringes', 'refills0', 'lorazepam', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q4h', 'every', 'hours', 'needed', 'anxiety', 'disp20', 'tablets', 'refills0', 'ranitidine', 'hcl', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'daily', 'daily', 'disp30', 'tablets', 'refills2', 'morphine', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q4h', 'every', 'hours', 'needed', 'pain', 'disp20', 'tablets', 'refills0', 'discharge', 'disposition', 'home', 'service', 'facility', 'care', 'vna', 'greater', 'location', 'un', 'discharge', 'diagnosis', 'primary', 'diagnoses', 'postobstructive', 'pneumonia', 'seconday', 'diagnoses', 'squamous', 'cell', 'lung', 'cancer', 'discharge', 'condition', 'mental', 'status', 'clear', 'coherent', 'level', 'consciousness', 'alert', 'interactive', 'activity', 'status', 'ambulatory', 'independent', 'discharge', 'instructions', 'dear', 'mr', 'known', 'lastname', 'pleasure', 'taking', 'care', 'hospital1', 'admitted', 'hospital', 'pneumonia', 'fever', 'low', 'blood', 'pressure', 'also', 'known', 'hypotension', 'known', 'lung', 'cancer', 'obstructing', 'airways', 'leading', 'known', 'postobstructive', 'pneumonia', 'procedure', 'debulk', 'lung', 'cancer', 'attempt', 'open', 'airways', 'order', 'allow', 'pneumonia', 'resolve', 'received', 'antibiotics', 'will', 'need', 'continue', 'outpatient', 'hospital', 'began', 'workup', 'staging', 'lung', 'cancer', 'brain', 'mri', 'show', 'metastases', 'scheduled', 'petct', 'scan', 'outpatient', 'will', 'need', 'follow', 'directions', 'oral', 'contrast', 'provided', 'prior', 'discharge', 'follow', 'dr', 'first', 'name', 'stitle', 'medical', 'oncology', 'hospital1', 'dr', 'first', 'name', 'stitle', 'thoracic', 'surgery', 'dr', 'last', 'name', 'stitle', 'radiation', 'oncology', 'following', 'changes', 'made', 'medications', 'start', 'using', 'combivent', 'inhaler', 'inhalations', 'every', 'hours', 'needed', 'shortness', 'breath', 'start', 'using', 'codeineguaifenesin', 'cough', 'syrup', 'ml', 'mouth', 'every', 'six', 'hours', 'needed', 'cough', 'start', 'using', 'fluconazole', 'mg', 'mouth', 'day', 'medication', 'sore', 'throat', 'will', 'need', 'take', 'days', 'start', 'taking', 'acetaminophen', 'mg', 'take', 'two', 'tablets', 'mouth', 'every', 'six', 'hours', 'needed', 'fever', 'exceed', 'tablets', 'per', 'day', 'start', 'using', 'lovenox', 'mg', 'subcutaneous', 'injection', 'twice', 'day', 'medication', 'blood', 'clots', 'found', 'legs', 'start', 'using', 'lorazepam', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'anxiety', 'start', 'taking', 'ranitidine', 'mg', 'mouth', 'day', 'medication', 'helps', 'reflux', 'start', 'taking', 'morphine', 'sulfate', 'ir', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'pain', 'followup', 'instructions', 'department', 'chest', 'disease', 'center', 'name', 'last', 'name', 'lf', 'first', 'name7', 'namepattern1', 'initial', 'namepattern1', 'last', 'name', 'namepattern4', 'please', 'call', 'thoracic', 'oncology', 'program', 'schedule', 'upcoming', 'appointment', 'dr', 'first', 'name', 'stitle', 'days', 'hospital', 'discharge', 'please', 'call', 'office', 'number', 'listed', 'make', 'appointment', 'location', 'hospital1', 'address', 'location', 'un', 'hospital', 'ward', 'name', 'location', 'un', 'numeric', 'identifier', 'phone', 'department', 'pulmonary', 'function', 'lab', 'monday', 'pulmonary', 'function', 'lab', 'telephonefax', 'building', 'gz', 'hospital', 'ward', 'name', 'building', 'felbeerghospital', 'ward', 'name', 'complex', 'location', 'un', 'campus', 'east', 'best', 'parking', 'main', 'garage', 'department', 'radiology', 'wednesday', 'pm', 'xmr', 'telephonefax', 'building', 'cc', 'location', 'un', 'hospital', 'campus', 'west', 'best', 'parking', 'street', 'address1', 'garage', 'petct', 'scan', 'department', 'radiology', 'thursday', 'pm', 'location', 'hospital', 'ward', 'name', 'center', 'location', 'un', 'campus', 'east', 'department', 'primary', 'care', 'name', 'dr', 'first', 'name8', 'namepattern2', 'doctor', 'last', 'name', 'monday', 'pm', 'location', 'location', 'un', 'hospital1', 'address', 'country', '3rd', 'fl', 'hospital1', 'numeric', 'identifier', 'phone', 'telephonefax', 'completed']\n",
            "['d_486', 'd_518', 'd_511', 'd_162', 'd_112', 'd_196', 'd_453', 'd_263', 'd_780', 'd_787', 'd_276', 'd_272', 'd_715', 'd_799', 'd_288', 'd_285', 'd_327', 'd_V15', 'd_E87']\n",
            "1606\n",
            "19\n",
            "52722\n",
            "52722\n"
          ]
        }
      ],
      "source": [
        "print(feature[0])\n",
        "print(label[0])\n",
        "print(len(feature[0]))\n",
        "print(len(label[0]))\n",
        "print(len(feature))\n",
        "print(len(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4la9dYtOTPLz"
      },
      "source": [
        "Create the sequence for label (ICD-9 codes). The key is a ICD-9 code, and the value is the sequence of a ICD-9 code in the code vector later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrzF4EYGTksY"
      },
      "outputs": [],
      "source": [
        "prevoc={}\n",
        "for i in label:\n",
        "  for j in i:\n",
        "    if j not in prevoc:\n",
        "      prevoc[j] = len(prevoc) # set up the order of codes (for vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skouqwX8TtHz"
      },
      "source": [
        "Print a sample key-value pairs (refer to label[0]).  \n",
        "Print the number of key-value pairs (codes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8zU8E3uTwvn",
        "outputId": "1571f32d-ae8d-4f34-ef36-cc5d6037e885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "941\n"
          ]
        }
      ],
      "source": [
        "# print(prevoc[\"d_486\"])\n",
        "# print(prevoc[\"d_518\"])\n",
        "# print(prevoc[\"d_511\"])\n",
        "print(len(prevoc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpMWaG1NVP4e"
      },
      "source": [
        "Load notevec.npy and wikivec.npy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P-pR1rSVXQn"
      },
      "outputs": [],
      "source": [
        "notevec=np.load('notevec.npy')\n",
        "wikivec=np.load('wikivec.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvRlVNfjVgon"
      },
      "source": [
        "Create mapping between ICD-9 codes and the index in the code vector by 2 dictionaries.  \n",
        "**This mapping is for all codes found in the combined dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxSZbDm9Vq9N"
      },
      "outputs": [],
      "source": [
        "label_to_ix = {}\n",
        "ix_to_label = {}\n",
        "\n",
        "# create a mapping between code and index\n",
        "for codes in label:\n",
        "  for code in codes:\n",
        "    if code not in label_to_ix:\n",
        "      label_to_ix[code]=len(label_to_ix)\n",
        "      ix_to_label[label_to_ix[code]]=code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz3jD8rBWDYa"
      },
      "source": [
        "Print sample key-value pairs.  \n",
        "Print the number of ICD-9 codes found in combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk3ABC2ZWGfM",
        "outputId": "9458594b-2408-4177-cef1-dabb19cf68d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_486\n",
            "Total number of codes = 941\n"
          ]
        }
      ],
      "source": [
        "# print(label_to_ix[\"d_486\"])\n",
        "print(ix_to_label[0])\n",
        "print(f\"Total number of codes = {len(label_to_ix)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4JxLlXIfI8_"
      },
      "source": [
        "Create a word vector (intersected words) for each of the ICD-9 codes found in combined_dataset.\n",
        "*   If a ICD-9 code can be found in Wiki documents: label index -> ICD-9 code -> sequence/index of Wiki document -> vector of intersected words of Wikidocument (1 x number of intersected words).\n",
        "*   If ICD-9 code cannot be found in Wiki document: zero vector in shape of (1 x number of intersected words).\n",
        "Create a mapping between ICD-9 code index in label and corresponding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m7qfTVDeap9"
      },
      "outputs": [],
      "source": [
        "tempwikivec=[]\n",
        "\n",
        "for i in range(0,len(ix_to_label)):\n",
        "  if ix_to_label[i] in wikivoc: # if a code in note can be found in wiki docs\n",
        "    temp=wikivec[codewiki[ix_to_label[i]][0]] # save wiki doc index to temp\n",
        "    tempwikivec.append(temp)\n",
        "  else:\n",
        "    tempwikivec.append([0.0]*wikivec.shape[1])\n",
        "wikivec=np.array(tempwikivec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh67nu1Mi-XD"
      },
      "source": [
        "Print sample result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0sD4EbodGiT"
      },
      "outputs": [],
      "source": [
        "# print(f\"If the ICD-9 code is = {ix_to_label[0]}\")\n",
        "# print(f\"The index of the corresponding wiki doc = {codewiki[ix_to_label[0]]}\")\n",
        "# print(f\"The index of the corresponding wiki doc = {codewiki[ix_to_label[0]][0]}\")\n",
        "# print(f\"The vector of intersected words for the corresponding wiki doc = {wikivec[codewiki[ix_to_label[0]][0]]}\")\n",
        "# print(f\"The shape of the vector is = {wikivec[codewiki[ix_to_label[0]][0]].shape}\")\n",
        "# print(f\"If the ICD-9 code cannot be found is Wiki docs, the vector = {[0.0]*wikivec.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWwAFL04ktBd"
      },
      "source": [
        "Create dataset. The dataset contains 3 parts:\n",
        "*   Feature: a list of lists, and each element is a list of words (strings) for one clinical note.\n",
        "*   Notevec: a list of vectors, and each element is a vector of intersected words for one clinical note.\n",
        "*   Label: a list of lists, and each element is a list of ICD-codes for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9OSJf1Nec4t"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "for i in range(0,len(feature)):\n",
        "  # save feature (list of words for note), note matrix and label (code) as a tuple\n",
        "  data.append((feature[i], notevec[i], label[i]))\n",
        "    \n",
        "data=np.array(data, dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Dug1BolmqH"
      },
      "source": [
        "Print the first set of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jugNQbELlqMx",
        "outputId": "a86fb5a2-b3ed-467f-bbff-3bc6e020408f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['admission', 'date', 'discharge', 'date', 'date', 'birth', 'sex', 'm', 'service', 'medicine', 'allergies', 'patient', 'recorded', 'known', 'allergies', 'drugs', 'attendinglast', 'name', 'un', 'chief', 'complaint', 'fever', 'cough', 'weakness', 'major', 'surgical', 'invasive', 'procedure', 'rigid', 'bronchoscopy', 'endobronchial', 'debulking', 'squamous', 'cell', 'lung', 'mass', 'endotracheal', 'intubation', 'history', 'present', 'illness', 'mr', 'known', 'lastname', 'yo', 'man', 'w', 'recentlydiagnosed', 'lul', 'poorlydifferentiated', 'squamous', 'cell', 'lung', 'cancer', 'admitted', 'hypotension', 'fever', 'secondary', 'postobstructive', 'pneumonia', 'patient', 'back', 'pain', 'weight', 'loss', 'productive', 'cough', 'mild', 'hemoptysis', 'beginning', 'month', 'month', 'time', 'revealed', 'lul', 'opacity', 'fu', 'ct', 'thorax', 'revealed', 'lul', 'mass', 'x', 'cm', 'concerning', 'malignancy', 'patient', 'underwent', 'flexible', 'bronchoscopy', 'endobronchial', 'biopsies', 'linear', 'endobronchial', 'ultrasound', 'lymph', 'node', 'biopsies', 'procedures', 'performed', 'dr', 'last', 'name', 'stitle', 'endobronchial', 'biopsies', 'revealed', 'poorlydifferentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'presented', 'pcps', 'dr', 'first', 'name', 'stitle', 'angels', 'office', 'worsening', 'cough', 'fever', 'weakness', 'vitals', 'pcps', 'office', 'sbp', 'tachycardic', 'breathing', 'ra', 'diminished', 'l', 'upper', 'lung', 'breath', 'sounds', 'per', 'pcp', 'name10', 'nameis', 'revealed', 'new', 'infiltrate', 'peripheral', 'mass', 'lesions', 'likely', 'consistent', 'pna', 'referred', 'ed', 'pcp', 'time', 'ed', 'vitals', '2l', 'monthyear', 'done', 'ed', 'also', 'showed', 'likely', 'postobstructive', 'pna', 'patient', 'given', '3l', 'ns', 'gram', 'iv', 'vancomycin', 'mg', 'po', 'levofloxacin', 'sent', 'micu', 'started', 'ampicillinsulbactam', 'unasyn', 'continued', 'levofloxacin', 'micu', 'hypotension', 'stablized', 'proven', 'fluid', 'responsive', 'breathing', 'comfortably', 'l', 'oxygen', 'lul', 'mass', 'thought', 'obstructing', 'airway', 'endobronchial', 'debulking', 'performed', 'afternoon', 'prior', 'transfer', 'micu', 'floor', 'suggested', 'ip', 'consult', 'patients', 'procedure', 'cb', 'hypoventilation', 'paco2', 'noted', 'one', 'point', 'mechanical', 'pacu', 'lowered', 'paco2', 'mid50s', 'patient', 'extubated', 'transferred', 'floor', 'o2', 'saturation', 'past', 'medical', 'history', 'hypercholesterolemia', 'osteoarthritis', 'hip', 'social', 'history', 'lives', 'alone', 'always', 'lived', 'ma', 'works', 'post', 'office', 'letter', 'carrier', 'prior', 'employment', 'government', 'contractor', 'smoked', 'ppd', 'x', 'yrs', 'quit', 'drinks', 'beers', 'family', 'history', 'prostate', 'cancer', 'father', 'deceased', 'hypertension', 'mother', 'alive', 'physical', 'exam', 'admission', 'vs', 't', 'bp', 'p', 'r', 'sat', '97ra', 'gen', 'nad', 'aaox3', 'looks', 'comfortable', 'heent', 'mm', 'slightly', 'dry', 'lesions', 'exudate', 'noted', 'neck', 'cervical', 'supraclavicular', 'lad', 'cv', 'rrr', 's1s2', 'mrg', 'audible', 'lungs', 'focal', 'area', 'wheezing', 'rales', 'left', 'upper', 'lobe', 'scattered', 'wheezes', 'left', 'lung', 'whole', 'right', 'lung', 'ctab', 'wrr', 'dullness', 'percussion', 'left', 'upper', 'lung', 'noted', 'egophony', 'abd', 'bs', 'normoactive', 'soft', 'ntnd', 'hepatosplenomegaly', 'noted', 'ext', 'wwp', 'pitting', 'edema', 'present', 'legs', 'bilaterally', 'halfway', 'shin', 'nontender', 'palpation', 'neuro', 'aaox3', 'cns', 'iixii', 'intact', 'sensation', 'intact', 'lt', 'discharge', 'vs', 'hr89', 'bp14080', 'rr18', 'general', 'comfortable', 'appearing', 'nad', 'heent', 'op', 'clear', 'cv', 'rrr', 'normal', 's1', 's1', 'mrg', 'resp', 'unlabored', 'breathing', 'minimal', 'diffuse', 'endexpiratory', 'wheezing', 'throughout', 'worse', 'lul', 'decreased', 'bs', 'left', 'lung', 'abdominal', 'sntnd', 'extremities', 'pulses', 'pertinent', 'results', 'admission', 'labs', '0545pm', 'blood', 'wbc228', 'rbc427', 'hgb120', 'hct363', 'mcv85', 'mch281', 'mchc330', 'rdw136', 'plt', 'ct309', '0545pm', 'blood', 'neuts91', 'bands2', 'lymphs2', 'monos5', 'eos0', 'baso0', 'atyps0', 'metas0', 'myelos0', '0545pm', 'blood', 'pt158', 'ptt314', 'inrpt14', '0545pm', 'blood', 'glucose114', 'urean20', 'creat12', 'na138', 'k42', 'cl99', 'hco326', 'angap17', '0545pm', 'blood', 'alt21', 'ast19', 'alkphos118', 'totbili31', '0928pm', 'blood', 'calcium77', 'phos38', 'mg18', 'ct', 'chest', 'contrast', 'enlarging', 'left', 'upper', 'lobe', 'mass', 'left', 'upper', 'bronchial', 'stenosis', 'level', 'stenosis', 'pulmonary', 'artery', 'less', '11mm', 'posterior', 'bronchus', 'encasement', 'left', 'upper', 'lobe', 'pulmonary', 'artery', 'postobstructive', 'pneumonia', 'new', 'bilateral', 'small', 'pleural', 'effusions', 'new', 'left', 'basal', 'pneumonia', 'probably', 'aspiration', 'left', 'upper', 'lobe', 'contents', 'interval', 'results', 'mri', 'head', 'without', 'contrast', 'evidence', 'intracranial', 'metastatic', 'disease', 'ct', 'chest', 'without', 'contrast', 'progression', 'left', 'upper', 'lobe', 'atelectasis', 'consolidation', 'status', 'post', 'recent', 'debridement', 'gas', 'area', 'previously', 'seen', 'lesion', 'reopening', 'left', 'upper', 'lobe', 'bronchus', 'stenosis', 'left', 'upper', 'lobe', 'bronchus', 'however', 'mild', 'increase', 'aeration', 'apicoposterior', 'segment', 'peripheral', 'dilated', 'bronchioles', 'progression', 'groundglass', 'consolidation', 'left', 'lower', 'lobe', 'possibly', 'related', 'aspiration', 'part', 'compressive', 'atelectasis', 'due', 'enlarging', 'left', 'pleural', 'effusion', 'small', 'right', 'pleural', 'effusion', 'adjacent', 'atelectasis', 'unchanged', 'lymphadenopathy', 'pleural', 'fluid', 'analysis', 'pmns', 'organisms', 'fluid', 'culture', 'without', 'growth', 'anaerobic', 'culture', 'without', 'growth', 'fdg', 'tumor', 'imaging', 'fdg', 'avid', 'left', 'upper', 'lobe', 'consolidation', 'focally', 'increased', 'fdg', 'avidity', 'surrounding', 'narrowed', 'left', 'upper', 'lobe', 'bronchus', 'compatible', 'known', 'squamous', 'cell', 'carcinoma', 'postobstructive', 'collapse', 'consolidation', 'left', 'upper', 'lobe', 'fdg', 'avid', 'consolidation', 'left', 'lower', 'lobe', 'lingula', 'likely', 'pneumonia', 'aspiration', 'moderate', 'left', 'small', 'right', 'pleural', 'effusions', 'fdg', 'avid', 'mediastinal', 'lymphadenopathy', 'trace', 'free', 'fluid', 'pelvis', 'chest', 'ct', 'contrast', 'interval', 'improvement', 'left', 'upper', 'lobe', 'postobstructive', 'pneumonia', 'improved', 'aeration', 'anterior', 'aspect', 'left', 'upper', 'lobe', 'foci', 'gas', 'within', 'consolidated', 'parenchyma', 'similar', 'appearance', 'potential', 'communication', 'left', 'upper', 'lobe', 'segmental', 'bronchus', 'collections', 'air', 'possible', 'known', 'mass', 'lesion', 'well', 'evaluated', 'due', 'surrounding', 'consolidation', 'slight', 'interval', 'increase', 'left', 'pleural', 'effusion', 'resolution', 'right', 'pleural', 'effusion', 'extensive', 'lymphadenopathy', 'unchanged', 'unclear', 'component', 'reactive', 'versus', 'involved', 'malignancy', 'ultrasound', 'doppler', 'lower', 'extremities', 'nonocclusive', 'thrombosis', 'bilateral', 'popliteal', 'right', 'peroneal', 'left', 'posterior', 'tibialis', 'veins', 'blood', 'cultures', 'repeated', 'times', 'admission', 'always', 'without', 'growth', 'chest', 'xrays', 'repeated', 'times', 'throughout', 'admission', 'demonstrated', 'left', 'upper', 'lobe', 'postobstructive', 'pneumonia', 'little', 'improvement', 'images', 'later', 'stay', 'revealed', 'almost', 'complete', 'white', 'left', 'lung', 'collapse', 'left', 'lower', 'lobe', 'detected', 'late', 'hospital', 'course', 'mediastinal', 'lymph', 'node', 'biopsy', 'lymph', 'node', '4r', 'biopsy', 'ad', 'malignancy', 'identified', 'lymph', 'node', '4l', 'biopsy', 'ef', 'malignancy', 'identified', 'lymph', 'node', 'level', 'biopsy', 'gh', 'malignancy', 'identified', 'lymph', 'node', '2r', 'biopsy', 'malignancy', 'identified', 'lymph', 'node', '2l', 'biopsy', 'j', 'malignancy', 'identified', 'discharge', 'results', '0720am', 'blood', 'wbc91', 'rbc366', 'hgb99', 'hct311', 'mcv85', 'mch270', 'mchc317', 'rdw157', 'plt', 'ct470', '0720am', 'blood', 'glucose86', 'urean5', 'creat07', 'na140', 'k38', 'cl101', 'hco332', 'angap11', '0720am', 'blood', 'albumin24', 'calcium80', 'phos36', 'mg23', 'brief', 'hospital', 'course', 'yo', 'man', 'left', 'upper', 'lobe', 'poorlydifferentiated', 'squamous', 'cell', 'carcinoma', 'presented', 'fevers', 'leukocytosis', 'hypotension', 'setting', 'lul', 'postobstructive', 'pna', 'hypotension', 'patient', 'presented', 'systolic', 'blood', 'pressures', '80s', 'patient', 'sent', 'micu', 'pressure', 'came', '100s', 'following', '4l', 'ns', 'mental', 'status', 'never', 'compromised', 'lactate', 'patient', 'subsequently', 'transferred', 'general', 'medicine', 'floors', 'patient', 'continued', 'intermittent', 'hypotensive', 'episodes', 'stay', 'associated', 'severe', 'night', 'sweats', 'fevers', 'infectious', 'work', 'negative', 'multiple', 'occassions', 'fevers', 'attributed', 'tumor', 'started', 'scheduled', 'acetaminophen', 'continuous', 'ivf', 'patient', 'subsequent', 'episodes', 'hypotension', 'postobstructive', 'pneumonia', 'setting', 'known', 'left', 'upper', 'lobe', 'squamous', 'cell', 'carcinoma', 'radiographic', 'evidence', 'high', 'concern', 'postobstructive', 'pneumonia', 'patient', 'started', 'vancomycin', 'levofloxacin', 'ed', 'switched', 'ampicillinsulbactam', 'micu', 'interventional', 'pulmonology', 'performed', 'rigid', 'bronchoscopy', 'bulk', 'resection', 'antibiotics', 'switched', 'levofloxacin', 'metronidazole', 'per', 'ids', 'recommendations', 'patient', 'subsequently', 'began', 'radiation', 'therapy', 'effort', 'shrink', 'tumor', 'hopes', 'definitive', 'therapy', 'pneumonia', 'repeat', 'chest', 'xrays', 'throughout', 'admission', 'demonstrated', 'little', 'improvement', 'postobstructive', 'pnemonia', 'subsequent', 'collapse', 'left', 'lower', 'lobe', 'progression', 'left', 'upper', 'lobe', 'patient', 'transitioned', 'levofloxacin', 'flagyl', 'antibiotics', 'discontinued', 'discharge', 'patient', 'completed', 'days', 'antibiotics', 'little', 'concern', 'infectious', 'etiology', 'fevers', 'per', 'left', 'upper', 'lobe', 'poorly', 'differentiated', 'squamous', 'cell', 'carcinoma', 'patient', 'diagnosed', 'several', 'weeks', 'prior', 'admission', 'time', 'lymph', 'nodes', 'clear', 'brain', 'mri', 'showed', 'early', 'hospitalization', 'revealed', 'evidence', 'brain', 'metastases', 'pet', 'scan', 'revealed', 'mediastinal', 'lymph', 'nodes', 'suspicious', 'metastases', 'patient', 'subsequently', 'began', 'xrt', 'treatments', 'shrink', 'tumor', 'enough', 'treat', 'postobstructive', 'pneumonia', 'thoracic', 'surgery', 'consulted', 'performed', 'mediastinoscopy', 'sampled', 'lymph', 'nodes', 'negative', 'malignant', 'cells', 'patient', 'discharged', 'instructions', 'follow', 'oncologist', 'well', 'chest', 'disease', 'center', 'pleural', 'effusion', 'pleural', 'fluid', 'negative', 'malignant', 'cells', 'thoracentesis', 'cultures', 'showing', 'empyema', 'exudative', 'parapneumonic', 'effusion', 'suspected', 'evidence', 'empyema', 'patient', 'treated', 'pneumonia', 'per', 'partial', 'occlusive', 'thromboses', 'patient', 'mild', 'rightsided', 'lower', 'extremity', 'edema', 'midway', 'admission', 'ultrasound', 'study', 'dopplers', 'revealed', 'nonocclusive', 'thromboses', 'bilateral', 'popliteal', 'right', 'peroneal', 'left', 'posterior', 'tibialis', 'veins', 'patient', 'initially', 'treated', 'heparin', 'drip', 'due', 'concerns', 'renal', 'function', 'patient', 'eventually', 'treated', 'enoxaparin', 'mg', 'subcutaneous', 'injection', 'twice', 'daily', 'discharged', 'home', 'instructions', 'continue', 'injections', 'thrombocytosis', 'likely', 'acute', 'phase', 'reactant', 'setting', 'ongoing', 'infection', 'inflammation', 'patients', 'platelet', 'counts', 'remained', 'elevated', 'throughout', 'admission', 'anemia', 'hematocrit', 'stable', 'throughout', 'admission', 'iron', 'studies', 'consistent', 'anemia', 'chronic', 'inflammation', 'medications', 'admission', 'none', 'discharge', 'medications', 'combivent', 'mcgactuation', 'aerosol', 'sig', 'inhalation', 'every', 'hours', 'needed', 'shortness', 'breath', 'wheezing', 'disp1', 'refills0', 'codeineguaifenesin', 'mg5', 'ml', 'syrup', 'sig', 'mls', 'po', 'q6h', 'every', 'hours', 'needed', 'cough', 'weeks', 'disp1', 'ml', 'bottle', 'refills1', 'fluconazole', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q24h', 'every', 'hours', 'days', 'disp8', 'tablets', 'refills0', 'acetaminophen', 'mg', 'tablet', 'sig', 'two', 'tablet', 'po', 'q6h', 'every', 'hours', 'disp120', 'tablets', 'refills0', 'lovenox', 'mg08', 'ml', 'syringe', 'sig', 'seventy', 'mg', 'subcutaneous', 'twice', 'day', 'disp10', 'syringes', 'refills0', 'lorazepam', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q4h', 'every', 'hours', 'needed', 'anxiety', 'disp20', 'tablets', 'refills0', 'ranitidine', 'hcl', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'daily', 'daily', 'disp30', 'tablets', 'refills2', 'morphine', 'mg', 'tablet', 'sig', 'one', 'tablet', 'po', 'q4h', 'every', 'hours', 'needed', 'pain', 'disp20', 'tablets', 'refills0', 'discharge', 'disposition', 'home', 'service', 'facility', 'care', 'vna', 'greater', 'location', 'un', 'discharge', 'diagnosis', 'primary', 'diagnoses', 'postobstructive', 'pneumonia', 'seconday', 'diagnoses', 'squamous', 'cell', 'lung', 'cancer', 'discharge', 'condition', 'mental', 'status', 'clear', 'coherent', 'level', 'consciousness', 'alert', 'interactive', 'activity', 'status', 'ambulatory', 'independent', 'discharge', 'instructions', 'dear', 'mr', 'known', 'lastname', 'pleasure', 'taking', 'care', 'hospital1', 'admitted', 'hospital', 'pneumonia', 'fever', 'low', 'blood', 'pressure', 'also', 'known', 'hypotension', 'known', 'lung', 'cancer', 'obstructing', 'airways', 'leading', 'known', 'postobstructive', 'pneumonia', 'procedure', 'debulk', 'lung', 'cancer', 'attempt', 'open', 'airways', 'order', 'allow', 'pneumonia', 'resolve', 'received', 'antibiotics', 'will', 'need', 'continue', 'outpatient', 'hospital', 'began', 'workup', 'staging', 'lung', 'cancer', 'brain', 'mri', 'show', 'metastases', 'scheduled', 'petct', 'scan', 'outpatient', 'will', 'need', 'follow', 'directions', 'oral', 'contrast', 'provided', 'prior', 'discharge', 'follow', 'dr', 'first', 'name', 'stitle', 'medical', 'oncology', 'hospital1', 'dr', 'first', 'name', 'stitle', 'thoracic', 'surgery', 'dr', 'last', 'name', 'stitle', 'radiation', 'oncology', 'following', 'changes', 'made', 'medications', 'start', 'using', 'combivent', 'inhaler', 'inhalations', 'every', 'hours', 'needed', 'shortness', 'breath', 'start', 'using', 'codeineguaifenesin', 'cough', 'syrup', 'ml', 'mouth', 'every', 'six', 'hours', 'needed', 'cough', 'start', 'using', 'fluconazole', 'mg', 'mouth', 'day', 'medication', 'sore', 'throat', 'will', 'need', 'take', 'days', 'start', 'taking', 'acetaminophen', 'mg', 'take', 'two', 'tablets', 'mouth', 'every', 'six', 'hours', 'needed', 'fever', 'exceed', 'tablets', 'per', 'day', 'start', 'using', 'lovenox', 'mg', 'subcutaneous', 'injection', 'twice', 'day', 'medication', 'blood', 'clots', 'found', 'legs', 'start', 'using', 'lorazepam', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'anxiety', 'start', 'taking', 'ranitidine', 'mg', 'mouth', 'day', 'medication', 'helps', 'reflux', 'start', 'taking', 'morphine', 'sulfate', 'ir', 'mg', 'mouth', 'every', 'four', 'hours', 'needed', 'pain', 'followup', 'instructions', 'department', 'chest', 'disease', 'center', 'name', 'last', 'name', 'lf', 'first', 'name7', 'namepattern1', 'initial', 'namepattern1', 'last', 'name', 'namepattern4', 'please', 'call', 'thoracic', 'oncology', 'program', 'schedule', 'upcoming', 'appointment', 'dr', 'first', 'name', 'stitle', 'days', 'hospital', 'discharge', 'please', 'call', 'office', 'number', 'listed', 'make', 'appointment', 'location', 'hospital1', 'address', 'location', 'un', 'hospital', 'ward', 'name', 'location', 'un', 'numeric', 'identifier', 'phone', 'department', 'pulmonary', 'function', 'lab', 'monday', 'pulmonary', 'function', 'lab', 'telephonefax', 'building', 'gz', 'hospital', 'ward', 'name', 'building', 'felbeerghospital', 'ward', 'name', 'complex', 'location', 'un', 'campus', 'east', 'best', 'parking', 'main', 'garage', 'department', 'radiology', 'wednesday', 'pm', 'xmr', 'telephonefax', 'building', 'cc', 'location', 'un', 'hospital', 'campus', 'west', 'best', 'parking', 'street', 'address1', 'garage', 'petct', 'scan', 'department', 'radiology', 'thursday', 'pm', 'location', 'hospital', 'ward', 'name', 'center', 'location', 'un', 'campus', 'east', 'department', 'primary', 'care', 'name', 'dr', 'first', 'name8', 'namepattern2', 'doctor', 'last', 'name', 'monday', 'pm', 'location', 'location', 'un', 'hospital1', 'address', 'country', '3rd', 'fl', 'hospital1', 'numeric', 'identifier', 'phone', 'telephonefax', 'completed']\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "['d_486', 'd_518', 'd_511', 'd_162', 'd_112', 'd_196', 'd_453', 'd_263', 'd_780', 'd_787', 'd_276', 'd_272', 'd_715', 'd_799', 'd_288', 'd_285', 'd_327', 'd_V15', 'd_E87']\n"
          ]
        }
      ],
      "source": [
        "print(data[0][0])\n",
        "print(data[0][1])\n",
        "print(data[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xbg7p7InnXq"
      },
      "source": [
        "Create mapping between ICD-9 codes and the index in the code vector by 2 dictionaries.  \n",
        "**Different from previous label_to_ix and ix_to_label, this mapping is for ICD-9 codes found in Wiki documents only.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiEMkZjkelOm"
      },
      "outputs": [],
      "source": [
        "label_to_ix = {}\n",
        "ix_to_label = {}\n",
        "\n",
        "for doc, note, codes in data:\n",
        "  for code in codes:\n",
        "    if code not in label_to_ix:\n",
        "      if code in wikivoc:\n",
        "        label_to_ix[code]=len(label_to_ix)\n",
        "        ix_to_label[label_to_ix[code]]=code\n",
        "\n",
        "np.save('label_to_ix',label_to_ix)\n",
        "np.save('ix_to_label',ix_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTLBW7dqUgf"
      },
      "source": [
        "Print sample key-value pairs.  \n",
        "Print the number of ICD-9 codes which **exists in both clinical notes and Wiki documents** (From paper: \"Of those codes, we selected a subset of 344 codes for which we found the corresponding Wikipedia document and used those codes in our experiments.\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUc2UUh-nMqZ",
        "outputId": "5c74e4d0-1b9e-4db7-c163-f185db2d1e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of codes = 344\n"
          ]
        }
      ],
      "source": [
        "# print(label_to_ix[\"d_486\"])\n",
        "# print(ix_to_label[0])\n",
        "print(f\"Total number of codes = {len(label_to_ix)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2BllzNvJnH"
      },
      "source": [
        "Split training data, validation data, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90BcKLLXerYC"
      },
      "outputs": [],
      "source": [
        "training_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "training_data, val_data = train_test_split(training_data, test_size=0.125, random_state=42)\n",
        "\n",
        "np.save('training_data',training_data)\n",
        "np.save('test_data',test_data)\n",
        "np.save('val_data',val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYfRRu_pB2K7"
      },
      "source": [
        "Create index for words in clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNJKARBVeuI4"
      },
      "outputs": [],
      "source": [
        "word_to_ix = {}\n",
        "ix_to_word={}\n",
        "ix_to_word[0]='OUT'\n",
        "\n",
        "for doc, note, codes in training_data:\n",
        "  for word in doc:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)+1\n",
        "      ix_to_word[word_to_ix[word]]=word  \n",
        "    \n",
        "np.save('word_to_ix',word_to_ix)\n",
        "np.save('ix_to_word',ix_to_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pl1zvOmGHBt"
      },
      "source": [
        "Print sample key-value pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiidFU4cDLtV",
        "outputId": "1c515003-8894-4355-82a5-44509e7a4d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUT\n",
            "admission\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(ix_to_word[0])\n",
        "print(ix_to_word[1])\n",
        "print(word_to_ix['admission'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YRAJ4AtJuem"
      },
      "source": [
        "Create a word vector (intersected words) for each of the ICD-9 codes found in **both Wiki document and clinical notes (combined dataset)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_VMHm0Eewv5"
      },
      "outputs": [],
      "source": [
        "newwikivec=[]\n",
        "for i in range(0,len(ix_to_label)):\n",
        "  newwikivec.append(wikivec[prevoc[ix_to_label[i]]])\n",
        "newwikivec=np.array(newwikivec)\n",
        "np.save('newwikivec',newwikivec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGp0wg9QKHSx"
      },
      "source": [
        "Print sample result.  \n",
        "Print the number of vectors in wikivec and newwikivec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWdQx4t0Hb7g",
        "outputId": "beb641b6-134c-4b56-9d62-2fb0be2a2fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_486\n",
            "0\n",
            "[1. 0. 0. ... 0. 0. 0.]\n",
            "344\n",
            "941\n"
          ]
        }
      ],
      "source": [
        "print(ix_to_label[0])\n",
        "print(prevoc[ix_to_label[0]])\n",
        "print(wikivec[prevoc[ix_to_label[0]]])\n",
        "print(len(newwikivec))\n",
        "print(len(wikivec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3bF8Y-_817e",
        "outputId": "9740f9b1-3d51-4489-e881-f27a51639b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 04m 47s\n"
          ]
        }
      ],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aalecYXO-8lt"
      },
      "source": [
        "# CAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDfshwRU9jvC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoPECSTU97Rg",
        "outputId": "80d162f6-cbf1-4e94-ea55-86ae9c455656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 57s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy', allow_pickle=True)\n",
        "training_data=np.load('training_data.npy', allow_pickle=True)\n",
        "test_data=np.load('test_data.npy', allow_pickle=True)\n",
        "val_data=np.load('val_data.npy', allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy', allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy', allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy', allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data, dtype=object)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype=int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okDHhv5Hu6dx",
        "outputId": "6e34353a-c3a9-47c4-8321-c513736a312a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 00s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "class CAML(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CAML, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)   \n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,300,10,padding=5)\n",
        "        self.H=nn.Linear(300, tagset_size )   \n",
        "        self.final = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize,bias=False)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.tanh(self.convs1(thisembeddings).transpose(1,2))  \n",
        "        \n",
        "        alpha=self.H.weight.matmul(thisembeddings.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(thisembeddings)\n",
        "       \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "              \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lXtJz6GunsQ",
        "outputId": "62ac2181-df3f-434d-d72c-b43209bb8f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max epochs = 5000\n",
            "Train basemodel\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.6044048484082241\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.7351386587887544\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.7708715147202543\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.7896376400647074\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.7972106688739532\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.8032410262762794\n",
            "Update the best model to epoch 5\n",
            "epoch = 6\n",
            "validation recall @ top- 10 0.8068210941543067\n",
            "Update the best model to epoch 6\n",
            "epoch = 7\n",
            "validation recall @ top- 10 0.8057932495000231\n",
            "epoch = 8\n",
            "validation recall @ top- 10 0.8050636399093615\n",
            "epoch = 9\n",
            "validation recall @ top- 10 0.8070068471661362\n",
            "Update the best model to epoch 9\n",
            "epoch = 10\n",
            "validation recall @ top- 10 0.8080802677316081\n",
            "Update the best model to epoch 10\n",
            "epoch = 11\n",
            "validation recall @ top- 10 0.8103594326902616\n",
            "Update the best model to epoch 11\n",
            "epoch = 12\n",
            "validation recall @ top- 10 0.8107272415390413\n",
            "Update the best model to epoch 12\n",
            "epoch = 13\n",
            "validation recall @ top- 10 0.8108112247445151\n",
            "Update the best model to epoch 13\n",
            "epoch = 14\n",
            "validation recall @ top- 10 0.8096504292932908\n",
            "epoch = 15\n",
            "validation recall @ top- 10 0.8116378123917865\n",
            "Update the best model to epoch 15\n",
            "epoch = 16\n",
            "validation recall @ top- 10 0.8080686757472396\n",
            "epoch = 17\n",
            "validation recall @ top- 10 0.8095293844686592\n",
            "epoch = 18\n",
            "validation recall @ top- 10 0.8068004456323206\n",
            "epoch = 19\n",
            "validation recall @ top- 10 0.8076591483243601\n",
            "epoch = 20\n",
            "validation recall @ top- 10 0.807397830668488\n",
            "[0.6044048484082241, 0.7351386587887544, 0.7708715147202543, 0.7896376400647074, 0.7972106688739532, 0.8032410262762794, 0.8068210941543067, 0.8057932495000231, 0.8050636399093615, 0.8070068471661362, 0.8080802677316081, 0.8103594326902616, 0.8107272415390413, 0.8108112247445151, 0.8096504292932908, 0.8116378123917865, 0.8080686757472396, 0.8095293844686592, 0.8068004456323206, 0.8076591483243601, 0.807397830668488] 15\n",
            "\n",
            "Train model with KSI\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.8113584807052281\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.8096797653301756\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.8095899164935788\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.8099924492307755\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.8095997721040316\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.8087621442463372\n",
            "[0.8113584807052281, 0.8096797653301756, 0.8095899164935788, 0.8099924492307755, 0.8095997721040316, 0.8087621442463372] 0\n",
            "Time duration: 0h 22m 13s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk = 10\n",
        "max_epochs = 5000 # Default is 5000\n",
        "print(f\"max epochs = {max_epochs}\")\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(max_epochs):  \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            \n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (f\"epoch = {epoch}\")\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "           \n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation recall @ top-',topk, np.mean(recall))\n",
        "           \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "            print(f\"Update the best model to epoch {bestiter}\")\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "\n",
        "    print(f\"Reach the max epochs, return the best model at epoch {bestiter}\")\n",
        "    return modelsaved[bestiter]\n",
        "\n",
        "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "print(\"Train basemodel\")\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'CAML_model')\n",
        "\n",
        "model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"\")\n",
        "print(\"Train model with KSI\")\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_CAML_model')\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ3VaeGnunce",
        "outputId": "221b13e1-d551-4fbc-a253-e7a561644b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test CAML baseline\n",
            "Test KSI+CAML\n",
            "Time duration: 0h 00m 27s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Test the model:\n",
        "\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    # model.cpu()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "       \n",
        "        targets = inputs[2].cuda()\n",
        "        # targets = inputs[2].cpu()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "        # tag_scores = model(inputs[0].cpu(),inputs[1].cpu() ,wikivec.cpu(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        targets = targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "               \n",
        "        # lossestest.append(loss.data.mean())\n",
        "        lossestest.append(loss.data.cpu().mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(int)\n",
        "    # print ('test loss', np.mean(lossestest))\n",
        "    # print ('top-',topk, np.mean(recall))\n",
        "    # print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    # print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    # print ('macro F1', f1_score(y_true, y_pred, average='macro'))\n",
        "    # print ('micro F1', f1_score(y_true, y_pred, average='micro'))\n",
        "    test_loss = np.mean(lossestest)\n",
        "    test_recall = np.mean(recall)\n",
        "    test_mac_auc = roc_auc_score(y_true, y_scores,average='macro')\n",
        "    test_mic_auc = roc_auc_score(y_true, y_scores,average='micro')\n",
        "    test_mac_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    test_mic_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "    return test_loss, test_recall, test_mac_auc, test_mic_auc, test_mac_f1, test_mic_f1\n",
        "\n",
        "print('Test CAML baseline')\n",
        "caml_loss, caml_recall, caml_mac_auc, caml_mic_auc, caml_mac_f1, caml_mic_f1 = testmodel(basemodel, 0)\n",
        "print('Test KSI+CAML')\n",
        "camlKSI_loss, camlKSI_recall, camlKSI_mac_auc, camlKSI_mic_auc, camlKSI_mac_f1, camlKSI_mic_f1 = testmodel(KSImodel, 1)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "WctM9Bmjxj7q",
        "outputId": "444b8cd1-077e-4cec-dbb4-4aed3d883f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Model      Loss  Recall@10  Macro_AUC  Micro_AUC  Macro_F1  \\\n",
              "0  CAML Baseline  0.033520   0.807766   0.852964   0.978119  0.278527   \n",
              "1     CAML + KSI  0.033304   0.808454   0.853904   0.978277  0.279034   \n",
              "\n",
              "   Micro_F1  \n",
              "0  0.656805  \n",
              "1  0.657468  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad1f23bd-780d-44b1-97eb-583ed462a9de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Recall@10</th>\n",
              "      <th>Macro_AUC</th>\n",
              "      <th>Micro_AUC</th>\n",
              "      <th>Macro_F1</th>\n",
              "      <th>Micro_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CAML Baseline</td>\n",
              "      <td>0.033520</td>\n",
              "      <td>0.807766</td>\n",
              "      <td>0.852964</td>\n",
              "      <td>0.978119</td>\n",
              "      <td>0.278527</td>\n",
              "      <td>0.656805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CAML + KSI</td>\n",
              "      <td>0.033304</td>\n",
              "      <td>0.808454</td>\n",
              "      <td>0.853904</td>\n",
              "      <td>0.978277</td>\n",
              "      <td>0.279034</td>\n",
              "      <td>0.657468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad1f23bd-780d-44b1-97eb-583ed462a9de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad1f23bd-780d-44b1-97eb-583ed462a9de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad1f23bd-780d-44b1-97eb-583ed462a9de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "result_caml = [['CAML Baseline', \n",
        "                caml_loss, \n",
        "                caml_recall, \n",
        "                caml_mac_auc, \n",
        "                caml_mic_auc, \n",
        "                caml_mac_f1, \n",
        "                caml_mic_f1], \n",
        "        ['CAML + KSI', \n",
        "         camlKSI_loss, \n",
        "         camlKSI_recall, \n",
        "         camlKSI_mac_auc, \n",
        "         camlKSI_mic_auc, \n",
        "         camlKSI_mac_f1, \n",
        "         camlKSI_mic_f1]]\n",
        "df_caml = pd.DataFrame(result_caml, columns=['Model', 'Loss', 'Recall@10', 'Macro_AUC', 'Micro_AUC', 'Macro_F1', 'Micro_F1'])\n",
        "df_caml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q3JjI9W5H7n"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMVm37jL5O1j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxIMrLTa55ra",
        "outputId": "54b7ac04-ea01-47b6-c4c6-7aa90f9229bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 56s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy', allow_pickle=True)\n",
        "training_data=np.load('training_data.npy', allow_pickle=True)\n",
        "test_data=np.load('test_data.npy', allow_pickle=True)\n",
        "val_data=np.load('val_data.npy', allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy', allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy', allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy', allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data, dtype=object)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YewIyMJX3QGE",
        "outputId": "f339fc81-3d88-4bf1-9676-169bb2163e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 00s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,100,3)\n",
        "        self.convs2 = nn.Conv1d(Embeddingsize,100,4)\n",
        "        self.convs3 = nn.Conv1d(Embeddingsize,100,5)\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        output1=self.tanh(self.convs1(thisembeddings))\n",
        "        output1=nn.MaxPool1d(output1.size()[2])(output1)\n",
        "        \n",
        "        output2=self.tanh(self.convs2(thisembeddings))\n",
        "        output2=nn.MaxPool1d(output2.size()[2])(output2)\n",
        "        \n",
        "        output3=self.tanh(self.convs3(thisembeddings))\n",
        "        output3=nn.MaxPool1d(output3.size()[2])(output3)\n",
        "        \n",
        "        output4 = torch.cat([output1,output2,output3], 1).squeeze(2)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        vec2 = self.hidden2tag(output4)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltyopui83P0j",
        "outputId": "58d65af7-294d-4b74-d248-fe8ce81e1c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max epochs = 5000\n",
            "Train basemodel\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.4424304468846055\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.5308329047788358\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.5740794226579505\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.6079622025717314\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.6447288187979041\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.6614662603343429\n",
            "Update the best model to epoch 5\n",
            "epoch = 6\n",
            "validation recall @ top- 10 0.6826196291581947\n",
            "Update the best model to epoch 6\n",
            "epoch = 7\n",
            "validation recall @ top- 10 0.6908042089212478\n",
            "Update the best model to epoch 7\n",
            "epoch = 8\n",
            "validation recall @ top- 10 0.7006907045359452\n",
            "Update the best model to epoch 8\n",
            "epoch = 9\n",
            "validation recall @ top- 10 0.7086330055181878\n",
            "Update the best model to epoch 9\n",
            "epoch = 10\n",
            "validation recall @ top- 10 0.7143359639700324\n",
            "Update the best model to epoch 10\n",
            "epoch = 11\n",
            "validation recall @ top- 10 0.7221337340582029\n",
            "Update the best model to epoch 11\n",
            "epoch = 12\n",
            "validation recall @ top- 10 0.7245529784698798\n",
            "Update the best model to epoch 12\n",
            "epoch = 13\n",
            "validation recall @ top- 10 0.7331144116444088\n",
            "Update the best model to epoch 13\n",
            "epoch = 14\n",
            "validation recall @ top- 10 0.7346623320075271\n",
            "Update the best model to epoch 14\n",
            "epoch = 15\n",
            "validation recall @ top- 10 0.739084766242849\n",
            "Update the best model to epoch 15\n",
            "epoch = 16\n",
            "validation recall @ top- 10 0.74258498694751\n",
            "Update the best model to epoch 16\n",
            "epoch = 17\n",
            "validation recall @ top- 10 0.7448436537455196\n",
            "Update the best model to epoch 17\n",
            "epoch = 18\n",
            "validation recall @ top- 10 0.7489912318837537\n",
            "Update the best model to epoch 18\n",
            "epoch = 19\n",
            "validation recall @ top- 10 0.7504308412054763\n",
            "Update the best model to epoch 19\n",
            "epoch = 20\n",
            "validation recall @ top- 10 0.7516970893754858\n",
            "Update the best model to epoch 20\n",
            "epoch = 21\n",
            "validation recall @ top- 10 0.7507147492625467\n",
            "epoch = 22\n",
            "validation recall @ top- 10 0.7532269572755552\n",
            "Update the best model to epoch 22\n",
            "epoch = 23\n",
            "validation recall @ top- 10 0.7536092283219894\n",
            "Update the best model to epoch 23\n",
            "epoch = 24\n",
            "validation recall @ top- 10 0.7542052036313841\n",
            "Update the best model to epoch 24\n",
            "epoch = 25\n",
            "validation recall @ top- 10 0.754400206743873\n",
            "Update the best model to epoch 25\n",
            "epoch = 26\n",
            "validation recall @ top- 10 0.7548458528291939\n",
            "Update the best model to epoch 26\n",
            "epoch = 27\n",
            "validation recall @ top- 10 0.7545866108028637\n",
            "epoch = 28\n",
            "validation recall @ top- 10 0.7561686807051905\n",
            "Update the best model to epoch 28\n",
            "epoch = 29\n",
            "validation recall @ top- 10 0.757016085319383\n",
            "Update the best model to epoch 29\n",
            "epoch = 30\n",
            "validation recall @ top- 10 0.7542160053990294\n",
            "epoch = 31\n",
            "validation recall @ top- 10 0.7579472403192409\n",
            "Update the best model to epoch 31\n",
            "epoch = 32\n",
            "validation recall @ top- 10 0.7533784844516794\n",
            "epoch = 33\n",
            "validation recall @ top- 10 0.7545045183128265\n",
            "epoch = 34\n",
            "validation recall @ top- 10 0.755801931155549\n",
            "epoch = 35\n",
            "validation recall @ top- 10 0.7536888329253996\n",
            "epoch = 36\n",
            "validation recall @ top- 10 0.7551620391467178\n",
            "[0.4424304468846055, 0.5308329047788358, 0.5740794226579505, 0.6079622025717314, 0.6447288187979041, 0.6614662603343429, 0.6826196291581947, 0.6908042089212478, 0.7006907045359452, 0.7086330055181878, 0.7143359639700324, 0.7221337340582029, 0.7245529784698798, 0.7331144116444088, 0.7346623320075271, 0.739084766242849, 0.74258498694751, 0.7448436537455196, 0.7489912318837537, 0.7504308412054763, 0.7516970893754858, 0.7507147492625467, 0.7532269572755552, 0.7536092283219894, 0.7542052036313841, 0.754400206743873, 0.7548458528291939, 0.7545866108028637, 0.7561686807051905, 0.757016085319383, 0.7542160053990294, 0.7579472403192409, 0.7533784844516794, 0.7545045183128265, 0.755801931155549, 0.7536888329253996, 0.7551620391467178] 31\n",
            "\n",
            "Train model with KSI\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.7617079210811224\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.7632571181651403\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.7637711379473388\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.7660241442077493\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.7680741047955959\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.7669783688214329\n",
            "epoch = 6\n",
            "validation recall @ top- 10 0.7660340090542793\n",
            "epoch = 7\n",
            "validation recall @ top- 10 0.7651142147534565\n",
            "epoch = 8\n",
            "validation recall @ top- 10 0.7667848070656706\n",
            "epoch = 9\n",
            "validation recall @ top- 10 0.7654266753368165\n",
            "[0.7617079210811224, 0.7632571181651403, 0.7637711379473388, 0.7660241442077493, 0.7680741047955959, 0.7669783688214329, 0.7660340090542793, 0.7651142147534565, 0.7667848070656706, 0.7654266753368165] 4\n",
            "Time duration: 0h 20m 17s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk = 10\n",
        "max_epochs = 5000 # Default is 5000\n",
        "print(f\"max epochs = {max_epochs}\")\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(max_epochs):  \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            \n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (f\"epoch = {epoch}\")\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "           \n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation recall @ top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "            print(f\"Update the best model to epoch {bestiter}\")\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "\n",
        "    print(f\"Reach the max epochs, return the best model at epoch {bestiter}\")\n",
        "    return modelsaved[bestiter]\n",
        "    \n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "print(\"Train basemodel\")\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'CNN_model')\n",
        "\n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"\")\n",
        "print(\"Train model with KSI\")\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_CNN_model')\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsHlC0dM3Ppz",
        "outputId": "ac35e33b-e36f-4573-fe89-c41f801ba06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test CNN baseline\n",
            "Test KSI+CNN\n",
            "Time duration: 0h 00m 24s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Test the model:\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "       \n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        \n",
        "        lossestest.append(loss.data.cpu().mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(int)\n",
        "    # print ('test loss', np.mean(lossestest))\n",
        "    # print ('top-',topk, np.mean(recall))\n",
        "    # print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    # print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    # print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    # print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "    test_loss = np.mean(lossestest)\n",
        "    test_recall = np.mean(recall)\n",
        "    test_mac_auc = roc_auc_score(y_true, y_scores,average='macro')\n",
        "    test_mic_auc = roc_auc_score(y_true, y_scores,average='micro')\n",
        "    test_mac_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    test_mic_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "    return test_loss, test_recall, test_mac_auc, test_mic_auc, test_mac_f1, test_mic_f1\n",
        "\n",
        "print('Test CNN baseline')\n",
        "cnn_loss, cnn_recall, cnn_mac_auc, cnn_mic_auc, cnn_mac_f1, cnn_mic_f1 = testmodel(basemodel, 0)\n",
        "print('Test KSI+CNN')\n",
        "cnnKSI_loss, cnnKSI_recall, cnnKSI_mac_auc, cnnKSI_mic_auc, cnnKSI_mac_f1, cnnKSI_mic_f1 = testmodel(KSImodel, 1)\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "wtI-cCHD0TGU",
        "outputId": "56a6496c-df1b-4a5b-a83c-be60ba987f2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Model      Loss  Recall@10  Macro_AUC  Micro_AUC  Macro_F1  Micro_F1\n",
              "0  CNN Baseline  0.038858   0.754856   0.831774   0.967348  0.214947  0.627527\n",
              "1     CNN + KSI  0.038051   0.765813   0.847441   0.970184  0.226481  0.635694"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e188962-abe4-46ce-b682-064ee9d6feb7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Recall@10</th>\n",
              "      <th>Macro_AUC</th>\n",
              "      <th>Micro_AUC</th>\n",
              "      <th>Macro_F1</th>\n",
              "      <th>Micro_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CNN Baseline</td>\n",
              "      <td>0.038858</td>\n",
              "      <td>0.754856</td>\n",
              "      <td>0.831774</td>\n",
              "      <td>0.967348</td>\n",
              "      <td>0.214947</td>\n",
              "      <td>0.627527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CNN + KSI</td>\n",
              "      <td>0.038051</td>\n",
              "      <td>0.765813</td>\n",
              "      <td>0.847441</td>\n",
              "      <td>0.970184</td>\n",
              "      <td>0.226481</td>\n",
              "      <td>0.635694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e188962-abe4-46ce-b682-064ee9d6feb7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e188962-abe4-46ce-b682-064ee9d6feb7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e188962-abe4-46ce-b682-064ee9d6feb7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "result_cnn = [['CNN Baseline', \n",
        "               cnn_loss, \n",
        "               cnn_recall, \n",
        "               cnn_mac_auc, \n",
        "               cnn_mic_auc, \n",
        "               cnn_mac_f1, \n",
        "               cnn_mic_f1], \n",
        "        ['CNN + KSI', \n",
        "         cnnKSI_loss, \n",
        "         cnnKSI_recall, \n",
        "         cnnKSI_mac_auc, \n",
        "         cnnKSI_mic_auc, \n",
        "         cnnKSI_mac_f1, \n",
        "         cnnKSI_mic_f1]]\n",
        "df_cnn = pd.DataFrame(result_cnn, columns=['Model', 'Loss', 'Recall@10', 'Macro_AUC', 'Micro_AUC', 'Macro_F1', 'Micro_F1'])\n",
        "df_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkDnKdxe59rK"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X66ixrUQ6E4i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB-EYuEP6IGe",
        "outputId": "e56fdf9f-cf33-4ee6-a10f-0eca203e7ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 54s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy', allow_pickle=True)\n",
        "training_data=np.load('training_data.npy', allow_pickle=True)\n",
        "test_data=np.load('test_data.npy', allow_pickle=True)\n",
        "val_data=np.load('val_data.npy', allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy', allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy', allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy', allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data, dtype=object)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqoAFdgx4v9a",
        "outputId": "e7200354-f2d9-4e2f-d7a2-c1c1ccef1756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 00s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "      \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "       \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(thisembeddings, self.hidden)\n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,2).transpose(0,1)\n",
        "        \n",
        "        output1=nn.MaxPool1d(lstm_out.size()[2])(lstm_out).view(batchsize,-1)\n",
        "        \n",
        "        vec2 = self.hidden2tag(output1)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfQfhEY64vzW",
        "outputId": "41c2cf38-c3b2-407a-ef0d-81b71a414c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max epochs = 5000\n",
            "Train basemodel\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.41915675729388885\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.4964443709283647\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.534239150672695\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.5581098953780302\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.5906730436186113\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.616447153576521\n",
            "Update the best model to epoch 5\n",
            "epoch = 6\n",
            "validation recall @ top- 10 0.6329954780439777\n",
            "Update the best model to epoch 6\n",
            "epoch = 7\n",
            "validation recall @ top- 10 0.6552253302863654\n",
            "Update the best model to epoch 7\n",
            "epoch = 8\n",
            "validation recall @ top- 10 0.6678222285296515\n",
            "Update the best model to epoch 8\n",
            "epoch = 9\n",
            "validation recall @ top- 10 0.6745732579811821\n",
            "Update the best model to epoch 9\n",
            "epoch = 10\n",
            "validation recall @ top- 10 0.6857097047979456\n",
            "Update the best model to epoch 10\n",
            "epoch = 11\n",
            "validation recall @ top- 10 0.6997465375512715\n",
            "Update the best model to epoch 11\n",
            "epoch = 12\n",
            "validation recall @ top- 10 0.7096993292483729\n",
            "Update the best model to epoch 12\n",
            "epoch = 13\n",
            "validation recall @ top- 10 0.7208846210095102\n",
            "Update the best model to epoch 13\n",
            "epoch = 14\n",
            "validation recall @ top- 10 0.7266796827484013\n",
            "Update the best model to epoch 14\n",
            "epoch = 15\n",
            "validation recall @ top- 10 0.7309898610464709\n",
            "Update the best model to epoch 15\n",
            "epoch = 16\n",
            "validation recall @ top- 10 0.7330629281365746\n",
            "Update the best model to epoch 16\n",
            "epoch = 17\n",
            "validation recall @ top- 10 0.7347730567047916\n",
            "Update the best model to epoch 17\n",
            "epoch = 18\n",
            "validation recall @ top- 10 0.7419084066619802\n",
            "Update the best model to epoch 18\n",
            "epoch = 19\n",
            "validation recall @ top- 10 0.7466012752920657\n",
            "Update the best model to epoch 19\n",
            "epoch = 20\n",
            "validation recall @ top- 10 0.749274865895651\n",
            "Update the best model to epoch 20\n",
            "epoch = 21\n",
            "validation recall @ top- 10 0.7547035996038524\n",
            "Update the best model to epoch 21\n",
            "epoch = 22\n",
            "validation recall @ top- 10 0.7571493650080121\n",
            "Update the best model to epoch 22\n",
            "epoch = 23\n",
            "validation recall @ top- 10 0.7569897179377186\n",
            "epoch = 24\n",
            "validation recall @ top- 10 0.760912884533752\n",
            "Update the best model to epoch 24\n",
            "epoch = 25\n",
            "validation recall @ top- 10 0.7606941179471385\n",
            "epoch = 26\n",
            "validation recall @ top- 10 0.764469191767622\n",
            "Update the best model to epoch 26\n",
            "epoch = 27\n",
            "validation recall @ top- 10 0.7660086832629677\n",
            "Update the best model to epoch 27\n",
            "epoch = 28\n",
            "validation recall @ top- 10 0.7649886453152468\n",
            "epoch = 29\n",
            "validation recall @ top- 10 0.7670606531878791\n",
            "Update the best model to epoch 29\n",
            "epoch = 30\n",
            "validation recall @ top- 10 0.7660001240060963\n",
            "epoch = 31\n",
            "validation recall @ top- 10 0.7669542702240184\n",
            "epoch = 32\n",
            "validation recall @ top- 10 0.7661112519461585\n",
            "epoch = 33\n",
            "validation recall @ top- 10 0.7697772512548066\n",
            "Update the best model to epoch 33\n",
            "epoch = 34\n",
            "validation recall @ top- 10 0.7677123429236384\n",
            "epoch = 35\n",
            "validation recall @ top- 10 0.7683423463002897\n",
            "epoch = 36\n",
            "validation recall @ top- 10 0.7689074869719944\n",
            "epoch = 37\n",
            "validation recall @ top- 10 0.7688869832047148\n",
            "epoch = 38\n",
            "validation recall @ top- 10 0.7702262020049365\n",
            "Update the best model to epoch 38\n",
            "epoch = 39\n",
            "validation recall @ top- 10 0.7699634924256384\n",
            "epoch = 40\n",
            "validation recall @ top- 10 0.7688582267282352\n",
            "epoch = 41\n",
            "validation recall @ top- 10 0.7709807938932101\n",
            "Update the best model to epoch 41\n",
            "epoch = 42\n",
            "validation recall @ top- 10 0.7692338770529726\n",
            "epoch = 43\n",
            "validation recall @ top- 10 0.7670430787451232\n",
            "epoch = 44\n",
            "validation recall @ top- 10 0.7688979534637163\n",
            "epoch = 45\n",
            "validation recall @ top- 10 0.7698961021191046\n",
            "epoch = 46\n",
            "validation recall @ top- 10 0.7699156071882571\n",
            "[0.41915675729388885, 0.4964443709283647, 0.534239150672695, 0.5581098953780302, 0.5906730436186113, 0.616447153576521, 0.6329954780439777, 0.6552253302863654, 0.6678222285296515, 0.6745732579811821, 0.6857097047979456, 0.6997465375512715, 0.7096993292483729, 0.7208846210095102, 0.7266796827484013, 0.7309898610464709, 0.7330629281365746, 0.7347730567047916, 0.7419084066619802, 0.7466012752920657, 0.749274865895651, 0.7547035996038524, 0.7571493650080121, 0.7569897179377186, 0.760912884533752, 0.7606941179471385, 0.764469191767622, 0.7660086832629677, 0.7649886453152468, 0.7670606531878791, 0.7660001240060963, 0.7669542702240184, 0.7661112519461585, 0.7697772512548066, 0.7677123429236384, 0.7683423463002897, 0.7689074869719944, 0.7688869832047148, 0.7702262020049365, 0.7699634924256384, 0.7688582267282352, 0.7709807938932101, 0.7692338770529726, 0.7670430787451232, 0.7688979534637163, 0.7698961021191046, 0.7699156071882571] 41\n",
            "\n",
            "Train model with KSI\n",
            "epoch = 0\n",
            "validation recall @ top- 10 0.7754507125782755\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top- 10 0.7777680744422477\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top- 10 0.7783557604514498\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top- 10 0.7796846206197312\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top- 10 0.7796908911708287\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top- 10 0.7789646451223894\n",
            "epoch = 6\n",
            "validation recall @ top- 10 0.7785336930779692\n",
            "epoch = 7\n",
            "validation recall @ top- 10 0.778134493907007\n",
            "epoch = 8\n",
            "validation recall @ top- 10 0.7785287483862124\n",
            "epoch = 9\n",
            "validation recall @ top- 10 0.7783702697175083\n",
            "[0.7754507125782755, 0.7777680744422477, 0.7783557604514498, 0.7796846206197312, 0.7796908911708287, 0.7789646451223894, 0.7785336930779692, 0.778134493907007, 0.7785287483862124, 0.7783702697175083] 4\n",
            "Time duration: 0h 59m 58s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk = 10\n",
        "max_epochs = 5000 # Default is 5000\n",
        "print(f\"max epochs = {max_epochs}\")\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(max_epochs):  \n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (f\"epoch = {epoch}\")\n",
        "        \n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation recall @ top-',topk, np.mean(recall))\n",
        "               \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "            print(f\"Update the best model to epoch {bestiter}\")\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "\n",
        "    print(f\"Reach the max epochs, return the best model at epoch {bestiter}\")\n",
        "    return modelsaved[bestiter]\n",
        "    \n",
        "model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"Train basemodel\")\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'LSTM_model')\n",
        "\n",
        "model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"\")\n",
        "print(\"Train model with KSI\")\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_LSTM_model')\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiC8A_fe4vmr",
        "outputId": "a670ab21-75ee-4adb-f94e-0a00ba7d5b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test LSTM baseline\n",
            "Test KSI+LSTM\n",
            "Time duration: 0h 00m 33s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Test the model:\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "        \n",
        "    for inputs in batchtest_data:\n",
        "        model.hidden = model.init_hidden()\n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        lossestest.append(loss.data.cpu().mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(int)\n",
        "    # print ('test loss', np.mean(lossestest))\n",
        "    # print ('top-',topk, np.mean(recall))\n",
        "    # print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    # print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    # print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    # print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "    test_loss = np.mean(lossestest)\n",
        "    test_recall = np.mean(recall)\n",
        "    test_mac_auc = roc_auc_score(y_true, y_scores,average='macro')\n",
        "    test_mic_auc = roc_auc_score(y_true, y_scores,average='micro')\n",
        "    test_mac_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    test_mic_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "    return test_loss, test_recall, test_mac_auc, test_mic_auc, test_mac_f1, test_mic_f1\n",
        "\n",
        "print('Test LSTM baseline')\n",
        "lstm_loss, lstm_recall, lstm_mac_auc, lstm_mic_auc, lstm_mac_f1, lstm_mic_f1 = testmodel(basemodel, 0)\n",
        "print('Test KSI+LSTM')\n",
        "lstmKSI_loss, lstmKSI_recall, lstmKSI_mac_auc, lstmKSI_mic_auc, lstmKSI_mac_f1, lstmKSI_mic_f1 = testmodel(KSImodel, 1)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "yyTdmPHX4CHd",
        "outputId": "b94da354-6282-476e-d950-bd7d9b5065ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Model      Loss  Recall@10  Macro_AUC  Micro_AUC  Macro_F1  \\\n",
              "0  LSTM Baseline  0.034217   0.768321   0.843087   0.970367  0.207647   \n",
              "1     LSTM + KSI  0.033033   0.778417   0.858165   0.972597  0.225144   \n",
              "\n",
              "   Micro_F1  \n",
              "0  0.646738  \n",
              "1  0.648586  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-beac8a66-2836-4f81-a807-d365afb976d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Recall@10</th>\n",
              "      <th>Macro_AUC</th>\n",
              "      <th>Micro_AUC</th>\n",
              "      <th>Macro_F1</th>\n",
              "      <th>Micro_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>0.034217</td>\n",
              "      <td>0.768321</td>\n",
              "      <td>0.843087</td>\n",
              "      <td>0.970367</td>\n",
              "      <td>0.207647</td>\n",
              "      <td>0.646738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM + KSI</td>\n",
              "      <td>0.033033</td>\n",
              "      <td>0.778417</td>\n",
              "      <td>0.858165</td>\n",
              "      <td>0.972597</td>\n",
              "      <td>0.225144</td>\n",
              "      <td>0.648586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-beac8a66-2836-4f81-a807-d365afb976d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-beac8a66-2836-4f81-a807-d365afb976d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-beac8a66-2836-4f81-a807-d365afb976d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "result_lstm = [['LSTM Baseline', \n",
        "                lstm_loss, \n",
        "                lstm_recall, \n",
        "                lstm_mac_auc, \n",
        "                lstm_mic_auc, \n",
        "                lstm_mac_f1, \n",
        "                lstm_mic_f1], \n",
        "        ['LSTM + KSI', \n",
        "         lstmKSI_loss, \n",
        "         lstmKSI_recall, \n",
        "         lstmKSI_mac_auc, \n",
        "         lstmKSI_mic_auc, \n",
        "         lstmKSI_mac_f1, \n",
        "         lstmKSI_mic_f1]]\n",
        "df_lstm = pd.DataFrame(result_lstm, columns=['Model', 'Loss', 'Recall@10', 'Macro_AUC', 'Micro_AUC', 'Macro_F1', 'Micro_F1'])\n",
        "df_lstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBdrr6p6Qd3"
      },
      "source": [
        "# LSTMatt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AI_2DjA6XNX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEiXQwOW6ZYH",
        "outputId": "6e5c3eda-d27a-48f0-ebbf-7de30ad6f05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 55s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_label=np.load('ix_to_label.npy', allow_pickle=True)\n",
        "training_data=np.load('training_data.npy', allow_pickle=True)\n",
        "test_data=np.load('test_data.npy', allow_pickle=True)\n",
        "val_data=np.load('val_data.npy', allow_pickle=True)\n",
        "word_to_ix=np.load('word_to_ix.npy', allow_pickle=True).item()\n",
        "ix_to_word=np.load('ix_to_word.npy', allow_pickle=True)\n",
        "newwikivec=np.load('newwikivec.npy', allow_pickle=True)\n",
        "wikivoc=np.load('wikivoc.npy', allow_pickle=True).item()\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data, dtype=object)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWX-Pan9AyTt",
        "outputId": "6bb00879-d306-455e-a619-b9676b2056e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time duration: 0h 00m 00s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "class LSTMattn(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTMattn, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.H=nn.Linear(hidden_dim, tagset_size )  \n",
        "        self.final = nn.Linear(hidden_dim, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            thisembeddings, self.hidden)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,1)\n",
        "\n",
        "        alpha=self.H.weight.matmul(lstm_out.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(lstm_out)\n",
        "        \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "                \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_vuC0SQAyMX",
        "outputId": "ef6ad544-b348-4940-f735-221f0621d9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train basemodel\n",
            "epoch = 0\n",
            "validation recall @ top-- 10 0.3796880833045024\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top-- 10 0.5111196937177896\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top-- 10 0.604538903363407\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top-- 10 0.6585091762268965\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top-- 10 0.6937992263722687\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top-- 10 0.7130799462899884\n",
            "Update the best model to epoch 5\n",
            "epoch = 6\n",
            "validation recall @ top-- 10 0.7296098975521876\n",
            "Update the best model to epoch 6\n",
            "epoch = 7\n",
            "validation recall @ top-- 10 0.7457491858711384\n",
            "Update the best model to epoch 7\n",
            "epoch = 8\n",
            "validation recall @ top-- 10 0.7620013719496009\n",
            "Update the best model to epoch 8\n",
            "epoch = 9\n",
            "validation recall @ top-- 10 0.7701372446019467\n",
            "Update the best model to epoch 9\n",
            "epoch = 10\n",
            "validation recall @ top-- 10 0.778430630381561\n",
            "Update the best model to epoch 10\n",
            "epoch = 11\n",
            "validation recall @ top-- 10 0.7804202547898998\n",
            "Update the best model to epoch 11\n",
            "epoch = 12\n",
            "validation recall @ top-- 10 0.7850564710383465\n",
            "Update the best model to epoch 12\n",
            "epoch = 13\n",
            "validation recall @ top-- 10 0.7864420004150514\n",
            "Update the best model to epoch 13\n",
            "epoch = 14\n",
            "validation recall @ top-- 10 0.7900108629937936\n",
            "Update the best model to epoch 14\n",
            "epoch = 15\n",
            "validation recall @ top-- 10 0.7900272465233112\n",
            "Update the best model to epoch 15\n",
            "epoch = 16\n",
            "validation recall @ top-- 10 0.7918326733976341\n",
            "Update the best model to epoch 16\n",
            "epoch = 17\n",
            "validation recall @ top-- 10 0.7931252066043157\n",
            "Update the best model to epoch 17\n",
            "epoch = 18\n",
            "validation recall @ top-- 10 0.7935986229750336\n",
            "Update the best model to epoch 18\n",
            "epoch = 19\n",
            "validation recall @ top-- 10 0.7948881945051319\n",
            "Update the best model to epoch 19\n",
            "epoch = 20\n",
            "validation recall @ top-- 10 0.7953602846451403\n",
            "Update the best model to epoch 20\n",
            "epoch = 21\n",
            "validation recall @ top-- 10 0.7961186582419538\n",
            "Update the best model to epoch 21\n",
            "epoch = 22\n",
            "validation recall @ top-- 10 0.7946441115180207\n",
            "epoch = 23\n",
            "validation recall @ top-- 10 0.7949140110447112\n",
            "epoch = 24\n",
            "validation recall @ top-- 10 0.7946551655180857\n",
            "epoch = 25\n",
            "validation recall @ top-- 10 0.7934045894632463\n",
            "epoch = 26\n",
            "validation recall @ top-- 10 0.7932616669790631\n",
            "[0.3796880833045024, 0.5111196937177896, 0.604538903363407, 0.6585091762268965, 0.6937992263722687, 0.7130799462899884, 0.7296098975521876, 0.7457491858711384, 0.7620013719496009, 0.7701372446019467, 0.778430630381561, 0.7804202547898998, 0.7850564710383465, 0.7864420004150514, 0.7900108629937936, 0.7900272465233112, 0.7918326733976341, 0.7931252066043157, 0.7935986229750336, 0.7948881945051319, 0.7953602846451403, 0.7961186582419538, 0.7946441115180207, 0.7949140110447112, 0.7946551655180857, 0.7934045894632463, 0.7932616669790631] 21\n",
            "\n",
            "Train model with KSI\n",
            "epoch = 0\n",
            "validation recall @ top-- 10 0.7969995360392447\n",
            "Update the best model to epoch 0\n",
            "epoch = 1\n",
            "validation recall @ top-- 10 0.7982239597821796\n",
            "Update the best model to epoch 1\n",
            "epoch = 2\n",
            "validation recall @ top-- 10 0.7984568927781526\n",
            "Update the best model to epoch 2\n",
            "epoch = 3\n",
            "validation recall @ top-- 10 0.7989728914105212\n",
            "Update the best model to epoch 3\n",
            "epoch = 4\n",
            "validation recall @ top-- 10 0.800439581900889\n",
            "Update the best model to epoch 4\n",
            "epoch = 5\n",
            "validation recall @ top-- 10 0.7994322689439063\n",
            "epoch = 6\n",
            "validation recall @ top-- 10 0.7991602585205418\n",
            "epoch = 7\n",
            "validation recall @ top-- 10 0.8002308758136559\n",
            "epoch = 8\n",
            "validation recall @ top-- 10 0.7995185061674258\n",
            "epoch = 9\n",
            "validation recall @ top-- 10 0.7986463284041134\n",
            "[0.7969995360392447, 0.7982239597821796, 0.7984568927781526, 0.7989728914105212, 0.800439581900889, 0.7994322689439063, 0.7991602585205418, 0.8002308758136559, 0.7995185061674258, 0.7986463284041134] 4\n",
            "Time duration: 0h 43m 59s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk = 10\n",
        "max_epochs = 5000 # Default is 5000\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(max_epochs):  \n",
        "       \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (f\"epoch = {epoch}\")\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "            model.hidden = model.init_hidden()\n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation recall @ top--',topk, np.mean(recall))\n",
        "                \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "            print(f\"Update the best model to epoch {bestiter}\")\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "    print(f\"Reach the max epochs, return the best model at epoch {bestiter}\")\n",
        "    return modelsaved[bestiter]\n",
        "    \n",
        "model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"Train basemodel\")\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'LSTMattn_model')\n",
        "\n",
        "model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "print(\"\")\n",
        "print(\"Train model with KSI\")\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_LSTMattn_model')\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K38RNna0AyC8",
        "outputId": "17a63684-cebf-4598-9d6b-ca55cacb6fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test LSTMatt baseline\n",
            "Test KSI+LSTMatt\n",
            "Time duration: 0h 00m 34s\n"
          ]
        }
      ],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "######################################################################\n",
        "# Test the model:\n",
        "\n",
        "def testmodel(modelstate, sim):\n",
        "    model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "        model.hidden = model.init_hidden()\n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        lossestest.append(loss.data.cpu().mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(int)\n",
        "    # print ('test loss', np.mean(lossestest))\n",
        "    # print ('top-',topk, np.mean(recall))\n",
        "    # print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    # print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    # print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    # print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "    test_loss = np.mean(lossestest)\n",
        "    test_recall = np.mean(recall)\n",
        "    test_mac_auc = roc_auc_score(y_true, y_scores,average='macro')\n",
        "    test_mic_auc = roc_auc_score(y_true, y_scores,average='micro')\n",
        "    test_mac_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    test_mic_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "    return test_loss, test_recall, test_mac_auc, test_mic_auc, test_mac_f1, test_mic_f1\n",
        "\n",
        "print('Test LSTMatt baseline')\n",
        "lstmatt_loss, lstmatt_recall, lstmatt_mac_auc, lstmatt_mic_auc, lstmatt_mac_f1, lstmatt_mic_f1 = testmodel(basemodel, 0)\n",
        "print('Test KSI+LSTMatt')\n",
        "lstmattKSI_loss, lstmattKSI_recall, lstmattKSI_mac_auc, lstmattKSI_mic_auc, lstmattKSI_mac_f1, lstmattKSI_mic_f1 = testmodel(KSImodel, 1)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64QNeZ_n5Qzo",
        "outputId": "07ee718a-b64f-49a8-d933-379a04bdc0a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Model      Loss  Recall@10  Macro_AUC  Micro_AUC  \\\n",
              "0  LSTM Attention Baseline  0.033961   0.793260   0.843429   0.974453   \n",
              "1     LSTM Attention + KSI  0.032939   0.796518   0.865558   0.975679   \n",
              "\n",
              "   Macro_F1  Micro_F1  \n",
              "0  0.249528  0.650250  \n",
              "1  0.258881  0.653255  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dc17f3d-9aad-4642-9801-05fe5d28c6e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Recall@10</th>\n",
              "      <th>Macro_AUC</th>\n",
              "      <th>Micro_AUC</th>\n",
              "      <th>Macro_F1</th>\n",
              "      <th>Micro_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Attention Baseline</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.793260</td>\n",
              "      <td>0.843429</td>\n",
              "      <td>0.974453</td>\n",
              "      <td>0.249528</td>\n",
              "      <td>0.650250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM Attention + KSI</td>\n",
              "      <td>0.032939</td>\n",
              "      <td>0.796518</td>\n",
              "      <td>0.865558</td>\n",
              "      <td>0.975679</td>\n",
              "      <td>0.258881</td>\n",
              "      <td>0.653255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dc17f3d-9aad-4642-9801-05fe5d28c6e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8dc17f3d-9aad-4642-9801-05fe5d28c6e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8dc17f3d-9aad-4642-9801-05fe5d28c6e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "result_lstmatt = [['LSTM Attention Baseline', \n",
        "                   lstmatt_loss, \n",
        "                   lstmatt_recall, \n",
        "                   lstmatt_mac_auc, \n",
        "                   lstmatt_mic_auc, \n",
        "                   lstmatt_mac_f1, \n",
        "                   lstmatt_mic_f1], \n",
        "        ['LSTM Attention + KSI', \n",
        "         lstmattKSI_loss, \n",
        "         lstmattKSI_recall, \n",
        "         lstmattKSI_mac_auc, \n",
        "         lstmattKSI_mic_auc, \n",
        "         lstmattKSI_mac_f1, \n",
        "         lstmattKSI_mic_f1]]\n",
        "df_lstmatt = pd.DataFrame(result_lstmatt, columns=['Model', 'Loss', 'Recall@10', 'Macro_AUC', 'Micro_AUC', 'Macro_F1', 'Micro_F1'])\n",
        "df_lstmatt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1UJ2A35_B6"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4hGiRvN6CJy",
        "outputId": "ca213293-042f-4aaa-ca38-ba1baea53322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Model      Loss  Recall@10  Macro_AUC  Micro_AUC  \\\n",
              "0            CAML Baseline  0.033520   0.807766   0.852964   0.978119   \n",
              "1               CAML + KSI  0.033304   0.808454   0.853904   0.978277   \n",
              "0             CNN Baseline  0.038858   0.754856   0.831774   0.967348   \n",
              "1                CNN + KSI  0.038051   0.765813   0.847441   0.970184   \n",
              "0            LSTM Baseline  0.034217   0.768321   0.843087   0.970367   \n",
              "1               LSTM + KSI  0.033033   0.778417   0.858165   0.972597   \n",
              "0  LSTM Attention Baseline  0.033961   0.793260   0.843429   0.974453   \n",
              "1     LSTM Attention + KSI  0.032939   0.796518   0.865558   0.975679   \n",
              "\n",
              "   Macro_F1  Micro_F1  \n",
              "0  0.278527  0.656805  \n",
              "1  0.279034  0.657468  \n",
              "0  0.214947  0.627527  \n",
              "1  0.226481  0.635694  \n",
              "0  0.207647  0.646738  \n",
              "1  0.225144  0.648586  \n",
              "0  0.249528  0.650250  \n",
              "1  0.258881  0.653255  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e67c039e-6d11-4100-9e56-eafc35764cc6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Recall@10</th>\n",
              "      <th>Macro_AUC</th>\n",
              "      <th>Micro_AUC</th>\n",
              "      <th>Macro_F1</th>\n",
              "      <th>Micro_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CAML Baseline</td>\n",
              "      <td>0.033520</td>\n",
              "      <td>0.807766</td>\n",
              "      <td>0.852964</td>\n",
              "      <td>0.978119</td>\n",
              "      <td>0.278527</td>\n",
              "      <td>0.656805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CAML + KSI</td>\n",
              "      <td>0.033304</td>\n",
              "      <td>0.808454</td>\n",
              "      <td>0.853904</td>\n",
              "      <td>0.978277</td>\n",
              "      <td>0.279034</td>\n",
              "      <td>0.657468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CNN Baseline</td>\n",
              "      <td>0.038858</td>\n",
              "      <td>0.754856</td>\n",
              "      <td>0.831774</td>\n",
              "      <td>0.967348</td>\n",
              "      <td>0.214947</td>\n",
              "      <td>0.627527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CNN + KSI</td>\n",
              "      <td>0.038051</td>\n",
              "      <td>0.765813</td>\n",
              "      <td>0.847441</td>\n",
              "      <td>0.970184</td>\n",
              "      <td>0.226481</td>\n",
              "      <td>0.635694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>0.034217</td>\n",
              "      <td>0.768321</td>\n",
              "      <td>0.843087</td>\n",
              "      <td>0.970367</td>\n",
              "      <td>0.207647</td>\n",
              "      <td>0.646738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM + KSI</td>\n",
              "      <td>0.033033</td>\n",
              "      <td>0.778417</td>\n",
              "      <td>0.858165</td>\n",
              "      <td>0.972597</td>\n",
              "      <td>0.225144</td>\n",
              "      <td>0.648586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Attention Baseline</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.793260</td>\n",
              "      <td>0.843429</td>\n",
              "      <td>0.974453</td>\n",
              "      <td>0.249528</td>\n",
              "      <td>0.650250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM Attention + KSI</td>\n",
              "      <td>0.032939</td>\n",
              "      <td>0.796518</td>\n",
              "      <td>0.865558</td>\n",
              "      <td>0.975679</td>\n",
              "      <td>0.258881</td>\n",
              "      <td>0.653255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e67c039e-6d11-4100-9e56-eafc35764cc6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e67c039e-6d11-4100-9e56-eafc35764cc6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e67c039e-6d11-4100-9e56-eafc35764cc6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "comparison_df = pd.concat([df_caml, df_cnn, df_lstm, df_lstmatt])\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhjFyQnkSR_7",
        "outputId": "7ebe8f2a-6561-462b-d3a3-dfe2ddf94c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time duration: 2h 43m 28s\n"
          ]
        }
      ],
      "source": [
        "total_stop = timeit.default_timer()\n",
        "total_duration = str(datetime.timedelta(seconds=round(total_stop - total_start)))\n",
        "total_duration = total_duration.split(\":\")\n",
        "print(f'Total time duration: {total_duration[0]}h {total_duration[1]}m {total_duration[2]}s')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6qaHQaTmJpV"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}