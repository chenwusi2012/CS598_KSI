{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Summary"
      ],
      "metadata": {
        "id": "N0vQiW_aeU3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is to investigate the performance of the standalone document similarity learning model in the task of the medical code prediction from the clinical notes. \n",
        "\n",
        "This is the ablation study of the reproduction study of the following paper:  \n",
        "\n",
        "Tian Bai and Slobodan Vucetic. 2019. Improving medical code prediction from clinical text via incorporating online knowledge sources. In The World WideWeb Conference, WWW ’19, page 72–82, New York, NY, USA. Association for Computing Machinery."
      ],
      "metadata": {
        "id": "qNL1NawyfsaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Preparation"
      ],
      "metadata": {
        "id": "seM7lyCOn_g1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfbLDAbD-Ho9"
      },
      "source": [
        "## 2.1 Check GPU Status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0kJAKAWc_4d"
      },
      "source": [
        "This notebook requires hardware acceleration with GPU. Run the following code to make sure a GPU is running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PysjRxy7Ue3a",
        "outputId": "0cbacc6b-6a08-42aa-ae02-e09a6daa384c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 28 04:21:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTpL8Y_PYIVf"
      },
      "source": [
        "## 2.2 Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sraXyzOFZFDJ"
      },
      "source": [
        "In this section, the following files are loaded:\n",
        "\n",
        "* `NOTEEVENTS.csv`\n",
        "* `DIAGNOSES_ICD.csv`\n",
        "* `wikipedia_knowledge`\n",
        "* `IDlist.npy`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnmtN942Zvsl"
      },
      "source": [
        "To load `NOTEEVENTS.csv` and `DIAGNOSES_ICD.csv`, take and pass the training \"CITI Data or Specimens Only Research\" at [https://about.citiprogram.org/](https://about.citiprogram.org/), and apply for the access to the MIMIC-III Clinical Database at PhysioNet at [https://physionet.org/content/mimiciii/1.4/](https://physionet.org/content/mimiciii/1.4/). After gaining the access, download `NOTEEVENTS.csv` and `DIAGNOSES_ICD.csv` from PhysioNet, and upload these two CSV files to a created folder \"cs598_project\" in Google drive.\n",
        "  \n",
        "Mount the Google Drive to Google Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "houR55vShhQE",
        "outputId": "f889153d-cee3-4a2d-a108-96ceaeefa3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l95OzOa5aToH"
      },
      "source": [
        "Display the files in the \"cs598_project\" folder. Make sure `NOTEEVENTS.csv` and `DIAGNOSES_ICD.csv` are uploaded to this folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ePN8cqnMog"
      },
      "outputs": [],
      "source": [
        "!ls drive/MyDrive/cs598_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY0S-SeNammD"
      },
      "source": [
        "Copy `NOTEEVENTS.csv` and `DIAGNOSES_ICD.csv` from Google Drive to Google Colab Runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnsInD8unfwD"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/cs598_project/DIAGNOSES_ICD.csv DIAGNOSES_ICD.csv\n",
        "!cp drive/MyDrive/cs598_project/NOTEEVENTS.csv NOTEEVENTS.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV4EY3L_a7dJ"
      },
      "source": [
        "`wikipedia_knowledge` and `IDlist.npy` can be downloaded from the GitHub Repository of the original paper ([https://github.com/tiantiantu/KSI](https://github.com/tiantiantu/KSI))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61F2nkrrNlfT"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tiantiantu/KSI/master/wikipedia_knowledge\n",
        "!wget https://github.com/tiantiantu/KSI/raw/master/IDlist.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Load Python Files"
      ],
      "metadata": {
        "id": "OKph7reEm6au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`preprocessing.py`, `training.py`, and `testing.py` contains the functions for data preprocessing, training, and testing of the deep learning models. They can be downloaded from the GitHub repository of this reproduction study ([https://github.com/chenwusi2012/CS598_KSI]())."
      ],
      "metadata": {
        "id": "EDStu2ARKHhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/chenwusi2012/CS598_KSI/main/preprocessing.py\n",
        "!wget https://raw.githubusercontent.com/chenwusi2012/CS598_KSI/main/training.py\n",
        "!wget https://raw.githubusercontent.com/chenwusi2012/CS598_KSI/main/testing.py"
      ],
      "metadata": {
        "id": "c5BsxpoXnTlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Check Loaded Files"
      ],
      "metadata": {
        "id": "LOvDPBiz2L6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the required files for this notebook have been loaded to the directory."
      ],
      "metadata": {
        "id": "15-MOjAp4MsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "assert os.path.exists('NOTEEVENTS.csv'), \"NOTEEVENTS.csv is not in the directory\"\n",
        "assert os.path.exists('DIAGNOSES_ICD.csv'), \"DIAGNOSES_ICD.csv is not in the directory\"\n",
        "assert os.path.exists('wikipedia_knowledge'), \"wikipedia_knowledge is not in the directory\"\n",
        "assert os.path.exists('IDlist.npy'), \"IDlist.npy is not in the directory\"\n",
        "assert os.path.exists('preprocessing.py'), \"preprocessing.py is not in the directory\"\n",
        "assert os.path.exists('training.py'), \"training.py is not in the directory\"\n",
        "assert os.path.exists('testing.py'), \"testing.py is not in the directory\""
      ],
      "metadata": {
        "id": "eDKtn0x-2UnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSFxY46aet2F"
      },
      "source": [
        "## 2.5 Running Time Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EebpslhDelbO"
      },
      "source": [
        "Import timeit and datetime to track the running time of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1HPfLA60Nc"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "import datetime\n",
        "total_start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Data Pre-processing"
      ],
      "metadata": {
        "id": "hs7LLN-goPiw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlWdp-4V-aqS"
      },
      "source": [
        "## 3.1 Data Pre-processing 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6t9c0y4ebaW"
      },
      "source": [
        "Install `stop-words` to filter out stop words in the clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHBorULhpNXR"
      },
      "outputs": [],
      "source": [
        "!pip install stop-words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed Python packages for this section."
      ],
      "metadata": {
        "id": "wKtUOzKfhFT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhl6WfZAoeUD"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import string\n",
        "from stop_words import get_stop_words    # download stop words package from https://pypi.org/project/stop-words/\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the start timestamp of this section to track the total running time of this section."
      ],
      "metadata": {
        "id": "RZv1K2xzgu83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjxnwUaaGg1i"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kk0iqHOBo4S"
      },
      "source": [
        "Create a dictionary from `NOTEEVENTS.csv`. The key is `HADM_ID` (ID of a visit), and the value is clinical note.  \n",
        "For clinical note, replace line change with whitespace, remove punctuation, and lowercase all letters.  \n",
        "From paper: \"During preprocessing we lowercased all\n",
        "tokens and removed punctuations, stop words, words containing\n",
        "only digits, and words whose frequency is less than 10\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVfixu8AVr2"
      },
      "outputs": [],
      "source": [
        "stop_words = get_stop_words('english')\n",
        "\n",
        "admidic=defaultdict(list)\n",
        "num_of_notes=0\n",
        "\n",
        "with open('NOTEEVENTS.csv', 'r') as csvfile:\n",
        "  spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "  for row in spamreader: # Iterate all entries in NOTEEVETS\n",
        "    if row[6]=='Discharge summary': # Check if the category is discharge summary\n",
        "      # identify patients through subject_id\n",
        "      # append text to the list of text for a HADM_ID (visit) \n",
        "      # lower case all words, and remove punctuation\n",
        "      admidic[row[2]].append(row[-1].replace('\\n',' ').translate(str.maketrans('','',string.punctuation)).lower())\n",
        "      num_of_notes = num_of_notes+1 # count number of discharge summary notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow1MYjANCmhO"
      },
      "source": [
        "Print the number of notes in `NOTEEVENTS.csv` (From paper: \"The total number of\n",
        "discharge summary notes is 59,652.\").  \n",
        "Print a sample key-value pair (the clinical note for visit 167853)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS_gtxj1Ahrv"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of notes in NOTEEVENTS.csv = {num_of_notes}\")\n",
        "print(admidic['167853'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-cDy3beDLjk"
      },
      "source": [
        "Calculate the word ocurrence (including stop words) in `NOTEEVENTS.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHvu1UZbDc_Z"
      },
      "outputs": [],
      "source": [
        "u=defaultdict(int) # The default count for each word is 0\n",
        "for i in admidic: # for each visit\n",
        "  for jj in admidic[i]: # for each saved note\n",
        "    line=jj.strip('\\n').split() # split into a list of words\n",
        "    for j in line: # Iterate each word\n",
        "      u[j]=u[j]+1 # count the number of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hfrS-9ENB6"
      },
      "source": [
        "Print the number of words (vocabulary) in `NOTEEVENTS.csv`.  \n",
        "Print the ocurrence of the word \"consistent\" in `NOTEEVENTS.csv`.  \n",
        "Check if the stop word \"the\" in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvhIGKugD0rb"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of words in NOTEEVENTS.csv = {len(u)}\")\n",
        "print(u[\"consistent\"])\n",
        "print(\"the\" in u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OiaRFg8E3Ep"
      },
      "source": [
        "Remove the stopwords.  \n",
        "Remove the words whose number of occurence is less than 10.  \n",
        "From paper: \"During preprocessing we lowercased all\n",
        "tokens and removed punctuations, stop words, words containing\n",
        "only digits, and words whose frequency is less than 10\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhdazHeWFFBg"
      },
      "outputs": [],
      "source": [
        "u2=defaultdict(int) # Create a new dict to filter out some words\n",
        "for i in u: # iterate each word\n",
        "  if i.isdigit()==False: # Make sure not a number\n",
        "    if u[i]>10: # Make sure the word occurence is higher than 10\n",
        "      if i not in stop_words: # Make sure not stop words\n",
        "        u2[i]=u[i]\n",
        "num_of_words_in_notes = len(u2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3WioX6eFW3C"
      },
      "source": [
        "Print the number of words (vocabulary) in `NOTEEVENTS.csv` AFTER the removal of stopwords (From paper: \"The final\n",
        "word vocabulary contains 47,965 unique words.\").  \n",
        "Check if the stop word \"the\" in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTGa6IooFTBV"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of words in NOTEEVENTS.csv = {num_of_words_in_notes}\")\n",
        "print(\"the\" in u2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIe0nf07Ij7S"
      },
      "source": [
        "Create a dictionary for `DIAGNOSES_ICD.csv`. The key is `HADM_ID`, and the value if a list of ICD-9 codes for `HADM_ID`.  \n",
        "Add a prefix \"d_\" to the ICD-9 codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkbht_bfHum5"
      },
      "outputs": [],
      "source": [
        "u=[]   \n",
        "\n",
        "file1=codecs.open('DIAGNOSES_ICD.csv','r')\n",
        "ad2c=defaultdict(list)\n",
        "line=file1.readline() # Skip the 1st line\n",
        "line=file1.readline() # Read the 2nd line\n",
        "\n",
        "while line:\n",
        "  line=line.strip().split(',') # Split a row into a list\n",
        "\n",
        "  if line[4][1:-1]!='': # If ICD9_CODE column is not empty\n",
        "    ad2c[line[2]].append(\"d_\"+line[4][1:-1]) # Append the code to list of codes for a HADM_ID\n",
        "  \n",
        "  line=file1.readline() # Read the next line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98bQUxEJGjl"
      },
      "source": [
        "Print a sample key-value pair (ICD-9 codes for visit 172335)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TygSO_osJPFP"
      },
      "outputs": [],
      "source": [
        "print(ad2c[\"172335\"])\n",
        "print(len(ad2c[\"172335\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUuWJPrmKRhv"
      },
      "source": [
        "Calculate the code ocurrence in `DIAGNOSES_ICD.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFsluq1iKNpB"
      },
      "outputs": [],
      "source": [
        "codeu=defaultdict(int)\n",
        "for i in ad2c:\n",
        "  for j in ad2c[i]:\n",
        "    codeu[j]=codeu[j]+1 # counter the occurence of codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GybgwUEBK2DI"
      },
      "source": [
        "Print the number of unique ICD-9 codes (original codes) in `DIAGNOSES_ICD.csv`.  \n",
        "Print the number of occurence for a code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4K06NaIKkdn"
      },
      "outputs": [],
      "source": [
        "print(f\"number of codes in DIAGNOSES_ICD.csv = {len(codeu)}\")\n",
        "print(codeu[\"d_486\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO0Dyqtc_aaa"
      },
      "source": [
        "Group ICD-9 codes in `DIAGNOSES_ICD.csv` by the first 3 letters (From paper: \"We extracted all\n",
        "listed ICD-9 diagnosis codes for each visit and grouped them by\n",
        "their first three digits\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j6BW_VA9M7j"
      },
      "outputs": [],
      "source": [
        "ad2c2 = defaultdict(list)\n",
        "for hadm_id in ad2c:\n",
        "  for code in ad2c[hadm_id]:\n",
        "    if code[0:5] not in ad2c2[hadm_id]:\n",
        "      ad2c2[hadm_id].append(code[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTpz739_mn6"
      },
      "source": [
        "Print a sample key-value pair (same as the one before ICD-9 code grouping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cScwSxfM_GCb"
      },
      "outputs": [],
      "source": [
        "print(ad2c2[\"172335\"])\n",
        "print(len(ad2c2[\"172335\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGWvVPCw_xrn"
      },
      "source": [
        "Calculate the code ocurrence in DIAGNOSES_ICD.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR1NS10fQs6z"
      },
      "outputs": [],
      "source": [
        "codeu2=defaultdict(int)\n",
        "for hadm_id in ad2c2:\n",
        "  for code in ad2c2[hadm_id]:\n",
        "    codeu2[code]=codeu2[code]+1 # counter the occurence of codes\n",
        "num_of_codes_in_notes = len(codeu2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa_vMKj0AEOb"
      },
      "source": [
        "Print the number of unique ICD-9 codes (after grouping) in `DIAGNOSES_ICD.csv` (From paper: \"The code vocabulary contains 942 codes.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bKVTDc_Q5W"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of codes after grouping = {num_of_codes_in_notes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCjfDnUNifD"
      },
      "source": [
        "Iterate `HADM_ID` in `IDlist.npy`, and combine the code data (from `DIAGNOSES_ICD.csv`) and note data (from `NOTEEVENTS.csv`) into a single file `combined_dataset`.  \n",
        "`combined_dataset`: A dataset contains the clinical notes of hospital visits and corresponding diagnosis (ICD-9 codes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31lmsgy23wPv"
      },
      "outputs": [],
      "source": [
        "fileo=codecs.open(\"combined_dataset\",'w')\n",
        "\n",
        "num_of_aggregated_notes = 0\n",
        "lower_limit_freq = 0\n",
        "upper_limit_freq = 10000000\n",
        "\n",
        "IDlist=np.load('IDlist.npy',encoding='bytes').astype(str) # a list of HADM_ID\n",
        "for hadm_id in IDlist:\n",
        "  if ad2c2[hadm_id]!=[]: # If HADM_ID exists in ad2c\n",
        "    add_to_set = False\n",
        "    for code in ad2c2[hadm_id]:\n",
        "      if codeu2[code] >= lower_limit_freq and codeu2[code] <= upper_limit_freq:\n",
        "        add_to_set = True\n",
        "    if add_to_set:\n",
        "      fileo.write('start! '+i+'\\n')\n",
        "      fileo.write('codes: ')\n",
        "      tempc=[]\n",
        "      for code in ad2c2[hadm_id]: # for each code\n",
        "        if codeu2[code] >= lower_limit_freq and codeu2[code] <= upper_limit_freq: # if code occurence greater than threshold\n",
        "          if code not in tempc:\n",
        "            tempc.append(code) # save d_ and first 3 digits of ICD-9 code\n",
        "      \n",
        "      for code in tempc:\n",
        "        fileo.write(code+\" \") # write code to combined dataset\n",
        "      fileo.write('\\n')\n",
        "      fileo.write('notes:\\n') # write note\n",
        "      for line in admidic[hadm_id]: # iterate each line    \n",
        "        thisline=line.strip('\\n').split() \n",
        "        for j in thisline: # iterate each word\n",
        "          if u2[j]!=0: # if this word is a qualified word\n",
        "            fileo.write(j+\" \") # write this word\n",
        "          fileo.write('\\n')\n",
        "      fileo.write('end!\\n')\n",
        "      num_of_aggregated_notes = num_of_aggregated_notes + 1\n",
        "fileo.close()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olb3AGDhEI-x"
      },
      "source": [
        "Print the number of note-code pairs added to the dataset (From paper:\"The number of aggregated discharge summary notes is 52,722..\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhzMBfP7D_qe"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of notes written to combined dataset = {num_of_aggregated_notes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the total running time of this section."
      ],
      "metadata": {
        "id": "ewGvvJBtg2aH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZDh7aDeD_fY"
      },
      "outputs": [],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COK-Hrv5-knl"
      },
      "source": [
        "## 3.2 Data Pre-processing 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed Python packages for this section."
      ],
      "metadata": {
        "id": "_5jp7zUZhIcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrbstc1758MT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the start timestamp of this section to track the total running time of this section."
      ],
      "metadata": {
        "id": "4yRFj1UOgxBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLfX0hlBGsBK"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2ORuVJIlnU"
      },
      "source": [
        "Build a vocabulary for Wiki documents. The key is a word, and the value is 1 if the word exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5kSmQc3ImLq"
      },
      "outputs": [],
      "source": [
        "wikivocab={}\n",
        "file1=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file1.readline()\n",
        "while line:\n",
        "  if line[0:3]!='XXX': # if this line is not start or end of doc\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    for i in line: # iterate each word\n",
        "      wikivocab[i.lower()]=1 # if a word exists in doc, set to 1\n",
        "  line=file1.readline()\n",
        "num_of_words_in_wiki = len(wikivocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxyybgFLJgNK"
      },
      "source": [
        "Print the size of the vocabulary for Wiki documents (From paper: \"The size\n",
        "of the word vocabulary of Wikipedia documents is 60,968.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4tCwHxXJDwi"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of unique words in Wiki documents = {num_of_words_in_wiki}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOo0A3-9KEAC"
      },
      "source": [
        "Build a vocabulary for `NOTEEVENTS.csv` (after word processing). The key is a word, and the value is 1 if the word exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In7y-or4KWE8"
      },
      "outputs": [],
      "source": [
        "notesvocab={}\n",
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  \n",
        "  if line[0]=='codes:':\n",
        "    line=filec.readline()\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    \n",
        "    if line[0]=='notes:': \n",
        "      line=filec.readline()\n",
        "      while line!='end!\\n':\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for word in line:\n",
        "          notesvocab[word]=1\n",
        "        line=filec.readline()            \n",
        "  line=filec.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPXbUzq6LcRR"
      },
      "source": [
        "Print the size of vocabulary for `NOTEEVENTS.csv` after word processing (From paper: \"The final word vocabulary contains 47,965 unique words.\").  \n",
        "Print a sample key-value pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtN8IgIZK_pY"
      },
      "outputs": [],
      "source": [
        "print(len(notesvocab))\n",
        "print(notesvocab[\"consistent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3VGW7dkL_PE"
      },
      "source": [
        "Get an intersection of the vocabulary for wiki documents and the one for `NOTEVENTS.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9JMMHhiMITq"
      },
      "outputs": [],
      "source": [
        "a1=set(notesvocab)\n",
        "a2=set(wikivocab)\n",
        "a3=a1.intersection(a2) # get the intersection\n",
        "num_of_words_in_both = len(a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ladA4oybMY73"
      },
      "source": [
        "Print number of unique words in the intersection (From paper: \"The size of the word vocabulary of Wikipedia documents is 60,968, out of which only 12,173 are also in the word vocabulary of MIMIC-III clinical notes.\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcJK3dukMeYS"
      },
      "outputs": [],
      "source": [
        "print(num_of_words_in_both)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZxImH0NdDf"
      },
      "source": [
        "Create a list of lists. Each element list is a list of intersected words for one Wiki document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHj2uu7SNaQn"
      },
      "outputs": [],
      "source": [
        "wikidocuments=[] # a list of lists, each element is a list of intersected and filtered words for one doc\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline() \n",
        "while line:\n",
        "  if line[0:4]=='XXXd': # Check if it is a start of a wiki doc\n",
        "    tempf=[]\n",
        "    line=file2.readline() # read the next line\n",
        "    while line[0:4]!='XXXe': # keep reading until the end of that doc\n",
        "      line=line.strip('\\n')\n",
        "      words=line.split()\n",
        "      for word in words:\n",
        "        if word.lower() in a3: # if this word also appears in note\n",
        "          tempf.append(word.lower()) # add this word to the list for this doc\n",
        "      line=file2.readline()\n",
        "    wikidocuments.append(tempf) # add word list of doc to list of docs \n",
        "  line=file2.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSeBdRaHOF3p"
      },
      "source": [
        "Print a sample element list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLfOCUqBN7o1"
      },
      "outputs": [],
      "source": [
        "print(wikidocuments[0])\n",
        "print(len(wikidocuments))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcK34TJLO9yM"
      },
      "source": [
        "Create a list of lists. Each element list is a list of intersected words for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjmPcDD6PCY1"
      },
      "outputs": [],
      "source": [
        "notesdocuments=[] # a list of lists, each element is a list of intersected and filtered words for one note\n",
        "file3=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "line=file3.readline()\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  if line[0]=='codes:': # if this line is for code\n",
        "    line=file3.readline() # skip this line\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    if line[0]=='notes:': # if this line is for note\n",
        "      tempf=[]\n",
        "      line=file3.readline()\n",
        "      while line!='end!\\n': # keep reading until the end of the note\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        for word in line:\n",
        "          if word in a3:\n",
        "            tempf.append(word)     \n",
        "        line=file3.readline()      \n",
        "      notesdocuments.append(tempf)\n",
        "  line=file3.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVRJqsmRN4Q"
      },
      "source": [
        "Print a sample element list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tcTvo8IRO4n"
      },
      "outputs": [],
      "source": [
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgP1wKEWSVV2"
      },
      "source": [
        "Set the sequence of words in the vocabulary matrix. Key is a word, and the value is the sequence/order in the vocabulary matrix.  \n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGwAnUihS5sY"
      },
      "outputs": [],
      "source": [
        "notesvocab={}\n",
        "for i in notesdocuments: # for each element (is a list) in list\n",
        "  for j in i: # for each word\n",
        "    if j.lower() not in notesvocab: # if a word is not in dict\n",
        "      # # set value of this word equal to current element (order of the token in matrix)\n",
        "      notesvocab[j.lower()]=len(notesvocab) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiUuPs8lTaow"
      },
      "source": [
        "Print sample key-value pairs (Compare with `notesdocuments[0]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4bUiVMrTd67"
      },
      "outputs": [],
      "source": [
        "print(notesvocab[\"admission\"])\n",
        "print(notesvocab[\"date\"])\n",
        "print(notesvocab[\"discharge\"])\n",
        "print(notesvocab[\"birth\"])\n",
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RKZthpkV8g1"
      },
      "source": [
        "Create a list of string for Wiki document, and each element is a string for a Wiki document (including intersected words).\n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWcFW-QEV63H"
      },
      "outputs": [],
      "source": [
        "wikidata=[] # each element is a string, each string contains words for a wiki doc\n",
        "for i in wikidocuments:\n",
        "  temp=''\n",
        "  for j in i:\n",
        "    temp=temp+j+\" \"\n",
        "  wikidata.append(temp)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0UU5z_OWHe2"
      },
      "source": [
        "Print a sample string (Compare with `wikidocuments[0]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fAfDRVWWK2u"
      },
      "outputs": [],
      "source": [
        "print(wikidata[0])\n",
        "print(wikidocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te_XDGn_VFlV"
      },
      "source": [
        "Create a list of string for clinical notes, and each element is a string for a note (including intersected words).  \n",
        "This is a preparation for building the matrices of the intersected words for Wiki documents and clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsxeKIhJVEI8"
      },
      "outputs": [],
      "source": [
        "notedata=[] # each element is a string, each string contains words for a note\n",
        "for i in notesdocuments: # for each element (list) in list\n",
        "  temp=''\n",
        "  for j in i: # for each word\n",
        "    temp=temp+j+\" \" # create a string, words for a note separated by a space\n",
        "  notedata.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRoisHIrVlYS"
      },
      "source": [
        "Print a sample string (Compare with `notesdocuments[0]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D9I-pHfVoOq"
      },
      "outputs": [],
      "source": [
        "print(notedata[0])\n",
        "print(notesdocuments[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-zZsHsfZETc"
      },
      "source": [
        "Create 2 word matrices (intersected words). One is for Wiki documents, and the other is for clinical notes.  \n",
        "`notevec`: A matrix of intersected words for clinical notes.  \n",
        "`wikivec`: A matrix of intersected words for Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBfUhADjZVha"
      },
      "outputs": [],
      "source": [
        "# create a matrix of token counts\n",
        "vect = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "# transfer list of string to matrix, if a word exists in a string, set value to 1\n",
        "binaryn = vect.fit_transform(notedata)\n",
        "binaryn=binaryn.A # Return self as an ndarray object.\n",
        "binaryn=np.array(binaryn,dtype=float)\n",
        "\n",
        "vect2 = CountVectorizer(min_df=1,vocabulary=notesvocab,binary=True)\n",
        "binaryk = vect2.fit_transform(wikidata)\n",
        "binaryk=binaryk.A\n",
        "binaryk=np.array(binaryk,dtype=float)\n",
        "notevec = binaryn\n",
        "wikivec = binaryk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDe_Hdt2Zy7s"
      },
      "source": [
        "Print the shape of the created matrices.  \n",
        "For the matrix for clinical notes, the size of 1st dimension is the number of notes, and the size of the 2nd dimension is the number of intersected words.  \n",
        "For the matrix for Wiki documents, the size of 1st dimension is the number of Wiki documents, and the size of the 2nd dimension is the number of intersected words.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alNBP-KwZ3L7"
      },
      "outputs": [],
      "source": [
        "print(f\"The shape of the matrix for clinical notes (notevec) = {binaryn.shape}\")\n",
        "print(f\"The shape of the matrix for wiki docs (wikivec) = {binaryk.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the total running time of this section."
      ],
      "metadata": {
        "id": "-Ku4DbKig4gD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Bc7NWJ6TuA"
      },
      "outputs": [],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA3PKA3o-zIW"
      },
      "source": [
        "## 3.3 Data Pre-processing 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed Python packages for this section."
      ],
      "metadata": {
        "id": "6pJLOFLqhJTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmglWv6s9Jhc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the start timestamp of this section to track the total running time of this section."
      ],
      "metadata": {
        "id": "rZAhvfuJgyVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMqsHWScDco"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dMkHxzFeEGD"
      },
      "source": [
        "Create 2 dictionaries for the ICD-9 codes for Wiki documents.  \n",
        "For the 1st dictionary, the key is a ICD-9 code which exists in Wiki documents, and the value is 1.\n",
        "For the 2nd dictionary, the key is a ICD-9 code which exists in Wiki documents, and the value is the list which contain the sequence number of Wiki documents for this ICD-9 code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNU5O0HNcLOB"
      },
      "outputs": [],
      "source": [
        "wikivoc={}\n",
        "codewiki=defaultdict(list)\n",
        "\n",
        "file2=codecs.open(\"wikipedia_knowledge\",'r','utf-8')\n",
        "line=file2.readline()\n",
        "count=0\n",
        "while line:\n",
        "  if line[0:4]=='XXXd': # read the start of a wiki doc\n",
        "    line=line.strip('\\n')\n",
        "    codes=line.split()\n",
        "    for code in codes:\n",
        "      if code[0:2]=='d_': # if it is a icd code\n",
        "        codewiki[code].append(count) # save the index of wiki doc to list for code\n",
        "        wikivoc[code]=1 # set value of code to 1\n",
        "    count=count+1\n",
        "  line=file2.readline()\n",
        "  num_of_codes_in_wiki = len(wikivoc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI6vsnX0gObv"
      },
      "source": [
        "Print the 2 dictionaries.  \n",
        "Print the number of ICD-9 codes in Wiki documents (**not all of them can be found in clinical notes**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TCFIlvqcQ4B"
      },
      "outputs": [],
      "source": [
        "print(wikivoc)\n",
        "print(codewiki)\n",
        "print(f\"Total number of codes = {num_of_codes_in_wiki}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXmyJz6chflJ"
      },
      "source": [
        "Each of the following 4 ICD-9 codes appears in 2 Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZR9TGFAhDM2"
      },
      "outputs": [],
      "source": [
        "print(codewiki['d_072'])\n",
        "print(codewiki['d_698'])\n",
        "print(codewiki['d_305'])\n",
        "print(codewiki['d_386'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXlRQZ4khr9V"
      },
      "source": [
        "For training purpose, each Wiki document can have more than one ICD-9 code, but each ICD-9 code can appear in only one Wiki document.  \n",
        "Correct the 4 ICD-9 codes above, and for each of these 4 codes, down-select one Wiki document.  \n",
        "`wikivoc`: A matrix of ICD-9 codes which appear in Wiki documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFSJFFcriWRC"
      },
      "outputs": [],
      "source": [
        "codewiki['d_072']=[214]\n",
        "codewiki['d_698']=[125]\n",
        "codewiki['d_305']=[250]\n",
        "codewiki['d_386']=[219]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJq3pVJOZHO"
      },
      "source": [
        "Prepare feature and label for the deep learning model.  \n",
        "Feature: A list of string. Each string is one clinical note.  \n",
        "Label: A list of string. Each string is the ICD-9 codes for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3pmjJZOTEX"
      },
      "outputs": [],
      "source": [
        "filec=codecs.open(\"combined_dataset\",'r','utf-8')\n",
        "\n",
        "line=filec.readline()\n",
        "\n",
        "feature=[]\n",
        "label=[]\n",
        "\n",
        "while line:\n",
        "  line=line.strip('\\n')\n",
        "  line=line.split()\n",
        "  \n",
        "  if line[0]=='codes:':\n",
        "    temp=line[1:] # read the codes of that node\n",
        "    label.append(temp) # add the code to list for label\n",
        "    line=filec.readline()\n",
        "    line=line.strip('\\n')\n",
        "    line=line.split()\n",
        "    if line[0]=='notes:':\n",
        "      tempf=[]\n",
        "      line=filec.readline()\n",
        "      \n",
        "      while line!='end!\\n': # read the notes until end\n",
        "        line=line.strip('\\n')\n",
        "        line=line.split()\n",
        "        tempf=tempf+line\n",
        "        line=filec.readline()\n",
        "      feature.append(tempf) # add list of words to list of feature\n",
        "  line=filec.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QN-HcplPzUM"
      },
      "source": [
        "Print a sample feature.  \n",
        "Print a sample label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UI622nTP3yu"
      },
      "outputs": [],
      "source": [
        "print(feature[0])\n",
        "print(label[0])\n",
        "print(len(feature[0]))\n",
        "print(len(label[0]))\n",
        "print(len(feature))\n",
        "print(len(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4la9dYtOTPLz"
      },
      "source": [
        "Create the sequence for label (ICD-9 codes). The key is a ICD-9 code, and the value is the sequence of a ICD-9 code in the code vector later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrzF4EYGTksY"
      },
      "outputs": [],
      "source": [
        "prevoc={}\n",
        "for i in label:\n",
        "  for j in i:\n",
        "    if j not in prevoc:\n",
        "      prevoc[j] = len(prevoc) # set up the order of codes (for vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skouqwX8TtHz"
      },
      "source": [
        "Print a sample key-value pairs (refer to `label[0]`).  \n",
        "Print the number of key-value pairs (codes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8zU8E3uTwvn"
      },
      "outputs": [],
      "source": [
        "print(prevoc[\"d_486\"])\n",
        "print(prevoc[\"d_518\"])\n",
        "print(prevoc[\"d_511\"])\n",
        "print(len(prevoc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvRlVNfjVgon"
      },
      "source": [
        "Create mapping between ICD-9 codes and the index in the code vector by 2 dictionaries.  \n",
        "**This mapping is for all codes found in the combined dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxSZbDm9Vq9N"
      },
      "outputs": [],
      "source": [
        "label_to_ix = {}\n",
        "ix_to_label = {}\n",
        "\n",
        "# create a mapping between code and index\n",
        "for codes in label:\n",
        "  for code in codes:\n",
        "    if code not in label_to_ix:\n",
        "      label_to_ix[code]=len(label_to_ix)\n",
        "      ix_to_label[label_to_ix[code]]=code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz3jD8rBWDYa"
      },
      "source": [
        "Print sample key-value pairs.  \n",
        "Print the number of ICD-9 codes found in combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk3ABC2ZWGfM"
      },
      "outputs": [],
      "source": [
        "print(label_to_ix[\"d_486\"])\n",
        "print(ix_to_label[0])\n",
        "print(f\"Total number of codes = {len(label_to_ix)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4JxLlXIfI8_"
      },
      "source": [
        "Create a word vector (intersected words) for each of the ICD-9 codes found in combined_dataset.\n",
        "*   If a ICD-9 code can be found in Wiki documents: label index -> ICD-9 code -> sequence/index of Wiki document -> vector of intersected words of Wikidocument (1 x number of intersected words).\n",
        "*   If ICD-9 code cannot be found in Wiki document: zero vector in shape of (1 x number of intersected words).\n",
        "Create a mapping between ICD-9 code index in label and corresponding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m7qfTVDeap9"
      },
      "outputs": [],
      "source": [
        "tempwikivec=[]\n",
        "\n",
        "for i in range(0,len(ix_to_label)):\n",
        "  if ix_to_label[i] in wikivoc: # if a code in note can be found in wiki docs\n",
        "    temp=wikivec[codewiki[ix_to_label[i]][0]] # save wiki doc index to temp\n",
        "    tempwikivec.append(temp)\n",
        "  else:\n",
        "    tempwikivec.append([0.0]*wikivec.shape[1])\n",
        "wikivec=np.array(tempwikivec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh67nu1Mi-XD"
      },
      "source": [
        "Print sample result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0sD4EbodGiT"
      },
      "outputs": [],
      "source": [
        "print(f\"If the ICD-9 code is = {ix_to_label[0]}\")\n",
        "print(f\"The index of the corresponding wiki doc = {codewiki[ix_to_label[0]]}\")\n",
        "print(f\"The index of the corresponding wiki doc = {codewiki[ix_to_label[0]][0]}\")\n",
        "print(f\"The vector of intersected words for the corresponding wiki doc = {wikivec[codewiki[ix_to_label[0]][0]]}\")\n",
        "print(f\"The shape of the vector is = {wikivec[codewiki[ix_to_label[0]][0]].shape}\")\n",
        "print(f\"If the ICD-9 code cannot be found is Wiki docs, the vector = {[0.0]*wikivec.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWwAFL04ktBd"
      },
      "source": [
        "Create dataset. The dataset contains 3 parts:\n",
        "*   Feature: a list of lists, and each element is a list of words (strings) for one clinical note.\n",
        "*   Notevec: a list of vectors, and each element is a vector of intersected words for one clinical note.\n",
        "*   Label: a list of lists, and each element is a list of ICD-codes for one clinical note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9OSJf1Nec4t"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "for i in range(0,len(feature)):\n",
        "  # save feature (list of words for note), note matrix and label (code) as a tuple\n",
        "  data.append((feature[i], notevec[i], label[i]))\n",
        "    \n",
        "data=np.array(data, dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Dug1BolmqH"
      },
      "source": [
        "Print the first set of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jugNQbELlqMx"
      },
      "outputs": [],
      "source": [
        "print(data[0][0])\n",
        "print(data[0][1])\n",
        "print(data[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xbg7p7InnXq"
      },
      "source": [
        "Create mapping between ICD-9 codes and the index in the code vector by 2 dictionaries.  \n",
        "**Different from previous `label_to_ix` and `ix_to_label`, this mapping is for ICD-9 codes found in Wiki documents only.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiEMkZjkelOm"
      },
      "outputs": [],
      "source": [
        "label_to_ix = {}\n",
        "ix_to_label = {}\n",
        "\n",
        "for doc, note, codes in data:\n",
        "  for code in codes:\n",
        "    if code not in label_to_ix:\n",
        "      if code in wikivoc:\n",
        "        label_to_ix[code]=len(label_to_ix)\n",
        "        ix_to_label[label_to_ix[code]]=code\n",
        "\n",
        "num_of_codes_in_both = len(label_to_ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTLBW7dqUgf"
      },
      "source": [
        "Print sample key-value pairs.  \n",
        "Print the number of ICD-9 codes which **exists in both clinical notes and Wiki documents** (From paper: \"Of those codes, we selected a subset of 344 codes for which we found the corresponding Wikipedia document and used those codes in our experiments.\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUc2UUh-nMqZ"
      },
      "outputs": [],
      "source": [
        "print(label_to_ix[\"d_486\"])\n",
        "print(ix_to_label[0])\n",
        "print(f\"Total number of codes = {num_of_codes_in_both}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2BllzNvJnH"
      },
      "source": [
        "Split training data, validation data, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90BcKLLXerYC"
      },
      "outputs": [],
      "source": [
        "training_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "training_data, val_data = train_test_split(training_data, test_size=0.125, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYfRRu_pB2K7"
      },
      "source": [
        "Create index for words in clinical notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNJKARBVeuI4"
      },
      "outputs": [],
      "source": [
        "word_to_ix = {}\n",
        "ix_to_word={}\n",
        "ix_to_word[0]='OUT'\n",
        "\n",
        "for doc, note, codes in training_data:\n",
        "  for word in doc:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)+1\n",
        "      ix_to_word[word_to_ix[word]]=word  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pl1zvOmGHBt"
      },
      "source": [
        "Print sample key-value pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiidFU4cDLtV"
      },
      "outputs": [],
      "source": [
        "print(ix_to_word[0])\n",
        "print(ix_to_word[1])\n",
        "print(word_to_ix['admission'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YRAJ4AtJuem"
      },
      "source": [
        "Create a word vector (intersected words) for each of the ICD-9 codes found in **both Wiki document and clinical notes (combined dataset)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_VMHm0Eewv5"
      },
      "outputs": [],
      "source": [
        "newwikivec=[]\n",
        "for i in range(0,len(ix_to_label)):\n",
        "  newwikivec.append(wikivec[prevoc[ix_to_label[i]]])\n",
        "newwikivec=np.array(newwikivec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGp0wg9QKHSx"
      },
      "source": [
        "Print sample result.  \n",
        "Print the number of vectors in wikivec and newwikivec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWdQx4t0Hb7g"
      },
      "outputs": [],
      "source": [
        "print(ix_to_label[0])\n",
        "print(prevoc[ix_to_label[0]])\n",
        "print(wikivec[prevoc[ix_to_label[0]]])\n",
        "print(len(newwikivec))\n",
        "print(len(wikivec))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the total running time of this section."
      ],
      "metadata": {
        "id": "yI8kbChFg70V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3bF8Y-_817e"
      },
      "outputs": [],
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Time duration: {duration[0]}h {duration[1]}m {duration[2]}s')  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Data Pre-processing 4"
      ],
      "metadata": {
        "id": "ZaFHYR_R0NLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform data processing on the training dataset, validation dataset, and test dataset. The function `preprocessing` can be found in `preprocessing.py`."
      ],
      "metadata": {
        "id": "lJMuRf_broxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "from preprocessing import preprocessing\n",
        "\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "batchtraining_data=preprocessing(training_data, label_to_ix, word_to_ix, wikivoc, batchsize)\n",
        "batchtest_data=preprocessing(test_data, label_to_ix, word_to_ix, wikivoc, batchsize)\n",
        "batchval_data=preprocessing(val_data, label_to_ix, word_to_ix, wikivoc, batchsize) "
      ],
      "metadata": {
        "id": "-UGuOeuZ0Sdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNEqr0QqvXan"
      },
      "source": [
        "# 4 Data Statistics (Overview of Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijIWWAC91OPo"
      },
      "source": [
        "The following table shows the statistics of the ***clinical notes from MIMIC-III dataset***. The values in the following table match the statistics in section 4.1 of the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U-LLj2FvY6L"
      },
      "outputs": [],
      "source": [
        "note_stat = []\n",
        "note_stat.append([\"Number of discharge summary notes\", num_of_notes])\n",
        "note_stat.append([\"Number of aggregated discharge summary notes\", num_of_aggregated_notes])\n",
        "note_stat.append([\"Number of words in discharge summary notes\", num_of_words_in_notes])\n",
        "note_stat.append([\"Number of codes in discharge summary notes\", num_of_codes_in_notes])\n",
        "df_note_stat = pd.DataFrame(note_stat, columns=['Statistics', 'Result'])\n",
        "df_note_stat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYswSNMq7koA"
      },
      "source": [
        "The following table shows the statistics of the ***Wikipedia articles for ICD-9 diagnosis codes***. The values in the following table match the statistics in section 4.1 of the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDlas4s810yZ"
      },
      "outputs": [],
      "source": [
        "wiki_stat = []\n",
        "num_of_words_in_wiki\n",
        "wiki_stat.append([\"Number of words in Wiki articles\", num_of_words_in_wiki])\n",
        "wiki_stat.append([\"Number of codes in Wiki articles\", num_of_codes_in_wiki])\n",
        "wiki_stat.append([\"Number of words in both Wiki and notes\", num_of_words_in_both])\n",
        "wiki_stat.append([\"Number of codes in both Wiki and notes\", num_of_codes_in_both])\n",
        "df_wiki_stat = pd.DataFrame(wiki_stat, columns=['Statistics', 'Result'])\n",
        "df_wiki_stat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Training and Testing of Document Similarity Learning Model"
      ],
      "metadata": {
        "id": "aCxeDVhmoeI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section investigates the performance of the standalone document similarity learning model (the 2nd part/subtask in the KSI framework) in the task of ICD-9 diagnosis code prediction from the clinical notes in MIMIC-III dataset.  \n",
        "\n",
        "First, import the needed Python packages for this section."
      ],
      "metadata": {
        "id": "V0wl-PsCV1Ev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDfshwRU9jvC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the start timestamp of this section to track the total running time of this section."
      ],
      "metadata": {
        "id": "vIWCQMFqX1YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = timeit.default_timer()"
      ],
      "metadata": {
        "id": "7wdBWP6Ilm4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the standalone document similarity learning model."
      ],
      "metadata": {
        "id": "_Jus4qx1YT6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okDHhv5Hu6dx"
      },
      "outputs": [],
      "source": [
        "Embeddingsize = 100\n",
        "hidden_dim = 200\n",
        "\n",
        "class DSL(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CAML, self).__init__()   \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize,bias=False)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)    \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "        nvec=nvec.view(batchsize,1,-1)\n",
        "        nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "        wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "        wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "        new=wiki*nvec\n",
        "        new=self.embedding(new)\n",
        "        vattention=self.sigmoid(self.vattention(new))\n",
        "        new=new*vattention\n",
        "        vec3=self.layer2(new)\n",
        "        vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        tag_scores = self.sigmoid(vec3)\n",
        "              \n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the standalone document similarity learning model. The training will stop if the top-10 recall score does not improve on the validation dataset in the next 5 epochs. The function `trainmodel` can be found in `training.py`."
      ],
      "metadata": {
        "id": "UE94KudHYgkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lXtJz6GunsQ"
      },
      "outputs": [],
      "source": [
        "from training import trainmodel\n",
        "\n",
        "topk = 10\n",
        "max_epochs = 5000 # Default is 5000\n",
        "print(f\"max epochs = {max_epochs}\")\n",
        "\n",
        "model = DSL(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "print(\"Train model\")\n",
        "DSLmodel= trainmodel(model, 1, topk, max_epochs, batchtraining_data, batchval_data, wikivec, loss_function, optimizer)\n",
        "torch.save(DSLmodel, 'DSL_ONLY_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the trained document similarity learning models on the test dataset. The function `testmodel` can be found in `testing.py`."
      ],
      "metadata": {
        "id": "TeEmocpzaHc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ3VaeGnunce"
      },
      "outputs": [],
      "source": [
        "from testing import testmodel\n",
        "\n",
        "\n",
        "print('Test DSL_Only')\n",
        "model1 = DSL(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "DSL_loss, DSL_recall, DSL_mac_auc, DSL_mic_auc, DSL_mac_f1, DSL_mic_f1 = testmodel(model1, basemodel, 0, batchtest_data, wikivec, topk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the total running time of this section."
      ],
      "metadata": {
        "id": "m3qBdL1saS9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop = timeit.default_timer()\n",
        "duration = str(datetime.timedelta(seconds=round(stop-start)))\n",
        "duration = duration.split(\":\")\n",
        "print(f'Running time of this section: {duration[0]}h {duration[1]}m {duration[2]}s') "
      ],
      "metadata": {
        "id": "bRG3E6bMnJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the performance metrics of the standalone CAML model and the CAML model with the KSI framework."
      ],
      "metadata": {
        "id": "ozo51c8MaYDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WctM9Bmjxj7q"
      },
      "outputs": [],
      "source": [
        "result_dsl = [['DSL_Standalone',  \n",
        "         round(DSL_mac_auc, 3), \n",
        "         round(DSL_mic_auc, 3), \n",
        "         round(DSL_mac_f1, 3), \n",
        "         round(DSL_mic_f1, 3),\n",
        "         round(DSL_loss, 3),\n",
        "         round(DSL_recall, 3)]]\n",
        "df_dsl = pd.DataFrame(result_dsl, columns=['Model', 'Macro_AUC', 'Micro_AUC', 'Macro_F1', 'Micro_F1', 'Loss', 'Recall@10'])\n",
        "df_dsl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Miscellaneous"
      ],
      "metadata": {
        "id": "Sigi4WuKeG1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the total running time of the notebook."
      ],
      "metadata": {
        "id": "0qoXT2M3ge5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhjFyQnkSR_7"
      },
      "outputs": [],
      "source": [
        "total_stop = timeit.default_timer()\n",
        "total_duration = str(datetime.timedelta(seconds=round(total_stop - total_start)))\n",
        "total_duration = total_duration.split(\":\")\n",
        "print(f'Notebook finished at {total_stop}')\n",
        "print(f'Total running time of this notebook: {total_duration[0]}h {total_duration[1]}m {total_duration[2]}s')   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}